{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicolasalan/td3/blob/main/td3_official.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUkL1MTSs3Nn"
      },
      "source": [
        "## **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdruBNVvMQ08",
        "outputId": "f99ef2e3-4a05-4f0f-b35a-3e3686e8a0c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jun  3 00:28:54 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add script in browser console: `inspect` => `console` => add script.\n",
        "\n",
        "```javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Conectado\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```"
      ],
      "metadata": {
        "id": "iSmLN3pgE2Fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Notes**\n",
        "* Use Apex ? -> memory"
      ],
      "metadata": {
        "id": "bQX-W5hEHOKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Montar o Google Drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WyCXKmNNBq9H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkNza9le_wuU"
      },
      "source": [
        "## **Install**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7Qa_nNJfpVwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d471c3-0f47-424f-c6b9-24364fb4eeb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 0s (2,590 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349114 sha256=da3102764567deb26325451a15eda4aefb0881b888c31de93fe512adb72e7d6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.2.1\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.3.1-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.3.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.0\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install swig\n",
        "\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]\n",
        "\n",
        "!pip install tqdm\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import**"
      ],
      "metadata": {
        "id": "BwxK-YPh0EKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import torch\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch versions not as required, installing nightly versions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjrS5wB7AOfA",
        "outputId": "981473a4-d4bd-4872-c7a6-9c13387071ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.3.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# debug memory cuda\n",
        "os.environ[\"PYTORCH_NO_CUDA_MEMORY_CACHING\"] = \"1\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,garbage_collection_threshold:0.8\"\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True"
      ],
      "metadata": {
        "id": "jsiky7ygIE38"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we're using a NVIDIA GPU\n",
        "if torch.cuda.is_available():\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find(\"failed\") >= 0:\n",
        "    print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n",
        "\n",
        "  # Get GPU name\n",
        "  gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "  gpu_name = gpu_name[1]\n",
        "  GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving\n",
        "  print(f'GPU name: {GPU_NAME}')\n",
        "\n",
        "  # Get GPU capability score\n",
        "  GPU_SCORE = torch.cuda.get_device_capability()\n",
        "  print(f\"GPU capability score: {GPU_SCORE}\")\n",
        "  if GPU_SCORE >= (8, 0):\n",
        "    print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")\n",
        "  else:\n",
        "    print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")\n",
        "\n",
        "  # Print GPU info\n",
        "  print(f\"GPU information:\\n{gpu_info}\")\n",
        "\n",
        "else:\n",
        "  print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFiWTJo9CzSf",
        "outputId": "236bc4ce-88d2-4e43-af4a-83495c32d63f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU name: Tesla_T4\n",
            "GPU capability score: (7, 5)\n",
            "GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\n",
            "GPU information:\n",
            "Mon Jun  3 00:30:47 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8              10W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check available GPU memory and total GPU memory\n",
        "try:\n",
        "  total_free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\n",
        "  print(f\"Total free GPU memory: {round(total_free_gpu_memory * 1e-9, 3)} GB\")\n",
        "  print(f\"Total GPU memory: {round(total_gpu_memory * 1e-9, 3)} GB\")\n",
        "except:\n",
        "  print(\"Please check that you have an NVIDIA GPU and installed a driver from \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13d_P3rnC1HJ",
        "outputId": "c1e0f2f0-f47c-4352-c611-37a21abccc2b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total free GPU memory: 15.728 GB\n",
            "Total GPU memory: 15.836 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size depending on amount of GPU memory\n",
        "try:\n",
        "  total_free_gpu_memory_gb = round(total_free_gpu_memory * 1e-9, 3)\n",
        "  if total_free_gpu_memory_gb >= 16:\n",
        "    BATCH_SIZE = 128 # Note: you could experiment with higher values here if you like.\n",
        "    print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE}\")\n",
        "  else:\n",
        "    BATCH_SIZE = 32\n",
        "    print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE}\")\n",
        "except:\n",
        "  BATCH_SIZE = 32 # minibatch size"
      ],
      "metadata": {
        "id": "Y2qHR2UKDojx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b09220a-4f49-436a-bade-bf9aa5ad6e56"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory available is 15.728 GB, using batch size of 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HH0qgyzCC1r7"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR_ACTOR = 1e-3         # learning rate of the actor\n",
        "LR_CRITIC = 1e-3        # learning rate of the critic\n",
        "UPDATE_EVERY_STEP = 2   # how often to update the target and actor networks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Replay**"
      ],
      "metadata": {
        "id": "ZADS3Hz_NdzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple, deque\n",
        "from typing import Tuple\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, buffer_size: int, batch_size: int):\n",
        "\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    def add(self, state: np.ndarray, action: np.ndarray, reward: np.float64, next_state: np.float32, done: bool) -> None:\n",
        "        \"\"\"Add experiences to the buffer\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state (np.ndarray): agent states\n",
        "            action (np.ndarray): agent action\n",
        "            reward (np.float64): agent reward\n",
        "            next_state (np.ndarray): agent next_state\n",
        "        \"\"\"\n",
        "        assert isinstance(state[0], np.float32), \"State is not of type (np.float32) in REPLAY BUFFER -> state type: {}.\".format(type(state))\n",
        "        # assert isinstance(action[0], np.float32), \"Action is not of type (np.float32) in REPLAY BUFFER -> action type: {}.\".format(type(action))\n",
        "        assert isinstance(reward, (int, np.float64)), \"Reward is not of type (np.float64 / int) in REPLAY BUFFER -> reward: {}.\".format(type(reward))\n",
        "        assert isinstance(next_state[0], np.float32), \"Next State is not of type (np.float32) in REPLAY BUFFER -> next state type: {}.\".format(type(next_state))\n",
        "        assert isinstance(done, bool), \"Done is not of type (bool) in REPLAY BUFFER -> done type: {}.\".format(type(done))\n",
        "\n",
        "        assert state.shape[0] == 24, \"The size of the state is not (24) in REPLAY BUFFER -> state size: {}.\".format(state.shape[0])\n",
        "        assert action.shape[0] == 4, \"The size of the action is not (4) in REPLAY BUFFER -> action size: {}.\".format(state.shape[0])\n",
        "        if isinstance(reward, np.float64):\n",
        "          assert reward.size == 1, \"The size of the reward is not (1) in REPLAY BUFFER -> reward size: {}.\".format(reward.size)\n",
        "        assert next_state.shape[0] == 24, \"The size of the next_state is not (24) in REPLAY BUFFER -> next_state size: {}.\".format(next_state.shape[0])\n",
        "\n",
        "        assert state.ndim == 1, \"The ndim of the state is not (1) in REPLAY BUFFER -> state ndim: {}.\".format(state.ndim)\n",
        "        assert action.ndim == 1, \"The ndim of the action is not (1) in REPLAY BUFFER -> action ndim: {}.\".format(state.ndim)\n",
        "        if isinstance(reward, np.float64):\n",
        "          assert reward.ndim == 0, \"The ndim of the reward is not (0) in REPLAY BUFFER -> reward ndim: {}.\".format(reward.ndim)\n",
        "        assert next_state.ndim == 1, \"The ndim of the next_state is not (1) in REPLAY BUFFER -> next_state ndim: {}.\".format(next_state.ndim)\n",
        "\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None])).int().to(device)\n",
        "\n",
        "        assert isinstance(states, torch.Tensor), \"State is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(actions, torch.Tensor), \"Actions is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(rewards, torch.Tensor), \"Rewards is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(next_states, torch.Tensor), \"Next states is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(dones, torch.Tensor), \"Dones is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "\n",
        "        assert states.dtype == torch.float32, \"The (state) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(states.dtype)\n",
        "        # assert actions.dtype == torch.float32,\"The (actions) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(actions.dtype)\n",
        "        assert rewards.dtype == torch.float32, \"The (rewards) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(rewards.dtype)\n",
        "        assert next_states.dtype == torch.float32, \"The (next_states) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(next_states.dtype)\n",
        "        assert dones.dtype == torch.int, \"The (dones) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(dones.dtype)\n",
        "\n",
        "        # TODO\n",
        "        # assert all(tensor.device.type == DEVICE for tensor in [states, actions, rewards, next_states, dones]), \"Each tensor must be on the same device in REPLAY BUFFER\"\n",
        "\n",
        "        return (\n",
        "            states, actions, rewards, next_states, dones\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> None:\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "Eqt5PjlK6aAQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "8wHw8OOk38ps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O7pJLds6C4gq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.checkpoint import checkpoint_sequential\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_size: int, action_size: int, max_action: float, l1=400, l2=300):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "        # Definir as camadas usando nn.Sequential\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(state_size, l1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l1, l2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l2, action_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, state) -> torch.Tensor:\n",
        "        assert isinstance(state, torch.Tensor), \"State is not of type torch.Tensor in ACTOR.\"\n",
        "        assert state.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in ACTOR.\"\n",
        "        assert state.shape[0] <= 24 or state.shape[0] >= BATCH_SIZE, \"The tensor shape is not torch.Size([24]) in ACTOR.\"\n",
        "        assert str(state.device.type) == str(DEVICE), \"The state must be on the same device in ACTOR.\"\n",
        "\n",
        "        x = checkpoint_sequential(self.layers, 3, state)\n",
        "        action = self.max_action * torch.tanh(x)\n",
        "        return action\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, l1=400, l2=300):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # Q1 architecture\n",
        "        self.q1_layers = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, l1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l1, l2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l2, l2)\n",
        "        )\n",
        "        self.q1_output = nn.Sequential(\n",
        "            nn.Linear(l2, 1)\n",
        "        )\n",
        "\n",
        "        # Q2 architecture\n",
        "        self.q2_layers = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, l1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l1, l2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l2, l2)\n",
        "        )\n",
        "        self.q2_output = nn.Sequential(\n",
        "            nn.Linear(l2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        assert isinstance(state, torch.Tensor), \"State is not of type torch.Tensor in CRITIC.\"\n",
        "        assert state.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert state.shape[0] == BATCH_SIZE, \"The tensor shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(state.device.type) == str(DEVICE), \"The state must be on the same device in CRITIC.\"\n",
        "\n",
        "        assert isinstance(action, torch.Tensor), \"Action is not of type torch.Tensor in CRITIC.\"\n",
        "        # assert action.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert action.shape[0] == BATCH_SIZE, \"The action shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(action.device.type) == str(DEVICE), \"The action must be on the same device in CRITIC.\"\n",
        "\n",
        "        s = torch.cat([state, action], dim=1)\n",
        "\n",
        "        # Q1 forward pass\n",
        "        q1 = checkpoint_sequential(self.q1_layers, 2, s)\n",
        "        q1 = self.q1_output(q1)\n",
        "\n",
        "        # Q2 forward pass\n",
        "        q2 = checkpoint_sequential(self.q2_layers, 2, s)\n",
        "        q2 = self.q2_output(q2)\n",
        "\n",
        "        return (q1, q2)\n",
        "\n",
        "    def Q1(self, state, action) -> torch.Tensor:\n",
        "        assert isinstance(state, torch.Tensor), \"State is not of type torch.Tensor in CRITIC.\"\n",
        "        assert state.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert state.shape[0] == BATCH_SIZE, \"The tensor shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(state.device.type) == str(DEVICE), \"The state must be on the same device in CRITIC.\"\n",
        "\n",
        "        assert isinstance(action, torch.Tensor), \"Action is not of type torch.Tensor in CRITIC.\"\n",
        "        # assert action.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert action.shape[0] == BATCH_SIZE, \"The action shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(action.device.type) == str(DEVICE), \"The action must be on the same device in CRITIC.\"\n",
        "\n",
        "        s = torch.cat([state, action], dim=1)\n",
        "\n",
        "        # Q1 forward pass\n",
        "        s1 = checkpoint_sequential(self.q1_layers, 2, s)\n",
        "        q1 = self.q1_output(s1)\n",
        "        return q1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Agent**"
      ],
      "metadata": {
        "id": "JPdq-Y3U3_0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "v3Cd3SiWC5Y5"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "from numpy import inf\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class TD3Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size: int, action_size: int, max_action: int, min_action: int, noise=0.2, noise_std=0.1, noise_clip=0.5, continue_training=False):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            max_action (ndarray): the maximum valid value for each action vector\n",
        "            min_action (ndarray): the minimum valid value for each action vector\n",
        "            noise (float): the range to generate random noise while learning\n",
        "            noise_std (float): the range to generate random noise while performing action\n",
        "            noise_clip (float): to clip random noise into this range\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.max_action = max_action\n",
        "        self.min_action = min_action\n",
        "        self.noise = noise\n",
        "        self.noise_std = noise_std\n",
        "        self.noise_clip = noise_clip\n",
        "\n",
        "        # Set the device globally\n",
        "        torch.set_default_device(device)\n",
        "\n",
        "        if continue_training:\n",
        "\n",
        "          # Transfer Learning\n",
        "          self.actor = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "          self.actor.load_state_dict(torch.load('/content/drive/MyDrive/models/checkpoint_actor_300.pth', map_location=device))\n",
        "          self.actor_target = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "          self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
        "          self.actor_optimizer.load_state_dict(torch.load('/content/drive/MyDrive/models/checkpoint_actor_optimizer_300.pth', map_location=device))\n",
        "\n",
        "          self.critic = Critic(state_size, action_size).to(device)\n",
        "          self.critic.load_state_dict(torch.load('/content/drive/MyDrive/models/checkpoint_critic_300.pth', map_location=device))\n",
        "          self.critic_target = Critic(state_size, action_size).to(device)\n",
        "          self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "          self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
        "          self.critic_optimizer.load_state_dict(torch.load('/content/drive/MyDrive/models/checkpoint_critic_optimizer_300.pth', map_location=device))\n",
        "\n",
        "        else:\n",
        "          # Actor Network (w/ Target Network)\n",
        "          self.actor = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "          self.actor_target = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "          self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
        "\n",
        "          # Critic Network (w/ Target Network)\n",
        "          self.critic = Critic(state_size, action_size).to(device)\n",
        "          self.critic_target = Critic(state_size, action_size).to(device)\n",
        "          self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "          self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
        "\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done) -> None:\n",
        "        \"\"\"Save experience in replay memory\"\"\"\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "    def predict(self, states) -> np.ndarray:\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "\n",
        "        assert isinstance(states, np.ndarray), \"States is not of data structure (np.ndarray) in PREDICT -> states: {}.\".format(type(states))\n",
        "        assert isinstance(states[0], np.float32), \"States is not of type (np.float32) in PREDICT -> states type: {}.\".format(type(states))\n",
        "        assert states.shape[0] == 24, \"The size of the states is not (24) in PREDICT -> states size: {}.\".format(states.shape[0])\n",
        "        assert states.ndim == 1, \"The ndim of the states is not (1) in PREDICT -> states ndim: {}.\".format(states.ndim)\n",
        "\n",
        "        state = torch.from_numpy(states).float().to(device)\n",
        "\n",
        "        self.actor.eval()\n",
        "        with torch.no_grad():\n",
        "          action = self.actor(state).cpu().data.numpy()\n",
        "\n",
        "        self.actor.train()\n",
        "\n",
        "        action = action.clip(self.min_action[0], self.max_action[0])\n",
        "\n",
        "        assert len(action) == self.action_size, \"The action size is different from the defined size in PREDICT.\"\n",
        "        # assert isinstance(action[0], np.float32), \"Action is not of type (np.float32) in PREDICT -> action type: {}.\".format(type(action))\n",
        "\n",
        "        return action\n",
        "\n",
        "    def learn(self, n_iteration: int, gamma=GAMMA) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\" Update policy and value parameters using given batch of experience tuples.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            n_iteration (int): the number of iterations to train network\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "\n",
        "        if len(self.memory) > BATCH_SIZE:\n",
        "            average_Q = 0\n",
        "            max_Q = -inf\n",
        "            average_critic_loss = 0\n",
        "            average_actor_loss = 0\n",
        "\n",
        "            actor_loss = 0\n",
        "\n",
        "            # print(\"n_iteration: \", n_iteration)\n",
        "\n",
        "            loop = tqdm(range(n_iteration), total=n_iteration, leave=False)\n",
        "            # print(\"Loop: \", loop)\n",
        "            for i in loop:\n",
        "                state, action, reward, next_state, done = self.memory.sample()\n",
        "\n",
        "                action_ = action.cpu().numpy()\n",
        "\n",
        "                # ---------------------------- update critic ---------------------------- #\n",
        "                # Get predicted next-state actions and Q values from target models\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    # Generate a random noise\n",
        "                    noise = torch.FloatTensor(action_).data.normal_(0, self.noise).to(device)\n",
        "                    noise = noise.clamp(-self.noise_clip, self.noise_clip)\n",
        "                    actions_next = (self.actor_target(next_state) + noise).clamp(self.min_action[0].astype(float), self.max_action[0].astype(float))\n",
        "\n",
        "                    Q1_targets_next, Q2_targets_next = self.critic_target(next_state, actions_next)\n",
        "\n",
        "                    Q_targets_next = torch.min(Q1_targets_next, Q2_targets_next)\n",
        "\n",
        "                    average_Q += torch.mean(Q_targets_next)\n",
        "                    max_Q = max(max_Q, torch.max(Q_targets_next))\n",
        "\n",
        "                    # Compute Q targets for current states (y_i)\n",
        "                    Q_targets = reward + (gamma * Q_targets_next * (1 - done)).detach()\n",
        "\n",
        "                # Compute critic loss\n",
        "                with torch.cuda.amp.autocast():\n",
        "                  Q1_expected, Q2_expected = self.critic(state, action)\n",
        "                  critic_loss = F.mse_loss(Q1_expected, Q_targets) + F.mse_loss(Q2_expected, Q_targets)\n",
        "\n",
        "                ###################\n",
        "                # Optimize Critic #\n",
        "                ###################\n",
        "\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                self.scaler.scale(critic_loss).backward()\n",
        "                self.scaler.step(self.critic_optimizer)\n",
        "\n",
        "\n",
        "                if i % UPDATE_EVERY_STEP == 0:\n",
        "                    # ---------------------------- update actor ---------------------------- #\n",
        "                    # Compute actor loss\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                      actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\n",
        "                    ##################\n",
        "                    # Optimize Actor #\n",
        "                    ##################\n",
        "\n",
        "                    self.actor_optimizer.zero_grad()\n",
        "                    self.scaler.scale(actor_loss).backward()\n",
        "                    self.scaler.step(self.actor_optimizer)\n",
        "\n",
        "                    # ----------------------- update target networks ----------------------- #\n",
        "                    self.soft_update(self.critic, self.critic_target, TAU)\n",
        "                    self.soft_update(self.actor, self.actor_target, TAU)\n",
        "\n",
        "                average_critic_loss += critic_loss\n",
        "                average_actor_loss += actor_loss\n",
        "\n",
        "                self.scaler.update()\n",
        "\n",
        "                loop.set_description(f\"Step Learn [{i}/{n_iteration}]\")\n",
        "                loop.set_postfix(critic_loss=critic_loss.item(), actor_loss=actor_loss.item())\n",
        "\n",
        "            loss_critic = average_critic_loss / n_iteration\n",
        "            loss_actor = average_actor_loss / n_iteration\n",
        "            average_policy = average_Q / n_iteration\n",
        "            max_policy = max_Q\n",
        "\n",
        "            # remove local variables and clear cache\n",
        "            del critic_loss\n",
        "            del actor_loss\n",
        "            del Q2_expected\n",
        "            del Q1_expected\n",
        "            del Q1_targets_next\n",
        "            del Q2_targets_next\n",
        "            del Q_targets\n",
        "            # # clear cache\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            return (loss_critic, loss_actor, average_policy, max_policy)\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau) -> None:\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        Params\n",
        "        ======\n",
        "            local_model: PyTorch model (weights will be copied from)\n",
        "            target_model: PyTorch model (weights will be copied to)\n",
        "            tau (float): interpolation parameter\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "    def save(self, filename, version) -> None:\n",
        "          \"\"\" Save the model \"\"\"\n",
        "          torch.save(self.critic.state_dict(), filename + \"_critic_\" + version + \".pth\")\n",
        "          torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer_\" + version + \".pth\")\n",
        "\n",
        "          torch.save(self.actor.state_dict(), filename + \"_actor_\" + version + \".pth\")\n",
        "          torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer_\" + version + \".pth\")\n",
        "\n",
        "    def load(self, filename) -> None:\n",
        "          \"\"\" Load the model \"\"\"\n",
        "          self.critic.load_state_dict(torch.load(filename + \"_critic.pth\")) # del torch.load\n",
        "          self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer.pth\"))\n",
        "          self.critic_target = copy.deepcopy(self.critic)\n",
        "\n",
        "          self.actor.load_state_dict(torch.load(filename + \"_actor.pth\"))\n",
        "          self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer.pth\"))\n",
        "          self.actor_target = copy.deepcopy(self.actor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Wrapper**"
      ],
      "metadata": {
        "id": "XXqRDsXVce1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import wandb\n",
        "from typing import Union\n",
        "from gym import spaces\n",
        "from gym.spaces import Box\n",
        "\n",
        "class CustomWrapper(gym.Wrapper):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        min_action: Union[float, int, np.ndarray],\n",
        "        max_action: Union[float, int, np.ndarray],\n",
        "    ):\n",
        "        \"\"\"Initializes the :class:`RescaleAction` wrapper.\n",
        "        Args:\n",
        "            env (Env): The environment to apply the wrapper\n",
        "            min_action (float, int or np.ndarray): The min values for each action. This may be a numpy array or a scalar.\n",
        "            max_action (float, int or np.ndarray): The max values for each action. This may be a numpy array or a scalar.\n",
        "        \"\"\"\n",
        "        assert isinstance(\n",
        "            env.action_space, spaces.Box\n",
        "        ), f\"expected Box action space, got {type(env.action_space)}\"\n",
        "        assert np.less_equal(min_action, max_action).all(), (min_action, max_action)\n",
        "\n",
        "        super().__init__(env)\n",
        "        self.min_action = (\n",
        "            np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + min_action\n",
        "        )\n",
        "        self.max_action = (\n",
        "            np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + max_action\n",
        "        )\n",
        "        self.action_space = spaces.Box(\n",
        "            low=min_action,\n",
        "            high=max_action,\n",
        "            shape=env.action_space.shape,\n",
        "            dtype=env.action_space.dtype,\n",
        "        )\n",
        "        low = self.observation_space.low[:24]\n",
        "        high = self.observation_space.high[:24]\n",
        "        self.observation_space = Box(low, high, dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, info = self.env.step(action)\n",
        "        obs = obs[:24]\n",
        "        return obs, reward, terminated, info\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        obs = obs[:24]\n",
        "        return obs\n",
        "\n",
        "    def action(self, action):\n",
        "        \"\"\"Rescales the action affinely from  [:attr:`min_action`, :attr:`max_action`] to the action space of the base environment, :attr:`env`.\n",
        "        Args:\n",
        "            action: The action to rescale\n",
        "        Returns:\n",
        "            The rescaled action\n",
        "        \"\"\"\n",
        "        assert np.all(np.greater_equal(action, self.min_action)), (\n",
        "            action,\n",
        "            self.min_action,\n",
        "        )\n",
        "        assert np.all(np.less_equal(action, self.max_action)), (action, self.max_action)\n",
        "        low = self.env.action_space.low\n",
        "        high = self.env.action_space.high\n",
        "        action = low + (high - low) * (\n",
        "            (action - self.min_action) / (self.max_action - self.min_action)\n",
        "        )\n",
        "        action = np.clip(action, low, high)\n",
        "        return action\n",
        "\n",
        "    def seed(self, seed):\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "gym.logger.set_level(40)\n",
        "env = CustomWrapper(gym.make(\"BipedalWalker-v3\"),  min_action = -1.0,  max_action = 1.0)\n",
        "env.seed(0)\n",
        "\n",
        "agent = TD3Agent(state_size=env.observation_space.shape[0], \\\n",
        "                 action_size=env.action_space.shape[0], \\\n",
        "                 max_action=env.action_space.high, \\\n",
        "                 min_action=env.action_space.low, continue_training=False)\n"
      ],
      "metadata": {
        "id": "YZNQsiSqciQ5"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train**"
      ],
      "metadata": {
        "id": "uV1KanhbJwfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.init(project=\"td3\")"
      ],
      "metadata": {
        "id": "d46eJqqklJvj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "178fd89a-80e8-43c8-9d13-1e0a111f29af"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ERQ6QREQC99V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f13a747-39ec-48d7-9327-1fb2cd63158a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 1\tAverage Score: -134.36\tScore: -134.36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 2\tAverage Score: -132.84\tScore: -131.32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 3\tAverage Score: -121.96\tScore: -100.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 4\tAverage Score: -116.86\tScore: -101.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 5\tAverage Score: -114.84\tScore: -106.77\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 6\tAverage Score: -113.43\tScore: -106.36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 7\tAverage Score: -111.59\tScore: -100.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 8\tAverage Score: -110.72\tScore: -104.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 9\tAverage Score: -109.75\tScore: -101.99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 10\tAverage Score: -109.39\tScore: -106.16\n",
            "\rEpisode 10\tAverage Score: -109.39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 11\tAverage Score: -109.02\tScore: -105.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 12\tAverage Score: -108.54\tScore: -103.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 13\tAverage Score: -108.49\tScore: -107.89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 14\tAverage Score: -108.48\tScore: -108.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 15\tAverage Score: -108.42\tScore: -107.59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 16\tAverage Score: -108.30\tScore: -106.50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 17\tAverage Score: -108.20\tScore: -106.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 18\tAverage Score: -108.10\tScore: -106.48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 19\tAverage Score: -108.02\tScore: -106.66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 20\tAverage Score: -107.93\tScore: -106.24\n",
            "\rEpisode 20\tAverage Score: -107.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 21\tAverage Score: -107.89\tScore: -106.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 22\tAverage Score: -107.83\tScore: -106.64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 23\tAverage Score: -107.78\tScore: -106.72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 24\tAverage Score: -107.73\tScore: -106.56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 25\tAverage Score: -107.68\tScore: -106.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 26\tAverage Score: -107.65\tScore: -106.78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 27\tAverage Score: -107.49\tScore: -103.33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 28\tAverage Score: -107.46\tScore: -106.66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 29\tAverage Score: -107.43\tScore: -106.63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 30\tAverage Score: -107.23\tScore: -101.45\n",
            "\rEpisode 30\tAverage Score: -107.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 31\tAverage Score: -107.20\tScore: -106.42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 32\tAverage Score: -107.10\tScore: -103.91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 33\tAverage Score: -106.97\tScore: -102.82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 34\tAverage Score: -106.84\tScore: -102.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 35\tAverage Score: -106.66\tScore: -100.59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 36\tAverage Score: -106.56\tScore: -102.95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 37\tAverage Score: -106.41\tScore: -100.80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 38\tAverage Score: -106.25\tScore: -100.49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 39\tAverage Score: -106.12\tScore: -101.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 40\tAverage Score: -105.98\tScore: -100.68\n",
            "\rEpisode 40\tAverage Score: -105.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 41\tAverage Score: -105.98\tScore: -106.05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 42\tAverage Score: -105.86\tScore: -100.69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 43\tAverage Score: -105.73\tScore: -100.56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 44\tAverage Score: -105.67\tScore: -102.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 45\tAverage Score: -105.58\tScore: -101.75\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-1f0e6a90cf07>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-1f0e6a90cf07>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_episodes, max_t)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-2ea27e4ad47f>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m           \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-8a2bfb92550b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     20\u001b[0m         )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"State is not of type torch.Tensor in ACTOR.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tensor elements are not of type torch.float32 in ACTOR.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def train(n_episodes=100, max_t=2000):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    times_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    solved = False\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        start_time = time.time()\n",
        "        for t in range(max_t):\n",
        "            action = agent.predict(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            if done or t==(max_t-1):\n",
        "                critic_loss, actor_loss, q, max = agent.learn(t)\n",
        "                break\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        scores_deque.append(score)\n",
        "        times_deque.append(duration)\n",
        "        scores.append(score)\n",
        "        mean_score = np.mean(scores_deque)\n",
        "        mean_times = np.mean(times_deque)\n",
        "\n",
        "        # wandb.log({'Score': mean_score, 'Critic loss': critic_loss, 'Actor loss': actor_loss, 'Average Q': q, 'Max. Q': max, \"Duration \": mean_times}, step=i_episode)\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, mean_score, score))\n",
        "        if i_episode % 10 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "        if i_episode % 500 == 0:\n",
        "            agent.save(\"checkpoint\", str(i_episode))\n",
        "        if mean_score >= 300 and solved == False:\n",
        "            print('\\rSolved at Episode {} !\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "            agent.save(\"checkpoint\")\n",
        "            solved = True\n",
        "\n",
        "    return scores\n",
        "\n",
        "scores = train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Result**"
      ],
      "metadata": {
        "id": "_GJ4lM-g8jB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "metadata": {
        "id": "iUJfDXmtMesi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "\n",
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "agent.actor.load_state_dict(torch.load('/content/checkpoint_actor_300.pth'))\n",
        "agent.critic.load_state_dict(torch.load('/content/checkpoint_critic_300.pth'))\n",
        "agent.actor_optimizer.load_state_dict(torch.load('/content/checkpoint_actor_optimizer_300.pth'))\n",
        "agent.critic_optimizer.load_state_dict(torch.load('/content/checkpoint_critic_optimizer_300.pth'))\n",
        "\n",
        "env = gym.make('BipedalWalker-v3')\n",
        "state = env.reset()\n",
        "score = 0\n",
        "img = plt.imshow(env.render('rgb_array'))\n",
        "while True:\n",
        "    img.set_data(env.render('rgb_array'))\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    action = agent.predict(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if np.any(done):\n",
        "        break\n",
        "\n",
        "print(\"Score: {}\".format(score))"
      ],
      "metadata": {
        "id": "IXJHQR6N8uwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test**"
      ],
      "metadata": {
        "id": "pt4M5an2gPmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestModel(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.env = gym.make(\"BipedalWalker-v3\")\n",
        "\n",
        "        # param model and buffer\n",
        "        self.batch_size = 100\n",
        "        self.buffer_size = int(1e5)\n",
        "        self.random_seed = 0\n",
        "        self.error = 0\n",
        "\n",
        "        # size action / state\n",
        "        self.action_size = self.env.action_space.shape[0]\n",
        "        self.state_size = self.env.observation_space.shape[0]\n",
        "\n",
        "        # min / max action\n",
        "        self.min_action = self.env.action_space.low\n",
        "        self.max_action = self.env.action_space.high\n",
        "\n",
        "        # min / max state\n",
        "        self.min_state = 0\n",
        "        self.max_state = 1\n",
        "\n",
        "        # min / max reward\n",
        "        self.min_reward = -300\n",
        "        self.max_reward = 300\n",
        "\n",
        "        self.model = TD3Agent(state_size=self.state_size, action_size=self.action_size, \\\n",
        "                         max_action=self.max_action, min_action=self.min_action, random_seed=self.random_seed)\n",
        "\n",
        "        self.memory = ReplayBufferPer(self.buffer_size)\n",
        "\n",
        "        # param number tests\n",
        "        self.num_attempts = 150\n",
        "\n",
        "    def _randomStates(self):\n",
        "        states = np.array([random.uniform(self.min_state, self.max_state) for _ in range(24)], dtype=np.float32)\n",
        "        return states\n",
        "\n",
        "    def _randomAction(self):\n",
        "        action = np.random.uniform(self.min_action, self.max_action, self.action_size)\n",
        "        return action\n",
        "\n",
        "    def _randomDone(self):\n",
        "        done = random.choice([True, False])\n",
        "        return done\n",
        "\n",
        "    def _randomReward(self):\n",
        "        reward = random.randint(self.min_reward, self.max_reward)\n",
        "        return reward\n",
        "\n",
        "    def test_predict_act(self):\n",
        "        \"\"\" Teste para verificar se o estado de saida da rede contem os valores minimos e maximos de ação que o ambiente exigi\n",
        "\n",
        "            Input: numpy.ndarray [24]\n",
        "            output: numpy.ndarray [4]\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        for _ in range(self.num_attempts):\n",
        "            states = self._randomStates()\n",
        "            action = self.model.predict(states)\n",
        "            is_valid = (isinstance(action, np.ndarray) and np.all(action >= self.min_action) and np.all(action <= self.max_action))\n",
        "\n",
        "            if not is_valid:\n",
        "                self.fail(\"Teste falhou na tentativa {}\".format(_ + 1))\n",
        "\n",
        "    def test_buffer_type(self):\n",
        "        while True:\n",
        "          next_state, reward, done, action, state = self._randomStates(), self._randomReward(), self._randomDone(), self._randomAction(), self._randomStates()\n",
        "\n",
        "          self.memory.add((state, action, reward, next_state, done), reward)\n",
        "\n",
        "          if len(self.memory) > self.batch_size:\n",
        "              break\n",
        "\n",
        "\n",
        "        (states, actions, rewards, next_states, dones), idxs, is_weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        self.assertIsInstance(states, torch.Tensor)\n",
        "        self.assertIsInstance(actions, torch.Tensor)\n",
        "        self.assertIsInstance(rewards, torch.Tensor)\n",
        "        self.assertIsInstance(next_states, torch.Tensor)\n",
        "        self.assertIsInstance(dones, torch.Tensor)\n",
        "\n",
        "    def test_buffer_size(self):\n",
        "        while True:\n",
        "          next_state, reward, done, action, state = self._randomStates(), self._randomReward(), self._randomDone(), self._randomAction(), self._randomStates()\n",
        "          self.memory.add((state, action, reward, next_state, done), reward)\n",
        "\n",
        "          if len(self.memory) > self.batch_size:\n",
        "            break\n",
        "\n",
        "        (states, actions, rewards, next_states, dones), idxs, is_weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        expected_batch_size = self.batch_size\n",
        "        self.assertEqual(states.size(0), expected_batch_size)\n",
        "        self.assertEqual(actions.size(0), expected_batch_size)\n",
        "        self.assertEqual(rewards.size(0), expected_batch_size)\n",
        "        self.assertEqual(next_states.size(0), expected_batch_size)\n",
        "        self.assertEqual(dones.size(0), expected_batch_size)\n",
        "\n",
        "    def test_buffer_range(self):\n",
        "        while True:\n",
        "          next_state, reward, done, action, state = self._randomStates(), self._randomReward(), self._randomDone(), self._randomAction(), self._randomStates()\n",
        "          self.memory.add((state, action, reward, next_state, done), reward)\n",
        "\n",
        "          if len(self.memory) > self.batch_size:\n",
        "            break\n",
        "\n",
        "        (states, actions, rewards, next_states, dones), idxs, weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        self.assertTrue(np.all(states[1].cpu().data.numpy() >= self.min_state) and np.all(states[1].cpu().data.numpy() <= self.max_state))\n",
        "        self.assertTrue(np.all(actions[1].cpu().data.numpy() >= self.min_action) and np.all(actions[1].cpu().data.numpy() <= self.max_action))\n",
        "        self.assertTrue(np.all(rewards[1].cpu().data.numpy() >= self.min_reward) and np.all(rewards[1].cpu().data.numpy() <= self.max_reward))\n",
        "        self.assertTrue(np.all(next_states[1].cpu().data.numpy() >= self.min_state) and np.all(next_states[1].cpu().data.numpy() <= self.max_state))\n",
        "        self.assertTrue(np.all(dones.cpu().data.numpy() >= 0.) and np.all(dones.cpu().data.numpy() <= 1.))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "id": "huHvCkTggSDs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gkNza9le_wuU",
        "ZADS3Hz_NdzM",
        "XXqRDsXVce1Y",
        "_GJ4lM-g8jB9",
        "pt4M5an2gPmn"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicolasalan/td3/blob/main/td3_official.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUkL1MTSs3Nn"
      },
      "source": [
        "## **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdruBNVvMQ08",
        "outputId": "47a716bb-d291-44b5-d844-d6fd972015eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add script in browser console: `inspect` => `console` => add script.\n",
        "\n",
        "```javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Conectado\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```"
      ],
      "metadata": {
        "id": "iSmLN3pgE2Fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Commit**\n",
        "* *add lr_schedule on Jun 7, 2024*"
      ],
      "metadata": {
        "id": "bQX-W5hEHOKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Montar o Google Drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WyCXKmNNBq9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkNza9le_wuU"
      },
      "source": [
        "## **Install**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Qa_nNJfpVwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27c5987f-a40e-42bc-8b4c-31775d7ef69a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,163 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 121913 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.1)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install swig\n",
        "\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]\n",
        "\n",
        "!pip install tqdm\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import and Setup**"
      ],
      "metadata": {
        "id": "BwxK-YPh0EKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import torch\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch versions not as required, installing nightly versions.\")"
      ],
      "metadata": {
        "id": "JjrS5wB7AOfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# debug memory cuda\n",
        "# os.environ[\"PYTORCH_NO_CUDA_MEMORY_CACHING\"] = \"1\"\n",
        "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,garbage_collection_threshold:0.8\"\n",
        "\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True"
      ],
      "metadata": {
        "id": "jsiky7ygIE38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we're using a NVIDIA GPU\n",
        "if torch.cuda.is_available():\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find(\"failed\") >= 0:\n",
        "    print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n",
        "\n",
        "  # Get GPU name\n",
        "  gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "  gpu_name = gpu_name[1]\n",
        "  GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving\n",
        "  print(f'GPU name: {GPU_NAME}')\n",
        "\n",
        "  # Get GPU capability score\n",
        "  GPU_SCORE = torch.cuda.get_device_capability()\n",
        "  print(f\"GPU capability score: {GPU_SCORE}\")\n",
        "  if GPU_SCORE >= (8, 0):\n",
        "    print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")\n",
        "  else:\n",
        "    print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")\n",
        "\n",
        "  # Print GPU info\n",
        "  print(f\"GPU information:\\n{gpu_info}\")\n",
        "\n",
        "else:\n",
        "  print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFiWTJo9CzSf",
        "outputId": "e024373b-dc72-4bb0-c353-481983a282e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU name: Tesla_T4\n",
            "GPU capability score: (7, 5)\n",
            "GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\n",
            "GPU information:\n",
            "Fri Jun  7 23:33:47 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8              11W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check available GPU memory and total GPU memory\n",
        "try:\n",
        "  total_free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\n",
        "  print(f\"Total free GPU memory: {round(total_free_gpu_memory * 1e-9, 3)} GB\")\n",
        "  print(f\"Total GPU memory: {round(total_gpu_memory * 1e-9, 3)} GB\")\n",
        "except:\n",
        "  print(\"Please check that you have an NVIDIA GPU and installed a driver from \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13d_P3rnC1HJ",
        "outputId": "f215ceaa-b14c-4d29-afaa-69e7bfe32f21"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total free GPU memory: 15.728 GB\n",
            "Total GPU memory: 15.836 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size depending on amount of GPU memory\n",
        "try:\n",
        "  total_free_gpu_memory_gb = round(total_free_gpu_memory * 1e-9, 3)\n",
        "  if total_free_gpu_memory_gb >= 16:\n",
        "    BATCH_SIZE = 1028 # Note: you could experiment with higher values here if you like.\n",
        "    print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE}\")\n",
        "  else:\n",
        "    BATCH_SIZE = 512\n",
        "    print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE}\")\n",
        "except:\n",
        "  BATCH_SIZE = 512 # minibatch size"
      ],
      "metadata": {
        "id": "Y2qHR2UKDojx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013edab8-9334-4886-d30d-7bddd853d38c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory available is 15.728 GB, using batch size of 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HH0qgyzCC1r7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60a828b-7e9e-42bb-a9b5-2b5eb158e595"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH_SIZE: 512\n",
            "BUFFER_SIZE: 100000\n",
            "GAMMA: 0.99\n",
            "TAU: 0.001\n",
            "LR_ACTOR: 0.001\n",
            "LR_CRITIC: 0.001\n",
            "UPDATE_EVERY_STEP: 2\n",
            "DEVICE: cpu\n",
            "WANDB: False\n",
            "PRETRAINED: False\n",
            "USE_AMP: True\n",
            "REENTRANT: True\n",
            "PATH: \n"
          ]
        }
      ],
      "source": [
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR_ACTOR = 1e-3         # learning rate of the actor\n",
        "LR_CRITIC = 1e-3        # learning rate of the critic\n",
        "UPDATE_EVERY_STEP = 2   # how often to update the target and actor networks\n",
        "BATCH_SIZE = 512        # minibatch size\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "WANDB = False\n",
        "PRETRAINED = False\n",
        "USE_AMP = True\n",
        "REENTRANT = True\n",
        "NUM_EPISODES = 2000\n",
        "\n",
        "if PRETRAINED:\n",
        "  PATH = '/content/'\n",
        "else:\n",
        "  PATH = ''\n",
        "\n",
        "print(f\"BATCH_SIZE: {BATCH_SIZE}\")\n",
        "print(f\"BUFFER_SIZE: {BUFFER_SIZE}\")\n",
        "print(f\"GAMMA: {GAMMA}\")\n",
        "print(f\"TAU: {TAU}\")\n",
        "print(f\"LR_ACTOR: {LR_ACTOR}\")\n",
        "print(f\"LR_CRITIC: {LR_CRITIC}\")\n",
        "print(f\"UPDATE_EVERY_STEP: {UPDATE_EVERY_STEP}\")\n",
        "print(f\"DEVICE: {DEVICE}\")\n",
        "print(f\"WANDB: {WANDB}\")\n",
        "print(f\"PRETRAINED: {PRETRAINED}\")\n",
        "print(f\"USE_AMP: {USE_AMP}\")\n",
        "print(f\"REENTRANT: {REENTRANT}\")\n",
        "print(f\"PATH: {PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Replay**"
      ],
      "metadata": {
        "id": "ZADS3Hz_NdzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple, deque\n",
        "from typing import Tuple\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, buffer_size: int, batch_size: int):\n",
        "\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    def add(self, state: np.ndarray, action: np.ndarray, reward: np.float64, next_state: np.float32, done: bool) -> None:\n",
        "        \"\"\"Add experiences to the buffer\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state (np.ndarray): agent states\n",
        "            action (np.ndarray): agent action\n",
        "            reward (np.float64): agent reward\n",
        "            next_state (np.ndarray): agent next_state\n",
        "        \"\"\"\n",
        "        assert isinstance(state[0], np.float32), \"State is not of type (np.float32) in REPLAY BUFFER -> state type: {}.\".format(type(state))\n",
        "        # assert isinstance(action[0], np.float32), \"Action is not of type (np.float32) in REPLAY BUFFER -> action type: {}.\".format(type(action))\n",
        "        assert isinstance(reward, (int, np.float64)), \"Reward is not of type (np.float64 / int) in REPLAY BUFFER -> reward: {}.\".format(type(reward))\n",
        "        assert isinstance(next_state[0], np.float32), \"Next State is not of type (np.float32) in REPLAY BUFFER -> next state type: {}.\".format(type(next_state))\n",
        "        assert isinstance(done, bool), \"Done is not of type (bool) in REPLAY BUFFER -> done type: {}.\".format(type(done))\n",
        "\n",
        "        assert state.shape[0] == 24, \"The size of the state is not (24) in REPLAY BUFFER -> state size: {}.\".format(state.shape[0])\n",
        "        assert action.shape[0] == 4, \"The size of the action is not (4) in REPLAY BUFFER -> action size: {}.\".format(state.shape[0])\n",
        "        if isinstance(reward, np.float64):\n",
        "          assert reward.size == 1, \"The size of the reward is not (1) in REPLAY BUFFER -> reward size: {}.\".format(reward.size)\n",
        "        assert next_state.shape[0] == 24, \"The size of the next_state is not (24) in REPLAY BUFFER -> next_state size: {}.\".format(next_state.shape[0])\n",
        "\n",
        "        assert state.ndim == 1, \"The ndim of the state is not (1) in REPLAY BUFFER -> state ndim: {}.\".format(state.ndim)\n",
        "        assert action.ndim == 1, \"The ndim of the action is not (1) in REPLAY BUFFER -> action ndim: {}.\".format(state.ndim)\n",
        "        if isinstance(reward, np.float64):\n",
        "          assert reward.ndim == 0, \"The ndim of the reward is not (0) in REPLAY BUFFER -> reward ndim: {}.\".format(reward.ndim)\n",
        "        assert next_state.ndim == 1, \"The ndim of the next_state is not (1) in REPLAY BUFFER -> next_state ndim: {}.\".format(next_state.ndim)\n",
        "\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None])).int().to(device)\n",
        "\n",
        "        assert isinstance(states, torch.Tensor), \"State is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(actions, torch.Tensor), \"Actions is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(rewards, torch.Tensor), \"Rewards is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(next_states, torch.Tensor), \"Next states is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(dones, torch.Tensor), \"Dones is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "\n",
        "        assert states.dtype == torch.float32, \"The (state) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(states.dtype)\n",
        "        # assert actions.dtype == torch.float32,\"The (actions) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(actions.dtype)\n",
        "        assert rewards.dtype == torch.float32, \"The (rewards) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(rewards.dtype)\n",
        "        assert next_states.dtype == torch.float32, \"The (next_states) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(next_states.dtype)\n",
        "        assert dones.dtype == torch.int, \"The (dones) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(dones.dtype)\n",
        "\n",
        "        # TODO\n",
        "        # assert all(tensor.device.type == DEVICE for tensor in [states, actions, rewards, next_states, dones]), \"Each tensor must be on the same device in REPLAY BUFFER\"\n",
        "\n",
        "        return (\n",
        "            states, actions, rewards, next_states, dones\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> None:\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "Eqt5PjlK6aAQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "8wHw8OOk38ps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "O7pJLds6C4gq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.checkpoint import checkpoint_sequential\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_size: int, action_size: int, max_action: float, l1=400, l2=300):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(state_size, l1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l1, l2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l2, action_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, state) -> torch.Tensor:\n",
        "        assert isinstance(state, torch.Tensor), \"State is not of type torch.Tensor in ACTOR.\"\n",
        "        assert state.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in ACTOR.\"\n",
        "        assert state.shape[0] <= 24 or state.shape[0] >= BATCH_SIZE, \"The tensor shape is not torch.Size([24]) in ACTOR.\"\n",
        "        assert str(state.device.type) == str(DEVICE), \"The state must be on the same device in ACTOR.\"\n",
        "\n",
        "        x = self.layers(state)\n",
        "        action = self.max_action * torch.tanh(x)\n",
        "        return action\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight) # Xavier initialization\n",
        "                m.bias.data.fill_(0.01) # bias initialization\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim: int, action_dim: int, l1=400, l2=300):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # Q1 architecture\n",
        "        self.q1 = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, l1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l1, l2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l2, l2),\n",
        "            nn.Linear(l2, 1)\n",
        "        )\n",
        "\n",
        "        # Q2 architecture\n",
        "        self.q2 = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, l1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l1, l2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l2, l2),\n",
        "            nn.Linear(l2, 1)\n",
        "        )\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # assert isinstance(state, torch.Tensor), \"State is not of type torch.Tensor in CRITIC.\"\n",
        "        assert state.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert state.shape[0] == BATCH_SIZE, \"The tensor shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(state.device.type) == str(DEVICE), \"The state must be on the same device in CRITIC.\"\n",
        "\n",
        "        # assert isinstance(action, torch.Tensor), \"Action is not of type torch.Tensor in CRITIC.\"\n",
        "        # assert action.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert action.shape[0] == BATCH_SIZE, \"The action shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(action.device.type) == str(DEVICE), \"The action must be on the same device in CRITIC.\"\n",
        "\n",
        "        s = torch.cat([state, action], dim=1)\n",
        "\n",
        "        # Q1 forward pass\n",
        "        q1 = checkpoint_sequential(self.q1, 2, s, use_reentrant=REENTRANT)\n",
        "\n",
        "        # Q2 forward pass\n",
        "        q2 = checkpoint_sequential(self.q2, 2, s, use_reentrant=REENTRANT)\n",
        "\n",
        "        return q1, q2\n",
        "\n",
        "    def Q1(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        # assert isinstance(state, torch.Tensor), \"State is not of type torch.Tensor in CRITIC.\"\n",
        "        assert state.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert state.shape[0] == BATCH_SIZE, \"The tensor shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(state.device.type) == str(DEVICE), \"The state must be on the same device in CRITIC.\"\n",
        "\n",
        "        # assert isinstance(action, torch.Tensor), \"Action is not of type torch.Tensor in CRITIC.\"\n",
        "        # assert action.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert action.shape[0] == BATCH_SIZE, \"The action shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(action.device.type) == str(DEVICE), \"The action must be on the same device in CRITIC.\"\n",
        "\n",
        "        s = torch.cat([state, action], dim=1)\n",
        "\n",
        "        # Q1 forward pass\n",
        "        s1 = checkpoint_sequential(self.q1, 2, s, use_reentrant=REENTRANT)\n",
        "        return s1\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight) # Xavier initialization\n",
        "                m.bias.data.fill_(0.01) # bias initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Agent**"
      ],
      "metadata": {
        "id": "JPdq-Y3U3_0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v3Cd3SiWC5Y5"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "from numpy import inf\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class TD3Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size: int, action_size: int, max_action: int, min_action: int, noise=0.2, noise_std=0.1, noise_clip=0.5, pretraining=False):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            max_action (ndarray): the maximum valid value for each action vector\n",
        "            min_action (ndarray): the minimum valid value for each action vector\n",
        "            noise (float): the range to generate random noise while learning\n",
        "            noise_std (float): the range to generate random noise while performing action\n",
        "            noise_clip (float): to clip random noise into this range\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.max_action = max_action\n",
        "        self.min_action = min_action\n",
        "        self.noise = noise\n",
        "        self.noise_std = noise_std\n",
        "        self.noise_clip = noise_clip\n",
        "\n",
        "        # Set the device globally\n",
        "        torch.set_default_device(device)\n",
        "\n",
        "        if pretraining:\n",
        "\n",
        "          # Transfer Learning\n",
        "\n",
        "          # Actor Network (w/ Target Network)\n",
        "          self.actor = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "          self.actor_target = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "          self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
        "\n",
        "          # Critic Network (w/ Target Network)\n",
        "          self.critic = Critic(state_size, action_size).to(device)\n",
        "          self.critic_target = Critic(state_size, action_size).to(device)\n",
        "          self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "          self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
        "\n",
        "          self.load(PATH + 'checkpoint')\n",
        "\n",
        "        else:\n",
        "          # Actor Network (w/ Target Network)\n",
        "          self.actor = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "          self.actor_target = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "          self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
        "\n",
        "          # Critic Network (w/ Target Network)\n",
        "          self.critic = Critic(state_size, action_size).to(device)\n",
        "          self.critic_target = Critic(state_size, action_size).to(device)\n",
        "          self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "          self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
        "\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "        self.clip_grad = torch.nn.utils.clip_grad_norm_\n",
        "\n",
        "        # Inicializar schedulers\n",
        "        self.actor_scheduler = optim.lr_scheduler.StepLR(self.actor_optimizer, step_size=10, gamma=0.9)\n",
        "        self.critic_scheduler = optim.lr_scheduler.StepLR(self.critic_optimizer, step_size=10, gamma=0.9)\n",
        "\n",
        "        # compile model\n",
        "        self.actor = torch.compile(self.actor)\n",
        "        self.critic = torch.compile(self.critic)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
        "\n",
        "    def step(self, state: np.ndarray, action: np.ndarray, reward: np.ndarray, next_state: np.ndarray, done: bool) -> None:\n",
        "        \"\"\"Save experience in replay memory\"\"\"\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "    def predict(self, states: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "\n",
        "        # assert isinstance(states, np.ndarray), \"States is not of data structure (np.ndarray) in PREDICT -> states: {}.\".format(type(states))\n",
        "        assert isinstance(states[0], np.float32), \"States is not of type (np.float32) in PREDICT -> states type: {}.\".format(type(states))\n",
        "        assert states.shape[0] == 24, \"The size of the states is not (24) in PREDICT -> states size: {}.\".format(states.shape[0])\n",
        "        assert states.ndim == 1, \"The ndim of the states is not (1) in PREDICT -> states ndim: {}.\".format(states.ndim)\n",
        "\n",
        "        state = torch.from_numpy(states).float().to(device)\n",
        "\n",
        "        self.actor.eval()\n",
        "        with torch.no_grad():\n",
        "          action = self.actor(state).cpu().data.numpy() # EXP: use detach ?\n",
        "\n",
        "        self.actor.train()\n",
        "\n",
        "        action = action.clip(self.min_action[0], self.max_action[0])\n",
        "\n",
        "        assert action.shape[0] == self.action_size, \"The action size is different from the defined size in PREDICT.\"\n",
        "        # assert isinstance(action[0], np.float32), \"Action is not of type (np.float32) in PREDICT -> action type: {}.\".format(type(action))\n",
        "\n",
        "        return action\n",
        "\n",
        "    def learn(self, n_iteration: int, episode: int, gamma=GAMMA) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\" Update policy and value parameters using given batch of experience tuples.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            n_iteration (int): the number of iterations to train network\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "\n",
        "        if len(self.memory) > BATCH_SIZE:\n",
        "            average_Q = 0\n",
        "            max_Q = -inf\n",
        "            average_critic_loss = 0\n",
        "            average_actor_loss = 0\n",
        "            actor_loss = 0\n",
        "\n",
        "            # start training\n",
        "            self.critic.train()\n",
        "            self.actor.train()\n",
        "\n",
        "            loop = tqdm(range(n_iteration), total=n_iteration, leave=False)\n",
        "\n",
        "            for i in loop:\n",
        "                # Sample a batch of experiences\n",
        "                state, action, reward, next_state, done = self.memory.sample()\n",
        "\n",
        "                action_ = action.cpu().numpy() # EXP: use detach ?\n",
        "\n",
        "                # ---------------------------- update critic ---------------------------- #\n",
        "                # Get predicted next-state actions and Q values from target models\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    # Generate a random noise\n",
        "                    noise = torch.FloatTensor(action_).data.normal_(0, self.noise).to(device)\n",
        "                    noise = noise.clamp(-self.noise_clip, self.noise_clip)\n",
        "                    actions_next = (self.actor_target(next_state) + noise).clamp(self.min_action[0].astype(float), self.max_action[0].astype(float))\n",
        "\n",
        "                    Q1_targets_next, Q2_targets_next = self.critic_target(next_state, actions_next)\n",
        "\n",
        "                    Q_targets_next = torch.min(Q1_targets_next, Q2_targets_next)\n",
        "\n",
        "                    average_Q += torch.mean(Q_targets_next)\n",
        "                    max_Q = max(max_Q, torch.max(Q_targets_next))\n",
        "\n",
        "                    # Compute Q targets for current states (y_i)\n",
        "                    Q_targets = reward + (gamma * Q_targets_next * (1 - done)).detach()\n",
        "\n",
        "                # Compute critic loss\n",
        "                self.critic_optimizer.zero_grad()\n",
        "\n",
        "                with torch.cuda.amp.autocast(dtype=torch.float16, enabled=USE_AMP):\n",
        "                  Q1_expected, Q2_expected = self.critic(state, action)\n",
        "                  critic_loss = F.mse_loss(Q1_expected, Q_targets) + F.mse_loss(Q2_expected, Q_targets)\n",
        "\n",
        "                ###################\n",
        "                # Optimize Critic #\n",
        "                ###################\n",
        "\n",
        "                self.scaler.scale(critic_loss).backward()\n",
        "                self.scaler.unscale_(self.critic_optimizer)\n",
        "                self.clip_grad(self.critic.parameters(), max_norm=1.0)\n",
        "                self.scaler.step(self.critic_optimizer)\n",
        "\n",
        "                if i % UPDATE_EVERY_STEP == 0:\n",
        "                    # ---------------------------- update actor ---------------------------- #\n",
        "                    # Compute actor loss\n",
        "                    self.actor_optimizer.zero_grad()\n",
        "\n",
        "                    with torch.cuda.amp.autocast(dtype=torch.float16, enabled=USE_AMP):\n",
        "                      actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\n",
        "                    ##################\n",
        "                    # Optimize Actor #\n",
        "                    ##################\n",
        "\n",
        "                    self.scaler.scale(actor_loss).backward()\n",
        "                    self.scaler.unscale_(self.actor_optimizer)\n",
        "                    self.clip_grad(self.actor.parameters(), max_norm=1.0)\n",
        "                    self.scaler.step(self.actor_optimizer)\n",
        "\n",
        "                    # ----------------------- update target networks ----------------------- #\n",
        "                    self.soft_update(self.critic, self.critic_target, TAU)\n",
        "                    self.soft_update(self.actor, self.actor_target, TAU)\n",
        "\n",
        "                average_critic_loss += critic_loss # EXP: use detach ?\n",
        "                average_actor_loss += actor_loss # EXP: use detach ?\n",
        "\n",
        "                self.scaler.update()\n",
        "\n",
        "                # Update schedulers\n",
        "                self.actor_scheduler.step()\n",
        "                self.critic_scheduler.step()\n",
        "\n",
        "                loop.set_description(f\"Learn Model [{episode}/{NUM_EPISODES}]\")\n",
        "                loop.set_postfix(critic_loss=critic_loss.item(), actor_loss=actor_loss.item())\n",
        "\n",
        "            loss_critic = average_critic_loss / n_iteration\n",
        "            loss_actor = average_actor_loss / n_iteration\n",
        "            average_policy = average_Q / n_iteration\n",
        "            max_policy = max_Q\n",
        "\n",
        "            # remove local variables and clear cache\n",
        "            # del critic_loss\n",
        "            # del actor_loss\n",
        "            # del Q2_expected\n",
        "            # del Q1_expected\n",
        "            # del Q1_targets_next\n",
        "            # del Q2_targets_next\n",
        "            # del Q_targets\n",
        "            # # clear cache\n",
        "            # torch.cuda.empty_cache()\n",
        "\n",
        "            return loss_critic, loss_actor, average_policy, max_policy\n",
        "\n",
        "    @staticmethod\n",
        "    def soft_update(local_model, target_model, tau) -> None:\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        Params\n",
        "        ======\n",
        "            local_model: PyTorch model (weights will be copied from)\n",
        "            target_model: PyTorch model (weights will be copied to)\n",
        "            tau (float): interpolation parameter\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "    def save(self, filename, version) -> None:\n",
        "          \"\"\" Save the model \"\"\"\n",
        "          torch.save(self.critic.state_dict(), filename + \"_critic_\" + version + \".pth\")\n",
        "          torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer_\" + version + \".pth\")\n",
        "\n",
        "          torch.save(self.actor.state_dict(), filename + \"_actor_\" + version + \".pth\")\n",
        "          torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer_\" + version + \".pth\")\n",
        "\n",
        "    def load(self, filename) -> None:\n",
        "          \"\"\" Load the model \"\"\"\n",
        "          self.critic.load_state_dict(torch.load(filename + \"_critic.pth\")) # del torch.load\n",
        "          self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer.pth\"))\n",
        "          self.critic_target = copy.deepcopy(self.critic)\n",
        "\n",
        "          self.actor.load_state_dict(torch.load(filename + \"_actor.pth\"))\n",
        "          self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer.pth\"))\n",
        "          self.actor_target = copy.deepcopy(self.actor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Wrapper**"
      ],
      "metadata": {
        "id": "XXqRDsXVce1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import wandb\n",
        "from typing import Union\n",
        "from gym import spaces\n",
        "from gym.spaces import Box\n",
        "\n",
        "class CustomWrapper(gym.Wrapper):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        min_action: Union[float, int, np.ndarray],\n",
        "        max_action: Union[float, int, np.ndarray],\n",
        "    ):\n",
        "        \"\"\"Initializes the :class:`RescaleAction` wrapper.\n",
        "        Args:\n",
        "            env (Env): The environment to apply the wrapper\n",
        "            min_action (float, int or np.ndarray): The min values for each action. This may be a numpy array or a scalar.\n",
        "            max_action (float, int or np.ndarray): The max values for each action. This may be a numpy array or a scalar.\n",
        "        \"\"\"\n",
        "        assert isinstance(\n",
        "            env.action_space, spaces.Box\n",
        "        ), f\"expected Box action space, got {type(env.action_space)}\"\n",
        "        assert np.less_equal(min_action, max_action).all(), (min_action, max_action)\n",
        "\n",
        "        super().__init__(env)\n",
        "        self.min_action = (\n",
        "            np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + min_action\n",
        "        )\n",
        "        self.max_action = (\n",
        "            np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + max_action\n",
        "        )\n",
        "        self.action_space = spaces.Box(\n",
        "            low=min_action,\n",
        "            high=max_action,\n",
        "            shape=env.action_space.shape,\n",
        "            dtype=env.action_space.dtype,\n",
        "        )\n",
        "        low = self.observation_space.low[:24]\n",
        "        high = self.observation_space.high[:24]\n",
        "        self.observation_space = Box(low, high, dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, info = self.env.step(action)\n",
        "        obs = obs[:24]\n",
        "        return obs, reward, terminated, info\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        obs = obs[:24]\n",
        "        return obs\n",
        "\n",
        "    def action(self, action):\n",
        "        \"\"\"Rescales the action affinely from  [:attr:`min_action`, :attr:`max_action`] to the action space of the base environment, :attr:`env`.\n",
        "        Args:\n",
        "            action: The action to rescale\n",
        "        Returns:\n",
        "            The rescaled action\n",
        "        \"\"\"\n",
        "        assert np.all(np.greater_equal(action, self.min_action)), (\n",
        "            action,\n",
        "            self.min_action,\n",
        "        )\n",
        "        assert np.all(np.less_equal(action, self.max_action)), (action, self.max_action)\n",
        "        low = self.env.action_space.low\n",
        "        high = self.env.action_space.high\n",
        "        action = low + (high - low) * (\n",
        "            (action - self.min_action) / (self.max_action - self.min_action)\n",
        "        )\n",
        "        action = np.clip(action, low, high)\n",
        "        return action\n",
        "\n",
        "    def seed_everything(self, seed: int = 42):\n",
        "        gym.logger.set_level(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "env = CustomWrapper(gym.make(\"BipedalWalker-v3\"),  min_action = -1.0,  max_action = 1.0)\n",
        "env.seed_everything()\n",
        "\n",
        "agent = TD3Agent(state_size=env.observation_space.shape[0], \\\n",
        "                 action_size=env.action_space.shape[0], \\\n",
        "                 max_action=env.action_space.high, \\\n",
        "                 min_action=env.action_space.low, pretraining=PRETRAINED)\n"
      ],
      "metadata": {
        "id": "YZNQsiSqciQ5",
        "outputId": "ad5e0e97-5a69-4c9b-9463-492e1043a604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'wandb'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-10845c6e7de7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wandb'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train**"
      ],
      "metadata": {
        "id": "uV1KanhbJwfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"td3\")"
      ],
      "metadata": {
        "id": "d46eJqqklJvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERQ6QREQC99V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22104d53-10ef-4d01-cc52-8ee05d134d1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "  0%|          | 0/1599 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Learn Model [0/1599]:   0%|          | 1/1599 [00:13<5:47:03, 13.03s/it, actor_loss=57.5, critic_loss=0.479]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 10\tAverage Score: -107.02\tAverage Times: 7.97\tLoss Critic: 4.15\tLoss Actor: 62.59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 20\tAverage Score: -106.24\tAverage Times: 4.76\tLoss Critic: 3.90\tLoss Actor: 66.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 30\tAverage Score: -106.28\tAverage Times: 3.97\tLoss Critic: 3.26\tLoss Actor: 68.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 40\tAverage Score: -106.83\tAverage Times: 3.47\tLoss Critic: 4.25\tLoss Actor: 69.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 50\tAverage Score: -108.97\tAverage Times: 3.38\tLoss Critic: 5.28\tLoss Actor: 68.56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 60\tAverage Score: -110.31\tAverage Times: 4.18\tLoss Critic: 5.43\tLoss Actor: 67.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 70\tAverage Score: -110.84\tAverage Times: 3.98\tLoss Critic: 3.98\tLoss Actor: 67.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 80\tAverage Score: -110.51\tAverage Times: 4.32\tLoss Critic: 4.87\tLoss Actor: 66.50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 90\tAverage Score: -109.99\tAverage Times: 4.03\tLoss Critic: 4.36\tLoss Actor: 67.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 100\tAverage Score: -109.73\tAverage Times: 3.78\tLoss Critic: 5.34\tLoss Actor: 68.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 110\tAverage Score: -109.72\tAverage Times: 3.14\tLoss Critic: 6.68\tLoss Actor: 68.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 120\tAverage Score: -109.32\tAverage Times: 3.17\tLoss Critic: 5.38\tLoss Actor: 69.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 130\tAverage Score: -108.94\tAverage Times: 3.10\tLoss Critic: 5.68\tLoss Actor: 69.81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 140\tAverage Score: -108.28\tAverage Times: 3.09\tLoss Critic: 5.10\tLoss Actor: 70.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 150\tAverage Score: -106.71\tAverage Times: 2.98\tLoss Critic: 5.64\tLoss Actor: 71.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 160\tAverage Score: -105.17\tAverage Times: 2.35\tLoss Critic: 5.29\tLoss Actor: 71.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 170\tAverage Score: -104.13\tAverage Times: 2.25\tLoss Critic: 5.70\tLoss Actor: 71.69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 180\tAverage Score: -103.56\tAverage Times: 1.75\tLoss Critic: 5.41\tLoss Actor: 72.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 190\tAverage Score: -103.23\tAverage Times: 1.77\tLoss Critic: 5.83\tLoss Actor: 72.50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 200\tAverage Score: -102.69\tAverage Times: 1.81\tLoss Critic: 4.75\tLoss Actor: 72.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 210\tAverage Score: -102.32\tAverage Times: 1.82\tLoss Critic: 5.86\tLoss Actor: 73.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 220\tAverage Score: -102.35\tAverage Times: 1.87\tLoss Critic: 5.53\tLoss Actor: 73.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 230\tAverage Score: -102.50\tAverage Times: 1.93\tLoss Critic: 5.32\tLoss Actor: 73.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 240\tAverage Score: -103.36\tAverage Times: 2.65\tLoss Critic: 5.40\tLoss Actor: 72.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 250\tAverage Score: -103.81\tAverage Times: 2.70\tLoss Critic: 5.04\tLoss Actor: 73.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 260\tAverage Score: -106.26\tAverage Times: 2.89\tLoss Critic: 5.57\tLoss Actor: 72.81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 270\tAverage Score: -107.32\tAverage Times: 3.05\tLoss Critic: 6.09\tLoss Actor: 73.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 280\tAverage Score: -109.30\tAverage Times: 4.94\tLoss Critic: 6.38\tLoss Actor: inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 290\tAverage Score: -109.50\tAverage Times: 5.40\tLoss Critic: 6.84\tLoss Actor: 71.88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 300\tAverage Score: -111.27\tAverage Times: 8.11\tLoss Critic: 6.94\tLoss Actor: inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 310\tAverage Score: -112.28\tAverage Times: 12.09\tLoss Critic: 6.89\tLoss Actor: inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 320\tAverage Score: -113.04\tAverage Times: 16.36\tLoss Critic: 7.38\tLoss Actor: inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 330\tAverage Score: -112.67\tAverage Times: 17.20\tLoss Critic: 8.01\tLoss Actor: 72.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 340\tAverage Score: -112.79\tAverage Times: 18.75\tLoss Critic: 8.48\tLoss Actor: 72.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 350\tAverage Score: -114.52\tAverage Times: 19.75\tLoss Critic: 7.63\tLoss Actor: 73.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 360\tAverage Score: -114.22\tAverage Times: 19.73\tLoss Critic: 7.91\tLoss Actor: 73.62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 370\tAverage Score: -113.10\tAverage Times: 19.62\tLoss Critic: 8.26\tLoss Actor: 74.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 380\tAverage Score: -113.19\tAverage Times: 17.90\tLoss Critic: 7.32\tLoss Actor: 73.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 390\tAverage Score: -115.35\tAverage Times: 17.66\tLoss Critic: 8.57\tLoss Actor: 73.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 400\tAverage Score: -115.33\tAverage Times: 15.57\tLoss Critic: 9.08\tLoss Actor: 73.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 410\tAverage Score: -114.45\tAverage Times: 11.63\tLoss Critic: 8.00\tLoss Actor: 74.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 420\tAverage Score: -115.28\tAverage Times: 7.49\tLoss Critic: 7.20\tLoss Actor: 73.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 430\tAverage Score: -118.28\tAverage Times: 7.68\tLoss Critic: 4.64\tLoss Actor: 73.88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 440\tAverage Score: -120.07\tAverage Times: 5.62\tLoss Critic: 7.90\tLoss Actor: 74.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 450\tAverage Score: -118.45\tAverage Times: 4.59\tLoss Critic: 7.18\tLoss Actor: 74.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 460\tAverage Score: -118.07\tAverage Times: 4.57\tLoss Critic: 6.98\tLoss Actor: 73.81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 470\tAverage Score: -120.94\tAverage Times: 5.22\tLoss Critic: 7.09\tLoss Actor: 73.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 480\tAverage Score: -120.04\tAverage Times: 8.69\tLoss Critic: 7.72\tLoss Actor: 72.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 490\tAverage Score: -119.26\tAverage Times: 9.13\tLoss Critic: 4.65\tLoss Actor: 72.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 500\tAverage Score: -118.33\tAverage Times: 10.37\tLoss Critic: 7.05\tLoss Actor: inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 510\tAverage Score: -117.88\tAverage Times: 14.91\tLoss Critic: 7.00\tLoss Actor: inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 520\tAverage Score: -116.75\tAverage Times: 19.22\tLoss Critic: 7.19\tLoss Actor: inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 530\tAverage Score: -114.90\tAverage Times: 18.24\tLoss Critic: 5.60\tLoss Actor: 72.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 540\tAverage Score: -113.01\tAverage Times: 18.52\tLoss Critic: 5.50\tLoss Actor: 72.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 550\tAverage Score: -114.46\tAverage Times: 20.87\tLoss Critic: 8.05\tLoss Actor: 71.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 560\tAverage Score: -113.92\tAverage Times: 25.15\tLoss Critic: 6.92\tLoss Actor: inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 570\tAverage Score: -111.86\tAverage Times: 24.94\tLoss Critic: 6.34\tLoss Actor: 70.81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 580\tAverage Score: -112.02\tAverage Times: 22.24\tLoss Critic: 6.90\tLoss Actor: inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Learn Model [1197/1599]:  75%|███████▍  | 1198/1599 [00:42<00:30, 12.97it/s, actor_loss=69, critic_loss=18.4]"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def train(n_episodes=2000, max_t=2000, use_wandb=False):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    times_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    solved = False\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        start_time = time.time()\n",
        "        for t in range(max_t):\n",
        "            action = agent.predict(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            if done or t==(max_t-1):\n",
        "                critic_loss, actor_loss, q, max = agent.learn(t, i_episode)\n",
        "                break\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        scores_deque.append(score)\n",
        "        times_deque.append(duration)\n",
        "        scores.append(score)\n",
        "        mean_score = np.mean(scores_deque)\n",
        "        mean_times = np.mean(times_deque)\n",
        "\n",
        "        if use_wandb:\n",
        "            wandb.log({'Score': mean_score, 'Critic loss': critic_loss, 'Actor loss': actor_loss, 'Average Q': q, 'Max. Q': max, \"Duration \": mean_times}, step=i_episode)\n",
        "        if i_episode % 10 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tAverage Times: {:.2f}\\tLoss Critic: {:.2f}\\tLoss Actor: {:.2f}'.format(i_episode, mean_score, mean_times, critic_loss, actor_loss))\n",
        "        if i_episode % 500 == 0:\n",
        "            agent.save(\"checkpoint\", str(i_episode))\n",
        "        if mean_score >= 300 and solved == False:\n",
        "            print('\\rSolved at Episode {} !\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "            agent.save(\"checkpoint\")\n",
        "            solved = True\n",
        "\n",
        "    return scores\n",
        "\n",
        "scores = train(n_episodes=NUM_EPISODES, use_wandb=WANDB)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Result**"
      ],
      "metadata": {
        "id": "_GJ4lM-g8jB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "metadata": {
        "id": "iUJfDXmtMesi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "\n",
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "agent.actor.load_state_dict(torch.load('/content/checkpoint_actor_300.pth'))\n",
        "agent.critic.load_state_dict(torch.load('/content/checkpoint_critic_300.pth'))\n",
        "agent.actor_optimizer.load_state_dict(torch.load('/content/checkpoint_actor_optimizer_300.pth'))\n",
        "agent.critic_optimizer.load_state_dict(torch.load('/content/checkpoint_critic_optimizer_300.pth'))\n",
        "\n",
        "env = gym.make('BipedalWalker-v3')\n",
        "state = env.reset()\n",
        "score = 0\n",
        "img = plt.imshow(env.render('rgb_array'))\n",
        "while True:\n",
        "    img.set_data(env.render('rgb_array'))\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    action = agent.predict(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if np.any(done):\n",
        "        break\n",
        "\n",
        "print(\"Score: {}\".format(score))"
      ],
      "metadata": {
        "id": "IXJHQR6N8uwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test**"
      ],
      "metadata": {
        "id": "pt4M5an2gPmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestModel(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.env = gym.make(\"BipedalWalker-v3\")\n",
        "\n",
        "        # param model and buffer\n",
        "        self.batch_size = 100\n",
        "        self.buffer_size = int(1e5)\n",
        "        self.random_seed = 0\n",
        "        self.error = 0\n",
        "\n",
        "        # size action / state\n",
        "        self.action_size = self.env.action_space.shape[0]\n",
        "        self.state_size = self.env.observation_space.shape[0]\n",
        "\n",
        "        # min / max action\n",
        "        self.min_action = self.env.action_space.low\n",
        "        self.max_action = self.env.action_space.high\n",
        "\n",
        "        # min / max state\n",
        "        self.min_state = 0\n",
        "        self.max_state = 1\n",
        "\n",
        "        # min / max reward\n",
        "        self.min_reward = -300\n",
        "        self.max_reward = 300\n",
        "\n",
        "        self.model = TD3Agent(state_size=self.state_size, action_size=self.action_size, \\\n",
        "                         max_action=self.max_action, min_action=self.min_action, random_seed=self.random_seed)\n",
        "\n",
        "        self.memory = ReplayBufferPer(self.buffer_size)\n",
        "\n",
        "        # param number tests\n",
        "        self.num_attempts = 150\n",
        "\n",
        "    def _randomStates(self):\n",
        "        states = np.array([random.uniform(self.min_state, self.max_state) for _ in range(24)], dtype=np.float32)\n",
        "        return states\n",
        "\n",
        "    def _randomAction(self):\n",
        "        action = np.random.uniform(self.min_action, self.max_action, self.action_size)\n",
        "        return action\n",
        "\n",
        "    def _randomDone(self):\n",
        "        done = random.choice([True, False])\n",
        "        return done\n",
        "\n",
        "    def _randomReward(self):\n",
        "        reward = random.randint(self.min_reward, self.max_reward)\n",
        "        return reward\n",
        "\n",
        "    def test_predict_act(self):\n",
        "        \"\"\" Teste para verificar se o estado de saida da rede contem os valores minimos e maximos de ação que o ambiente exigi\n",
        "\n",
        "            Input: numpy.ndarray [24]\n",
        "            output: numpy.ndarray [4]\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        for _ in range(self.num_attempts):\n",
        "            states = self._randomStates()\n",
        "            action = self.model.predict(states)\n",
        "            is_valid = (isinstance(action, np.ndarray) and np.all(action >= self.min_action) and np.all(action <= self.max_action))\n",
        "\n",
        "            if not is_valid:\n",
        "                self.fail(\"Teste falhou na tentativa {}\".format(_ + 1))\n",
        "\n",
        "    def test_buffer_type(self):\n",
        "        while True:\n",
        "          next_state, reward, done, action, state = self._randomStates(), self._randomReward(), self._randomDone(), self._randomAction(), self._randomStates()\n",
        "\n",
        "          self.memory.add((state, action, reward, next_state, done), reward)\n",
        "\n",
        "          if len(self.memory) > self.batch_size:\n",
        "              break\n",
        "\n",
        "\n",
        "        (states, actions, rewards, next_states, dones), idxs, is_weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        self.assertIsInstance(states, torch.Tensor)\n",
        "        self.assertIsInstance(actions, torch.Tensor)\n",
        "        self.assertIsInstance(rewards, torch.Tensor)\n",
        "        self.assertIsInstance(next_states, torch.Tensor)\n",
        "        self.assertIsInstance(dones, torch.Tensor)\n",
        "\n",
        "    def test_buffer_size(self):\n",
        "        while True:\n",
        "          next_state, reward, done, action, state = self._randomStates(), self._randomReward(), self._randomDone(), self._randomAction(), self._randomStates()\n",
        "          self.memory.add((state, action, reward, next_state, done), reward)\n",
        "\n",
        "          if len(self.memory) > self.batch_size:\n",
        "            break\n",
        "\n",
        "        (states, actions, rewards, next_states, dones), idxs, is_weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        expected_batch_size = self.batch_size\n",
        "        self.assertEqual(states.size(0), expected_batch_size)\n",
        "        self.assertEqual(actions.size(0), expected_batch_size)\n",
        "        self.assertEqual(rewards.size(0), expected_batch_size)\n",
        "        self.assertEqual(next_states.size(0), expected_batch_size)\n",
        "        self.assertEqual(dones.size(0), expected_batch_size)\n",
        "\n",
        "    def test_buffer_range(self):\n",
        "        while True:\n",
        "          next_state, reward, done, action, state = self._randomStates(), self._randomReward(), self._randomDone(), self._randomAction(), self._randomStates()\n",
        "          self.memory.add((state, action, reward, next_state, done), reward)\n",
        "\n",
        "          if len(self.memory) > self.batch_size:\n",
        "            break\n",
        "\n",
        "        (states, actions, rewards, next_states, dones), idxs, weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        self.assertTrue(np.all(states[1].cpu().data.numpy() >= self.min_state) and np.all(states[1].cpu().data.numpy() <= self.max_state))\n",
        "        self.assertTrue(np.all(actions[1].cpu().data.numpy() >= self.min_action) and np.all(actions[1].cpu().data.numpy() <= self.max_action))\n",
        "        self.assertTrue(np.all(rewards[1].cpu().data.numpy() >= self.min_reward) and np.all(rewards[1].cpu().data.numpy() <= self.max_reward))\n",
        "        self.assertTrue(np.all(next_states[1].cpu().data.numpy() >= self.min_state) and np.all(next_states[1].cpu().data.numpy() <= self.max_state))\n",
        "        self.assertTrue(np.all(dones.cpu().data.numpy() >= 0.) and np.all(dones.cpu().data.numpy() <= 1.))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "id": "huHvCkTggSDs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gkNza9le_wuU",
        "ZADS3Hz_NdzM",
        "_GJ4lM-g8jB9",
        "pt4M5an2gPmn"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicolasalan/td3/blob/main/vault.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUkL1MTSs3Nn"
      },
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s53BlzVCk8g_",
        "outputId": "ad7112a6-daf5-4367-b7e4-2fddf08a68a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 31 23:27:30 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAddaBONmh68"
      },
      "source": [
        "Adicionar script ao console desse navegador: `inspecionar` => `console` => `adicionar script`.\n",
        "```\n",
        "function ConnectButton(){\n",
        "    console.log(\"Conectado\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkNza9le_wuU"
      },
      "source": [
        "# **Install**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7Qa_nNJfpVwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb60831-3865-419d-9b59-82a2078b2a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 0s (8,170 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 120874 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349143 sha256=3ccd62d7bf0db82a004a5bb76acef587c2b15dcf5e144f106ce1beda85a80e8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.1.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.33.1-py2.py3-none-any.whl (243 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=d827f097c571f80a454b4edade8f2135babeed9f023dfb1a0dc95def3cb6191a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 pathtools-0.1.2 sentry-sdk-1.33.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.15.12\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install swig\n",
        "\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]\n",
        "\n",
        "!pip install torch\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAatMkwQCxk5"
      },
      "source": [
        "# **TD3 - Twin Delayed DDPG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PhKTN31AC0QQ"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "import torch\n",
        "import wandb\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import gymnasium as gym\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from numpy import inf\n",
        "from collections import namedtuple, deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HH0qgyzCC1r7"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 100        # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR_ACTOR = 1e-3         # learning rate of the actor\n",
        "LR_CRITIC = 1e-3        # learning rate of the critic\n",
        "UPDATE_EVERY_STEP = 2   # how often to update the target and actor networks\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-PIsAAovC3pL"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "O7pJLds6C4gq"
      },
      "outputs": [],
      "source": [
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size, max_action, seed, l1=400, l2=300):\n",
        "        super(Actor, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "\n",
        "        self.l1 = nn.Linear(state_size, l1)\n",
        "        self.l2 = nn.Linear(l1, l2)\n",
        "        self.l3 = nn.Linear(l2, action_size)\n",
        "        self.reset_parameters()\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.l1.weight.data.uniform_(*hidden_init(self.l1))\n",
        "        self.l2.weight.data.uniform_(*hidden_init(self.l2))\n",
        "        self.l3.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.l1(state))\n",
        "        x = F.relu(self.l2(x))\n",
        "        action = self.max_action * torch.tanh(self.l3(x))\n",
        "        return action\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, seed, l1=400, l2=300):\n",
        "        super(Critic, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, l1)\n",
        "        self.l2 = nn.Linear(l1, l2)\n",
        "        self.l3 = nn.Linear(action_dim, l2)\n",
        "        self.l4 = nn.Linear(l2, 1)\n",
        "        self.reset_parameters_q1()\n",
        "\n",
        "        self.l5 = nn.Linear(state_dim + action_dim, l1)\n",
        "        self.l6 = nn.Linear(l1, l2)\n",
        "        self.l7 = nn.Linear(action_dim, l2)\n",
        "        self.l8 = nn.Linear(l2, 1)\n",
        "        self.reset_parameters_q2()\n",
        "\n",
        "    def reset_parameters_q1(self):\n",
        "        self.l1.weight.data.uniform_(*hidden_init(self.l1))\n",
        "        self.l2.weight.data.uniform_(*hidden_init(self.l2))\n",
        "        self.l3.weight.data.uniform_(*hidden_init(self.l3))\n",
        "        self.l4.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def reset_parameters_q2(self):\n",
        "        self.l5.weight.data.uniform_(*hidden_init(self.l5))\n",
        "        self.l6.weight.data.uniform_(*hidden_init(self.l6))\n",
        "        self.l7.weight.data.uniform_(*hidden_init(self.l7))\n",
        "        self.l8.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        s = torch.cat([state, action], dim=1)\n",
        "\n",
        "        s1 = F.relu(self.l1(s))\n",
        "        s1 = F.relu(self.l2(s1))\n",
        "        a1 = F.relu(self.l3(action))\n",
        "        s1 = s1 + a1\n",
        "        q1 = self.l4(s1)\n",
        "\n",
        "        s2 = F.relu(self.l5(s))\n",
        "        s2 = F.relu(self.l6(s2))\n",
        "        a2 = F.relu(self.l7(action))\n",
        "        s2 = s2 + a2\n",
        "        q2 = self.l8(s2)\n",
        "        return q1, q2\n",
        "\n",
        "    def Q1(self, state, action):\n",
        "        s = torch.cat([state, action], dim=1)\n",
        "\n",
        "        s1 = F.relu(self.l1(s))\n",
        "        s1 = F.relu(self.l2(s1))\n",
        "        a1 = F.relu(self.l3(action))\n",
        "        s1 = s1 + a1\n",
        "        q1 = self.l4(s1)\n",
        "        return q1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v3Cd3SiWC5Y5"
      },
      "outputs": [],
      "source": [
        "class TD3Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, max_action, min_action, random_seed, noise=0.2, noise_std=0.1, noise_clip=0.5):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            max_action (ndarray): the maximum valid value for each action vector\n",
        "            min_action (ndarray): the minimum valid value for each action vector\n",
        "            random_seed (int): random seed\n",
        "            noise (float): the range to generate random noise while learning\n",
        "            noise_std (float): the range to generate random noise while performing action\n",
        "            noise_clip (float): to clip random noise into this range\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.max_action = max_action\n",
        "        self.min_action = min_action\n",
        "        self.noise = noise\n",
        "        self.noise_std = noise_std\n",
        "        self.noise_clip = noise_clip\n",
        "\n",
        "        # Actor Network (w/ Target Network)\n",
        "        self.actor = Actor(state_size, action_size, float(max_action[0]), random_seed).to(device)\n",
        "        self.actor_target = Actor(state_size, action_size, float(max_action[0]), random_seed).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
        "\n",
        "        # Critic Network (w/ Target Network)\n",
        "        self.critic = Critic(state_size, action_size, random_seed).to(device)\n",
        "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Save experience in replay memory\"\"\"\n",
        "        if isinstance(state, np.ndarray):\n",
        "            states = state\n",
        "        elif isinstance(state, tuple):\n",
        "            states = np.array(state[0], dtype=np.float32)\n",
        "\n",
        "        self.memory.add(states, action, reward, next_state, done)\n",
        "\n",
        "    def predict(self, states):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        if isinstance(states, np.ndarray):\n",
        "            convert_state = states\n",
        "        elif isinstance(states, tuple):\n",
        "            convert_state = np.array(states[0], dtype=np.float32)\n",
        "\n",
        "        state = torch.from_numpy(convert_state).float().to(device)\n",
        "\n",
        "        self.actor.eval()\n",
        "        with torch.no_grad():\n",
        "            action = self.actor(state).cpu().data.numpy()\n",
        "\n",
        "        self.actor.train()\n",
        "\n",
        "        return action.clip(self.min_action[0], self.max_action[0])\n",
        "\n",
        "    def learn(self, n_iteraion, gamma=GAMMA):\n",
        "        \"\"\" Update policy and value parameters using given batch of experience tuples.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            n_iteraion (int): the number of iterations to train network\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "\n",
        "        if len(self.memory) > BATCH_SIZE:\n",
        "            average_Q = 0\n",
        "            max_Q = -inf\n",
        "            average_critic_loss = 0\n",
        "            average_actor_loss = 0\n",
        "\n",
        "            for i in range(n_iteraion):\n",
        "                state, action, reward, next_state, done = self.memory.sample()\n",
        "\n",
        "                action_ = action.cpu().numpy()\n",
        "\n",
        "                # ---------------------------- update critic ---------------------------- #\n",
        "                # Get predicted next-state actions and Q values from target models\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    # Generate a random noise\n",
        "                    noise = torch.FloatTensor(action_).data.normal_(0, self.noise).to(device)\n",
        "                    noise = noise.clamp(-self.noise_clip, self.noise_clip)\n",
        "                    actions_next = (self.actor_target(next_state) + noise).clamp(self.min_action[0].astype(float), self.max_action[0].astype(float))\n",
        "\n",
        "                    Q1_targets_next, Q2_targets_next = self.critic_target(next_state, actions_next)\n",
        "\n",
        "                    Q_targets_next = torch.min(Q1_targets_next, Q2_targets_next)\n",
        "\n",
        "                    average_Q += torch.mean(Q_targets_next)\n",
        "                    max_Q = max(max_Q, torch.max(Q_targets_next))\n",
        "\n",
        "                    # Compute Q targets for current states (y_i)\n",
        "                    Q_targets = reward + (gamma * Q_targets_next * (1 - done)).detach()\n",
        "\n",
        "                # Compute critic loss\n",
        "                Q1_expected, Q2_expected = self.critic(state, action)\n",
        "                critic_loss = F.mse_loss(Q1_expected, Q_targets) + F.mse_loss(Q2_expected, Q_targets)\n",
        "\n",
        "                # Minimize the loss\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                if i % UPDATE_EVERY_STEP == 0:\n",
        "                    # ---------------------------- update actor ---------------------------- #\n",
        "                    # Compute actor loss\n",
        "                    actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\n",
        "                    # Minimize the loss\n",
        "                    self.actor_optimizer.zero_grad()\n",
        "                    actor_loss.backward()\n",
        "                    self.actor_optimizer.step()\n",
        "\n",
        "                    # ----------------------- update target networks ----------------------- #\n",
        "                    self.soft_update(self.critic, self.critic_target, TAU)\n",
        "                    self.soft_update(self.actor, self.actor_target, TAU)\n",
        "\n",
        "                average_critic_loss += critic_loss\n",
        "                average_actor_loss += actor_loss\n",
        "\n",
        "            loss_critic = average_critic_loss / n_iteraion\n",
        "            loss_actor = average_actor_loss / n_iteraion\n",
        "            average_policy = average_Q / n_iteraion\n",
        "            max_policy = max_Q\n",
        "\n",
        "            return loss_critic, loss_actor, average_policy, max_policy\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        Params\n",
        "        ======\n",
        "            local_model: PyTorch model (weights will be copied from)\n",
        "            target_model: PyTorch model (weights will be copied to)\n",
        "            tau (float): interpolation parameter\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "    def save(self, filename):\n",
        "          \"\"\" Save the model \"\"\"\n",
        "          torch.save(self.critic.state_dict(), filename + \"_critic\")\n",
        "          torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
        "\n",
        "          torch.save(self.actor.state_dict(), filename + \"_actor\")\n",
        "          torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
        "\n",
        "    def load(self, filename):\n",
        "          \"\"\" Load the model \"\"\"\n",
        "          self.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
        "          self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
        "          self.critic_target = copy.deepcopy(self.critic)\n",
        "\n",
        "          self.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
        "          self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
        "          self.actor_target = copy.deepcopy(self.actor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9tSI3g3C9K0",
        "outputId": "8eadc581-8b96-41f8-bc9b-6afb22a6e146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho de cada ação: 4\n",
            "Ligação superior de cada ação: 1.0\n",
            "Ligação inferior de cada ação: -1.0\n",
            "Cada um observa um estado com comprimento: 24\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"BipedalWalker-v3\")\n",
        "\n",
        "# tamanho de cada ação\n",
        "action_size = env.action_space\n",
        "print('Tamanho de cada ação:', action_size.shape[0])\n",
        "\n",
        "# vínculo superior de cada ação\n",
        "upper_bond = env.action_space.high\n",
        "print('Ligação superior de cada ação:', upper_bond[0])\n",
        "\n",
        "# vínculo inferior de cada ação\n",
        "lower_bond = env.action_space.low\n",
        "print('Ligação inferior de cada ação:', lower_bond[0])\n",
        "\n",
        "#examina o espaço de estados\n",
        "states = env.observation_space\n",
        "state_size = states.shape[0]\n",
        "print('Cada um observa um estado com comprimento: {}'.format(state_size))\n",
        "\n",
        "agent = TD3Agent(state_size=env.observation_space.shape[0], \\\n",
        "                 action_size=env.action_space.shape[0], \\\n",
        "                 max_action=env.action_space.high, \\\n",
        "                 min_action=env.action_space.low, random_seed=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "RVNLbR1PNH44",
        "outputId": "1554da00-3d6c-4496-e2bb-c594de206767"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231031_232937-ryligrio</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/grottimeireles/td3/runs/ryligrio' target=\"_blank\">supernatural-specter-45</a></strong> to <a href='https://wandb.ai/grottimeireles/td3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/grottimeireles/td3' target=\"_blank\">https://wandb.ai/grottimeireles/td3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/grottimeireles/td3/runs/ryligrio' target=\"_blank\">https://wandb.ai/grottimeireles/td3/runs/ryligrio</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/grottimeireles/td3/runs/ryligrio?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7ac0a2412770>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"td3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "ERQ6QREQC99V",
        "outputId": "7665aab8-742d-4e44-fdf4-59824e7280f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: -224.58\n",
            "Episode 134\tAverage Score: -225.91\tScore: -225.89"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-be671b825f9a>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-be671b825f9a>\u001b[0m in \u001b[0;36mtd3\u001b[0;34m(n_episodes, max_t)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-cadadbfc961a>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, n_iteraion, gamma)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iteraion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0maction_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f6bc2bb6b6cf>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_vhstack_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def td3(n_episodes=2000, max_t=2000):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    times_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    solved = False\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        start_time = time.time()\n",
        "        for t in range(max_t):\n",
        "            action = agent.predict(state)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            if done or t==(max_t-1):\n",
        "                critic_loss, actor_loss, q, max = agent.learn(t)\n",
        "                break\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        scores_deque.append(score)\n",
        "        times_deque.append(duration)\n",
        "        scores.append(score)\n",
        "        mean_score = np.mean(scores_deque)\n",
        "        mean_times = np.mean(times_deque)\n",
        "\n",
        "        wandb.log({'Score': mean_score, 'Critic loss': critic_loss, 'Actor loss': actor_loss, 'Average Q': q, 'Max. Q': max, \"Duration \": mean_times}, step=i_episode)\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, mean_score, score), end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "        if mean_score >= 300 and solved == False:\n",
        "            solved = True\n",
        "            print('\\rSolved at Episode {} !\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "\n",
        "            agent.save(\"checkpoint\")\n",
        "\n",
        "    return scores\n",
        "\n",
        "scores = td3()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Result**"
      ],
      "metadata": {
        "id": "_GJ4lM-g8jB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalações para ver o agente\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "metadata": {
        "id": "p7xRX7rx8l1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "\n",
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "agent.actor_local.load_state_dict(torch.load('actor_checkpoint.pth'))\n",
        "agent.critic_local.load_state_dict(torch.load('critic_checkpoint.pth'))\n",
        "\n",
        "env = gym.make('BipedalWalker-v3')\n",
        "state = env.reset()\n",
        "score = 0\n",
        "img = plt.imshow(env.render('rgb_array'))\n",
        "while True:\n",
        "    img.set_data(env.render('rgb_array'))\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    action = agent.predict(state)\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if np.any(done):\n",
        "        break\n",
        "\n",
        "print(\"Score: {}\".format(score))"
      ],
      "metadata": {
        "id": "IXJHQR6N8uwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Custom Env**"
      ],
      "metadata": {
        "id": "5xJXZ1fl8mCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pygame\n",
        "import math\n",
        "\n",
        "screen_width = 1500\n",
        "screen_height = 800\n",
        "check_point = ((1200, 660), (1250, 120), (190, 200), (1030, 270), (250, 475), (650, 690))\n",
        "\n",
        "class Car:\n",
        "    def __init__(self, car_file, map_file, pos):\n",
        "        self.surface = pygame.image.load(car_file)\n",
        "        self.map = pygame.image.load(map_file)\n",
        "        self.surface = pygame.transform.scale(self.surface, (100, 100))\n",
        "        self.rotate_surface = self.surface\n",
        "        self.pos = pos\n",
        "        self.angle = 0\n",
        "        self.speed = 0\n",
        "        self.center = [self.pos[0] + 50, self.pos[1] + 50]\n",
        "        self.radars = []\n",
        "        self.radars_for_draw = []\n",
        "        self.is_alive = True\n",
        "        self.current_check = 0\n",
        "        self.prev_distance = 0\n",
        "        self.cur_distance = 0\n",
        "        self.goal = False\n",
        "        self.check_flag = False\n",
        "        self.distance = 0\n",
        "        self.time_spent = 0\n",
        "        for d in range(-90, 120, 45):\n",
        "            self.check_radar(d)\n",
        "\n",
        "        for d in range(-90, 120, 45):\n",
        "            self.check_radar_for_draw(d)\n",
        "\n",
        "    def draw(self, screen):\n",
        "        screen.blit(self.rotate_surface, self.pos)\n",
        "\n",
        "    def draw_collision(self, screen):\n",
        "        for i in range(4):\n",
        "            x = int(self.four_points[i][0])\n",
        "            y = int(self.four_points[i][1])\n",
        "            pygame.draw.circle(screen, (255, 255, 255), (x, y), 5)\n",
        "\n",
        "    def draw_radar(self, screen):\n",
        "        for r in self.radars_for_draw:\n",
        "            pos, dist = r\n",
        "            pygame.draw.line(screen, (0, 255, 0), self.center, pos, 1)\n",
        "            pygame.draw.circle(screen, (0, 255, 0), pos, 5)\n",
        "\n",
        "    def check_collision(self):\n",
        "        self.is_alive = True\n",
        "        for p in self.four_points:\n",
        "            if self.map.get_at((int(p[0]), int(p[1]))) == (255, 255, 255, 255):\n",
        "                self.is_alive = False\n",
        "                break\n",
        "\n",
        "    def check_radar(self, degree):\n",
        "        len = 0\n",
        "        x = int(self.center[0] + math.cos(math.radians(360 - (self.angle + degree))) * len)\n",
        "        y = int(self.center[1] + math.sin(math.radians(360 - (self.angle + degree))) * len)\n",
        "\n",
        "        while not self.map.get_at((x, y)) == (255, 255, 255, 255) and len < 300:\n",
        "            len = len + 1\n",
        "            x = int(self.center[0] + math.cos(math.radians(360 - (self.angle + degree))) * len)\n",
        "            y = int(self.center[1] + math.sin(math.radians(360 - (self.angle + degree))) * len)\n",
        "\n",
        "        dist = int(math.sqrt(math.pow(x - self.center[0], 2) + math.pow(y - self.center[1], 2)))\n",
        "        self.radars.append([(x, y), dist])\n",
        "\n",
        "\n",
        "    def check_radar_for_draw(self, degree):\n",
        "        len = 0\n",
        "        x = int(self.center[0] + math.cos(math.radians(360 - (self.angle + degree))) * len)\n",
        "        y = int(self.center[1] + math.sin(math.radians(360 - (self.angle + degree))) * len)\n",
        "\n",
        "        while not self.map.get_at((x, y)) == (255, 255, 255, 255) and len < 300:\n",
        "            len = len + 1\n",
        "            x = int(self.center[0] + math.cos(math.radians(360 - (self.angle + degree))) * len)\n",
        "            y = int(self.center[1] + math.sin(math.radians(360 - (self.angle + degree))) * len)\n",
        "\n",
        "        dist = int(math.sqrt(math.pow(x - self.center[0], 2) + math.pow(y - self.center[1], 2)))\n",
        "        self.radars_for_draw.append([(x, y), dist])\n",
        "\n",
        "    def check_checkpoint(self):\n",
        "        p = check_point[self.current_check]\n",
        "        self.prev_distance = self.cur_distance\n",
        "        dist = get_distance(p, self.center)\n",
        "        if dist < 70:\n",
        "            self.current_check += 1\n",
        "            self.prev_distance = 9999\n",
        "            self.check_flag = True\n",
        "            if self.current_check >= len(check_point):\n",
        "                self.current_check = 0\n",
        "                self.goal = True\n",
        "            else:\n",
        "                self.goal = False\n",
        "\n",
        "        self.cur_distance = dist\n",
        "\n",
        "    def update(self):\n",
        "        #check speed\n",
        "        self.speed -= 0.5\n",
        "        if self.speed > 10:\n",
        "            self.speed = 10\n",
        "        if self.speed < 1:\n",
        "            self.speed = 1\n",
        "\n",
        "        #check position\n",
        "        self.rotate_surface = rot_center(self.surface, self.angle)\n",
        "        self.pos[0] += math.cos(math.radians(360 - self.angle)) * self.speed\n",
        "        if self.pos[0] < 20:\n",
        "            self.pos[0] = 20\n",
        "        elif self.pos[0] > screen_width - 120:\n",
        "            self.pos[0] = screen_width - 120\n",
        "\n",
        "        self.distance += self.speed\n",
        "        self.time_spent += 1\n",
        "        self.pos[1] += math.sin(math.radians(360 - self.angle)) * self.speed\n",
        "        if self.pos[1] < 20:\n",
        "            self.pos[1] = 20\n",
        "        elif self.pos[1] > screen_height - 120:\n",
        "            self.pos[1] = screen_height - 120\n",
        "\n",
        "        # caculate 4 collision points\n",
        "        self.center = [int(self.pos[0]) + 50, int(self.pos[1]) + 50]\n",
        "        len = 40\n",
        "        left_top = [self.center[0] + math.cos(math.radians(360 - (self.angle + 30))) * len, self.center[1] + math.sin(math.radians(360 - (self.angle + 30))) * len]\n",
        "        right_top = [self.center[0] + math.cos(math.radians(360 - (self.angle + 150))) * len, self.center[1] + math.sin(math.radians(360 - (self.angle + 150))) * len]\n",
        "        left_bottom = [self.center[0] + math.cos(math.radians(360 - (self.angle + 210))) * len, self.center[1] + math.sin(math.radians(360 - (self.angle + 210))) * len]\n",
        "        right_bottom = [self.center[0] + math.cos(math.radians(360 - (self.angle + 330))) * len, self.center[1] + math.sin(math.radians(360 - (self.angle + 330))) * len]\n",
        "        self.four_points = [left_top, right_top, left_bottom, right_bottom]\n",
        "\n",
        "class PyGame2D:\n",
        "    def __init__(self):\n",
        "        pygame.init()\n",
        "        self.screen = pygame.display.set_mode((screen_width, screen_height))\n",
        "        self.clock = pygame.time.Clock()\n",
        "        self.font = pygame.font.SysFont(\"Arial\", 30)\n",
        "        self.car = Car('car.png', 'map.png', [700, 650])\n",
        "        self.game_speed = 60\n",
        "        self.mode = 0\n",
        "\n",
        "    def action(self, action):\n",
        "        if action == 0:\n",
        "            self.car.speed += 2\n",
        "        if action == 1:\n",
        "            self.car.angle += 5\n",
        "        elif action == 2:\n",
        "            self.car.angle -= 5\n",
        "\n",
        "        self.car.update()\n",
        "        self.car.check_collision()\n",
        "        self.car.check_checkpoint()\n",
        "\n",
        "        self.car.radars.clear()\n",
        "        for d in range(-90, 120, 45):\n",
        "            self.car.check_radar(d)\n",
        "\n",
        "    def evaluate(self):\n",
        "        reward = 0\n",
        "        \"\"\"\n",
        "        if self.car.check_flag:\n",
        "            self.car.check_flag = False\n",
        "            reward = 2000 - self.car.time_spent\n",
        "            self.car.time_spent = 0\n",
        "        \"\"\"\n",
        "        if not self.car.is_alive:\n",
        "            reward = -10000 + self.car.distance\n",
        "\n",
        "        elif self.car.goal:\n",
        "            reward = 10000\n",
        "        return reward\n",
        "\n",
        "    def is_done(self):\n",
        "        if not self.car.is_alive or self.car.goal:\n",
        "            self.car.current_check = 0\n",
        "            self.car.distance = 0\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def observe(self):\n",
        "        # return state\n",
        "        radars = self.car.radars\n",
        "        ret = [0, 0, 0, 0, 0]\n",
        "        for i, r in enumerate(radars):\n",
        "            ret[i] = int(r[1] / 30)\n",
        "\n",
        "        return tuple(ret)\n",
        "\n",
        "    def view(self):\n",
        "        # draw game\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT:\n",
        "                done = True\n",
        "            elif event.type == pygame.KEYDOWN:\n",
        "                if event.key == pygame.K_m:\n",
        "                    self.mode += 1\n",
        "                    self.mode = self.mode % 3\n",
        "\n",
        "        self.screen.blit(self.car.map, (0, 0))\n",
        "\n",
        "\n",
        "        if self.mode == 1:\n",
        "            self.screen.fill((0, 0, 0))\n",
        "\n",
        "        self.car.radars_for_draw.clear()\n",
        "        for d in range(-90, 120, 45):\n",
        "            self.car.check_radar_for_draw(d)\n",
        "\n",
        "        pygame.draw.circle(self.screen, (255, 255, 0), check_point[self.car.current_check], 70, 1)\n",
        "        self.car.draw_collision(self.screen)\n",
        "        self.car.draw_radar(self.screen)\n",
        "        self.car.draw(self.screen)\n",
        "\n",
        "\n",
        "        text = self.font.render(\"Press 'm' to change view mode\", True, (255, 255, 0))\n",
        "        text_rect = text.get_rect()\n",
        "        text_rect.center = (screen_width/2, 100)\n",
        "        self.screen.blit(text, text_rect)\n",
        "\n",
        "\n",
        "\n",
        "        pygame.display.flip()\n",
        "        self.clock.tick(self.game_speed)\n",
        "\n",
        "\n",
        "def get_distance(p1, p2):\n",
        "\treturn math.sqrt(math.pow((p1[0] - p2[0]), 2) + math.pow((p1[1] - p2[1]), 2))\n",
        "\n",
        "def rot_center(image, angle):\n",
        "    orig_rect = image.get_rect()\n",
        "    rot_image = pygame.transform.rotate(image, angle)\n",
        "    rot_rect = orig_rect.copy()\n",
        "    rot_rect.center = rot_image.get_rect().center\n",
        "    rot_image = rot_image.subsurface(rot_rect).copy()\n",
        "    return rot_image\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "\n",
        "class CustomEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.pygame = PyGame2D()\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        self.observation_space = spaces.Box(np.array([0, 0, 0, 0, 0]), np.array([10, 10, 10, 10, 10]), dtype=np.int)\n",
        "\n",
        "    def reset(self):\n",
        "        del self.pygame\n",
        "        self.pygame = PyGame2D()\n",
        "        obs = self.pygame.observe()\n",
        "        return obs\n",
        "\n",
        "    def step(self, action):\n",
        "        self.pygame.action(action)\n",
        "        obs = self.pygame.observe()\n",
        "        reward = self.pygame.evaluate()\n",
        "        done = self.pygame.is_done()\n",
        "        return obs, reward, done, {}\n",
        "\n",
        "    def render(self, mode=\"human\", close=False):\n",
        "        self.pygame.view()"
      ],
      "metadata": {
        "id": "B_HMmQ2k8m-C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qUkL1MTSs3Nn",
        "gkNza9le_wuU",
        "_GJ4lM-g8jB9",
        "5xJXZ1fl8mCL"
      ],
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMYTMSNZ1rLsD7+V95SIMut",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
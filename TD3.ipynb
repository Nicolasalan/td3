{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicolasalan/td3/blob/main/TD3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUkL1MTSs3Nn"
      },
      "source": [
        "## **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdruBNVvMQ08",
        "outputId": "692e8844-b881-47ca-e9b0-4b9fc3cbca11"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add script in browser console: `inspect` => `console` => add script.\n",
        "\n",
        "```javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Conectado\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```"
      ],
      "metadata": {
        "id": "iSmLN3pgE2Fu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkNza9le_wuU"
      },
      "source": [
        "## **Install**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "7Qa_nNJfpVwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23edaeed-215c-4fd2-edeb-e857783704aa",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.3.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install swig\n",
        "\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]\n",
        "\n",
        "# !pip install torch\n",
        "# !pip install matplotlib\n",
        "# !pip install numpy\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "i1A9WpNXPh06",
        "outputId": "016309b5-1dc5-4e2b-c216-010f9d114022"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightning in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.11.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.25.2)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.0)\n",
            "Requirement already satisfied: torch<4.0,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.4.0.post0)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.11.0)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=1.13.0->lightning) (12.5.40)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import**"
      ],
      "metadata": {
        "id": "BwxK-YPh0EKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import torch\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch versions not as required, installing nightly versions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjrS5wB7AOfA",
        "outputId": "ba3a5779-b54e-440f-c457-7df4e6a31058"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.3.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we're using a NVIDIA GPU\n",
        "if torch.cuda.is_available():\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find(\"failed\") >= 0:\n",
        "    print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n",
        "\n",
        "  # Get GPU name\n",
        "  gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "  gpu_name = gpu_name[1]\n",
        "  GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving\n",
        "  print(f'GPU name: {GPU_NAME}')\n",
        "\n",
        "  # Get GPU capability score\n",
        "  GPU_SCORE = torch.cuda.get_device_capability()\n",
        "  print(f\"GPU capability score: {GPU_SCORE}\")\n",
        "  if GPU_SCORE >= (8, 0):\n",
        "    print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")\n",
        "  else:\n",
        "    print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")\n",
        "\n",
        "  # Print GPU info\n",
        "  print(f\"GPU information:\\n{gpu_info}\")\n",
        "\n",
        "else:\n",
        "  print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFiWTJo9CzSf",
        "outputId": "fae74709-ecd6-446d-a772-2eddb9cf1638"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check available GPU memory and total GPU memory\n",
        "try:\n",
        "  total_free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\n",
        "  print(f\"Total free GPU memory: {round(total_free_gpu_memory * 1e-9, 3)} GB\")\n",
        "  print(f\"Total GPU memory: {round(total_gpu_memory * 1e-9, 3)} GB\")\n",
        "except:\n",
        "  print(\"Please check that you have an NVIDIA GPU and installed a driver from \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13d_P3rnC1HJ",
        "outputId": "521fa5bb-3f06-465e-ce0e-7bb8d3cbca7b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please check that you have an NVIDIA GPU and installed a driver from \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size depending on amount of GPU memory\n",
        "try:\n",
        "  total_free_gpu_memory_gb = round(total_free_gpu_memory * 1e-9, 3)\n",
        "  if total_free_gpu_memory_gb >= 16:\n",
        "    BATCH_SIZE = 128 # Note: you could experiment with higher values here if you like.\n",
        "    print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE}\")\n",
        "  else:\n",
        "    BATCH_SIZE = 32\n",
        "    print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE}\")\n",
        "except:\n",
        "  BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "Y2qHR2UKDojx"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "HH0qgyzCC1r7"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 100        # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR_ACTOR = 1e-3         # learning rate of the actor\n",
        "LR_CRITIC = 1e-3        # learning rate of the critic\n",
        "UPDATE_EVERY_STEP = 2   # how often to update the target and actor networks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Replay**"
      ],
      "metadata": {
        "id": "ZADS3Hz_NdzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple, deque\n",
        "from typing import Tuple\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, buffer_size: int, batch_size: int):\n",
        "\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    def add(self, state: np.ndarray, action: np.ndarray, reward: np.float64, next_state: np.float32, done: bool) -> None:\n",
        "        \"\"\"Add experiences to the buffer\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state (np.ndarray): agent states\n",
        "            action (np.ndarray): agent action\n",
        "            reward (np.float64): agent reward\n",
        "            next_state (np.ndarray): agent next_state\n",
        "        \"\"\"\n",
        "\n",
        "        assert isinstance(state, np.ndarray), \"State is not of data structure (np.ndarray) in REPLAY BUFFER -> state: {}.\".format(type(state))\n",
        "        assert isinstance(action, np.ndarray), \"Action is not of data structure (np.ndarray) in REPLAY BUFFER -> action: {}.\".format(type(action))\n",
        "        assert isinstance(next_state, np.ndarray), \"Next State is not of data structure (np.ndarray) in REPLAY BUFFER -> next state: {}.\".format(type(next_state))\n",
        "\n",
        "        assert isinstance(state[0], np.float32), \"State is not of type (np.float32) in REPLAY BUFFER -> state type: {}.\".format(type(state))\n",
        "        assert isinstance(action[0], np.float32), \"Action is not of type (np.float32) in REPLAY BUFFER -> action type: {}.\".format(type(action))\n",
        "        assert isinstance(reward, (int, np.float64)), \"Reward is not of type (np.float64 / int) in REPLAY BUFFER -> reward: {}.\".format(type(reward))\n",
        "        assert isinstance(next_state[0], np.float32), \"Next State is not of type (np.float32) in REPLAY BUFFER -> next state type: {}.\".format(type(next_state))\n",
        "        assert isinstance(done, bool), \"Done is not of type (bool) in REPLAY BUFFER -> done type: {}.\".format(type(done))\n",
        "\n",
        "        assert state.shape[0] == 24, \"The size of the state is not (24) in REPLAY BUFFER -> state size: {}.\".format(state.shape[0])\n",
        "        assert action.shape[0] == 4, \"The size of the action is not (4) in REPLAY BUFFER -> action size: {}.\".format(state.shape[0])\n",
        "        if isinstance(reward, np.float64):\n",
        "          assert reward.size == 1, \"The size of the reward is not (1) in REPLAY BUFFER -> reward size: {}.\".format(reward.size)\n",
        "        assert next_state.shape[0] == 24, \"The size of the next_state is not (24) in REPLAY BUFFER -> next_state size: {}.\".format(next_state.shape[0])\n",
        "\n",
        "        assert state.ndim == 1, \"The ndim of the state is not (1) in REPLAY BUFFER -> state ndim: {}.\".format(state.ndim)\n",
        "        assert action.ndim == 1, \"The ndim of the action is not (1) in REPLAY BUFFER -> action ndim: {}.\".format(state.ndim)\n",
        "        if isinstance(reward, np.float64):\n",
        "          assert reward.ndim == 0, \"The ndim of the reward is not (0) in REPLAY BUFFER -> reward ndim: {}.\".format(reward.ndim)\n",
        "        assert next_state.ndim == 1, \"The ndim of the next_state is not (1) in REPLAY BUFFER -> next_state ndim: {}.\".format(next_state.ndim)\n",
        "\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None])).int().to(device)\n",
        "\n",
        "        assert isinstance(states, torch.Tensor), \"State is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(actions, torch.Tensor), \"Actions is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(rewards, torch.Tensor), \"Rewards is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(next_states, torch.Tensor), \"Next states is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(dones, torch.Tensor), \"Dones is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "\n",
        "        assert states.dtype == torch.float32, \"The (state) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(states.dtype)\n",
        "        assert actions.dtype == torch.float32,\"The (actions) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(actions.dtype)\n",
        "        assert rewards.dtype == torch.float32, \"The (rewards) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(rewards.dtype)\n",
        "        assert next_states.dtype == torch.float32, \"The (next_states) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(next_states.dtype)\n",
        "        assert dones.dtype == torch.int, \"The (dones) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(dones.dtype)\n",
        "\n",
        "        # TODO\n",
        "        # assert all(tensor.device.type == DEVICE for tensor in [states, actions, rewards, next_states, dones]), \"Each tensor must be on the same device in REPLAY BUFFER\"\n",
        "\n",
        "        return (\n",
        "            states, actions, rewards, next_states, dones\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> None:\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "Eqt5PjlK6aAQ"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import IterableDataset\n",
        "from typing import Iterator\n",
        "\n",
        "class RLDataset(IterableDataset):\n",
        "    \"\"\"Iterable Dataset containing the ExperienceBuffer which will be updated with new experiences during training.\n",
        "\n",
        "    >>> RLDataset(ReplayBuffer(5))  # doctest: +ELLIPSIS\n",
        "    <...reinforce_learn_Qnet.RLDataset object at ...>\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, buffer: ReplayBuffer, sample_size: int = 200) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            buffer: replay buffer\n",
        "            sample_size: number of experiences to sample at a time\n",
        "        \"\"\"\n",
        "        self.buffer = buffer\n",
        "        self.sample_size = sample_size\n",
        "\n",
        "    def __iter__(self) -> Iterator:\n",
        "        states, actions, rewards, dones, new_states = self.buffer.sample()\n",
        "        for i in range(len(dones)):\n",
        "            yield states[i], actions[i], rewards[i], dones[i], new_states[i]\n"
      ],
      "metadata": {
        "id": "fK7XfBvrS8dU"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "8wHw8OOk38ps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "O7pJLds6C4gq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size: int, action_size: int, max_action: float, l1=400, l2=300):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_size, l1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l1, l2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l2, action_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.reset_parameters()\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.net[0].weight.data.uniform_(*hidden_init(self.net[0]))\n",
        "        self.net[2].weight.data.uniform_(*hidden_init(self.net[2]))\n",
        "        self.net[4].weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state) -> torch.Tensor:\n",
        "        assert isinstance(state, torch.Tensor), \"State is not of type torch.Tensor in ACTOR.\"\n",
        "        assert state.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in ACTOR.\"\n",
        "        assert state.shape[0] <= 24 or state.shape[0] >= BATCH_SIZE, \"The tensor shape is not torch.Size([24]) in ACTOR.\"\n",
        "        assert str(state.device.type) == str(DEVICE), \"The state must be on the same device in ACTOR.\"\n",
        "\n",
        "        # x = self.net(state)\n",
        "        # action = self.max_action * x\n",
        "        return self.net(state)\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, l1=400, l2=300):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # Critic Q1\n",
        "        self.net1 = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, l1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l1, l2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l2, 1)\n",
        "        )\n",
        "\n",
        "        # Critic Q2\n",
        "        self.net2 = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, l1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l1, l2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l2, 1)\n",
        "        )\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.net1[0].weight.data.uniform_(*hidden_init(self.net1[0]))\n",
        "        self.net1[2].weight.data.uniform_(*hidden_init(self.net1[2]))\n",
        "        self.net1[4].weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "        self.net2[0].weight.data.uniform_(*hidden_init(self.net2[0]))\n",
        "        self.net2[2].weight.data.uniform_(*hidden_init(self.net2[2]))\n",
        "        self.net2[4].weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state, action) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        assert isinstance(state, torch.Tensor), \"State is not of type torch.Tensor in CRITIC.\"\n",
        "        assert state.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert state.shape[0] == BATCH_SIZE, \"The tensor shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(state.device.type) == str(DEVICE), \"The state must be on the same device in CRITIC.\"\n",
        "\n",
        "        assert isinstance(action, torch.Tensor), \"Action is not of type torch.Tensor in CRITIC.\"\n",
        "        assert action.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert action.shape[0] == BATCH_SIZE, \"The action shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(action.device.type) == str(DEVICE), \"The action must be on the same device in CRITIC.\"\n",
        "\n",
        "        sa = torch.cat([state, action], dim=1)\n",
        "\n",
        "        return self.net1(sa), self.net2(sa)\n",
        "\n",
        "    def Q1(self, state, action) -> torch.Tensor:\n",
        "        assert isinstance(state, torch.Tensor), \"State is not of type torch.Tensor in CRITIC.\"\n",
        "        assert state.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert state.shape[0] == BATCH_SIZE, \"The tensor shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(state.device.type) == str(DEVICE), \"The state must be on the same device in CRITIC.\"\n",
        "\n",
        "        assert isinstance(action, torch.Tensor), \"Action is not of type torch.Tensor in CRITIC.\"\n",
        "        assert action.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert action.shape[0] == BATCH_SIZE, \"The action shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(action.device.type) == str(DEVICE), \"The action must be on the same device in CRITIC.\"\n",
        "\n",
        "        sa = torch.cat([state, action], dim=1)\n",
        "\n",
        "        return self.net1(sa)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Wrapper**"
      ],
      "metadata": {
        "id": "z3eHfN0cpzU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import wandb\n",
        "from typing import Union\n",
        "from gym import spaces\n",
        "from gym.spaces import Box\n",
        "\n",
        "class CustomWrapper(gym.Wrapper):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        min_action: Union[float, int, np.ndarray],\n",
        "        max_action: Union[float, int, np.ndarray],\n",
        "    ):\n",
        "        \"\"\"Initializes the :class:`RescaleAction` wrapper.\n",
        "        Args:\n",
        "            env (Env): The environment to apply the wrapper\n",
        "            min_action (float, int or np.ndarray): The min values for each action. This may be a numpy array or a scalar.\n",
        "            max_action (float, int or np.ndarray): The max values for each action. This may be a numpy array or a scalar.\n",
        "        \"\"\"\n",
        "        assert isinstance(\n",
        "            env.action_space, spaces.Box\n",
        "        ), f\"expected Box action space, got {type(env.action_space)}\"\n",
        "        assert np.less_equal(min_action, max_action).all(), (min_action, max_action)\n",
        "\n",
        "        super().__init__(env)\n",
        "        self.min_action = (\n",
        "            np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + min_action\n",
        "        )\n",
        "        self.max_action = (\n",
        "            np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + max_action\n",
        "        )\n",
        "        self.action_space = spaces.Box(\n",
        "            low=min_action,\n",
        "            high=max_action,\n",
        "            shape=env.action_space.shape,\n",
        "            dtype=env.action_space.dtype,\n",
        "        )\n",
        "        low = self.observation_space.low[:24]\n",
        "        high = self.observation_space.high[:24]\n",
        "        self.observation_space = Box(low, high, dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, info = self.env.step(action)\n",
        "        obs = obs[:24]\n",
        "        return obs, reward, terminated, info\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        obs = obs[:24]\n",
        "        return obs\n",
        "\n",
        "    def action(self, action):\n",
        "        \"\"\"Rescales the action affinely from  [:attr:`min_action`, :attr:`max_action`] to the action space of the base environment, :attr:`env`.\n",
        "        Args:\n",
        "            action: The action to rescale\n",
        "        Returns:\n",
        "            The rescaled action\n",
        "        \"\"\"\n",
        "        assert np.all(np.greater_equal(action, self.min_action)), (\n",
        "            action,\n",
        "            self.min_action,\n",
        "        )\n",
        "        assert np.all(np.less_equal(action, self.max_action)), (action, self.max_action)\n",
        "        low = self.env.action_space.low\n",
        "        high = self.env.action_space.high\n",
        "        action = low + (high - low) * (\n",
        "            (action - self.min_action) / (self.max_action - self.min_action)\n",
        "        )\n",
        "        action = np.clip(action, low, high)\n",
        "        return action\n",
        "\n",
        "    def seed(self, seed):\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "# gym.logger.set_level(40)\n",
        "# env = CustomWrapper(gym.make(\"BipedalWalker-v3\"),  min_action = -1.0,  max_action = 1.0)\n",
        "# env.seed(0)\n",
        "\n",
        "# agent = TD3Agent(state_size=env.observation_space.shape[0], \\\n",
        "#                  action_size=env.action_space.shape[0], \\\n",
        "#                  max_action=env.action_space.high, \\\n",
        "#                  min_action=env.action_space.low, continue_training=False)\n"
      ],
      "metadata": {
        "id": "YZNQsiSqciQ5"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Agent**"
      ],
      "metadata": {
        "id": "JPdq-Y3U3_0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "v3Cd3SiWC5Y5"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "from numpy import inf\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, env: gym.Env, replay_buffer: ReplayBuffer, max_action, min_action) -> None:\n",
        "        \"\"\"Initialize an Agent object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            max_action (ndarray): the maximum valid value for each action vector\n",
        "            min_action (ndarray): the minimum valid value for each action vector\n",
        "            noise (float): the range to generate random noise while learning\n",
        "            noise_std (float): the range to generate random noise while performing action\n",
        "            noise_clip (float): to clip random noise into this range\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.reset()\n",
        "        self.state = self.env.reset()\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.min_action = min_action\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = replay_buffer\n",
        "\n",
        "    def action(self, actor, device) -> np.ndarray:\n",
        "\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        state = torch.tensor([self.state])\n",
        "\n",
        "        if device not in [\"cpu\"]:\n",
        "          state = state.cuda(device).cpu().data.numpy()\n",
        "\n",
        "        action = actor(state).cpu().data.numpy()\n",
        "\n",
        "        action = action.clip(self.min_action[0], self.max_action[0])\n",
        "\n",
        "        return action\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        \"\"\"Resets the environment and updates the state.\"\"\"\n",
        "        self.state = self.env.reset()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, actor: nn.Module, device: str = \"cpu\") -> Tuple[float, bool]:\n",
        "        action = self.action(actor, device)\n",
        "\n",
        "        next_state, reward, done, _ = self.env.step(action[0])\n",
        "\n",
        "        self.memory.add(self.state, action[0], reward, next_state, done)\n",
        "\n",
        "        self.state = next_state\n",
        "\n",
        "        if done:\n",
        "            self.reset()\n",
        "        return reward, done\n",
        "\n",
        "env = CustomWrapper(gym.make(\"BipedalWalker-v3\"),  min_action = -1.0,  max_action = 1.0)\n",
        "memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
        "agent = Agent(env=env, replay_buffer=memory, max_action=env.action_space.high, min_action=env.action_space.low)\n",
        "actor = Actor(env.observation_space.shape[0], env.action_space.shape[0], float(env.action_space.high[0])).to(device)\n",
        "\n",
        "for i in range(10):\n",
        "  agent.step(actor)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, List, Tuple\n",
        "from lightning.pytorch import LightningModule\n",
        "from collections import OrderedDict, deque, namedtuple\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "\n",
        "from numpy import inf\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "NOISE = 0.2\n",
        "NOISE_STD = 0.1\n",
        "NOISE_CLIP = 0.5\n",
        "\n",
        "class TD3Lightning(LightningModule):\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 state_size: int,\n",
        "                 action_size: int,\n",
        "                 max_action: int,\n",
        "                 min_action: int,\n",
        "                 sync_rate: int = 10,\n",
        "                 lr: float = 1e-2,\n",
        "                 batch_size: int = 100,\n",
        "                 episode_length: int = 50,\n",
        "                 warm_start_steps: int = 200):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            max_action (ndarray): the maximum valid value for each action vector\n",
        "            min_action (ndarray): the minimum valid value for each action vector\n",
        "            noise (float): the range to generate random noise while learning\n",
        "            noise_std (float): the range to generate random noise while performing action\n",
        "            noise_clip (float): to clip random noise into this range\n",
        "        \"\"\"\n",
        "        super(TD3Lightning, self).__init__()\n",
        "        self.warm_start_steps = warm_start_steps\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.max_action = max_action\n",
        "        self.min_action = min_action\n",
        "        self.env = env\n",
        "        self.nb_optim_iters = 4,\n",
        "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
        "        self.agent = Agent(self.env, self.memory, self.max_action, self.min_action)\n",
        "\n",
        "        self.total_reward = 0\n",
        "        self.episode_reward = 0\n",
        "        self.lr = lr\n",
        "        self.sync_rate = sync_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.episode_length = episode_length\n",
        "\n",
        "        # Set the device globally\n",
        "        torch.set_default_device(device)\n",
        "\n",
        "        # Transfer Learning\n",
        "        self.actor = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "        self.actor_target = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
        "\n",
        "        self.critic = Critic(state_size, action_size).to(device)\n",
        "        self.critic_target = Critic(state_size, action_size).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
        "\n",
        "        self.populate(self.warm_start_steps)\n",
        "\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "\n",
        "    def populate(self, steps: int = 1000) -> None:\n",
        "        \"\"\"Carries out several random steps through the environment to initially fill up the replay buffer with\n",
        "        experiences.\n",
        "\n",
        "        Args:\n",
        "            steps: number of random steps to populate the buffer with\n",
        "\n",
        "        \"\"\"\n",
        "        for i in range(steps):\n",
        "            self.agent.step(self.actor)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Passes in a state x through the network and returns the policy and a sampled action\n",
        "        Args:\n",
        "            x: environment state\n",
        "        Returns:\n",
        "            Tuple of policy and action\n",
        "        \"\"\"\n",
        "        action = self.actor(state)\n",
        "        action = action * self.max_action\n",
        "        Q1_expected, Q2_expected = self.critic(state, action)\n",
        "\n",
        "        return action, Q1_expected, Q2_expected\n",
        "\n",
        "    def actor_loss(self, state, action) -> torch.Tensor:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\n",
        "        return actor_loss\n",
        "\n",
        "    def critic_loss(self, Q1_expected, Q2_expected, Q_targets) -> torch.Tensor:\n",
        "\n",
        "        critic_loss = F.mse_loss(Q1_expected, Q_targets) + F.mse_loss(Q2_expected, Q_targets)\n",
        "\n",
        "        return critic_loss\n",
        "\n",
        "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx):\n",
        "        \"\"\" Update policy and value parameters using given batch of experience tuples.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            n_iteraion (int): the number of iterations to train network\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "\n",
        "        device = self.get_device(batch)\n",
        "\n",
        "        if len(self.memory) > BATCH_SIZE:\n",
        "            average_critic_loss = 0\n",
        "            average_actor_loss = 0\n",
        "\n",
        "            state, action, reward, next_state, done = batch\n",
        "\n",
        "            action_ = action.cpu().numpy()\n",
        "\n",
        "            # ---------------------------- update critic ---------------------------- #\n",
        "            # Get predicted next-state actions and Q values from target models\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                # Generate a random noise\n",
        "                noise = torch.FloatTensor(action_).data.normal_(0, NOISE).to(device)\n",
        "                noise = noise.clamp(-NOISE_CLIP, NOISE_CLIP)\n",
        "                actions_next = (self.actor_target(next_state) + noise).clamp(self.min_action[0].astype(float), self.max_action[0].astype(float))\n",
        "\n",
        "                Q1_targets_next, Q2_targets_next = self.critic_target(next_state, actions_next)\n",
        "\n",
        "                Q_targets_next = torch.min(Q1_targets_next, Q2_targets_next)\n",
        "\n",
        "                # Compute Q targets for current states (y_i)\n",
        "                Q_targets = reward + (GAMMA * Q_targets_next * (1 - done)).detach()\n",
        "\n",
        "            # Compute critic loss\n",
        "            Q1_expected, Q2_expected = self.critic(state, action)\n",
        "\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss = self.critic_loss(Q1_expected, Q2_expected, Q_targets)\n",
        "\n",
        "            self.manual_backward(critic_loss)\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "            if i % UPDATE_EVERY_STEP == 0:\n",
        "                # ---------------------------- update actor ---------------------------- #\n",
        "                # Compute actor loss\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\n",
        "                self.manual_backward(actor_loss)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                # ----------------------- update target networks ----------------------- #\n",
        "                self.soft_update(self.critic, self.critic_target, TAU)\n",
        "                self.soft_update(self.actor, self.actor_target, TAU)\n",
        "\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau) -> None:\n",
        "        \"\"\"Soft update model parameters.\n",
        "        _target = *_local + (1 - )*_target\n",
        "        Params\n",
        "        ======\n",
        "            local_model: PyTorch model (weights will be copied from)\n",
        "            target_model: PyTorch model (weights will be copied to)\n",
        "            tau (float): interpolation parameter\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "    def configure_optimizers(self) -> List[Optimizer]:\n",
        "        \"\"\" Initialize Adam optimizer\"\"\"\n",
        "        optimizer_actor = optim.Adam(self.actor.parameters(), lr=self.lr)\n",
        "        optimizer_critic = optim.Adam(self.critic.parameters(), lr=self.lr)\n",
        "\n",
        "        return optimizer_actor, optimizer_critic\n",
        "\n",
        "\n",
        "    def optimizer_step(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Run 'nb_optim_iters' number of iterations of gradient descent on actor and critic\n",
        "        for each data sample.\n",
        "        \"\"\"\n",
        "        for i in range(self.nb_optim_iters):\n",
        "            super().optimizer_step(*args, **kwargs)\n",
        "\n",
        "    def save(self, filename, version) -> None:\n",
        "          \"\"\" Save the model \"\"\"\n",
        "          torch.save(self.critic.state_dict(), filename + \"_critic_\" + version + \".pth\")\n",
        "          torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer_\" + version + \".pth\")\n",
        "\n",
        "          torch.save(self.actor.state_dict(), filename + \"_actor_\" + version + \".pth\")\n",
        "          torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer_\" + version + \".pth\")\n",
        "\n",
        "    def load(self, filename) -> None:\n",
        "          \"\"\" Load the model \"\"\"\n",
        "          self.critic.load_state_dict(torch.load(filename + \"_critic.pth\"))\n",
        "          self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer.pth\"))\n",
        "          self.critic_target = copy.deepcopy(self.critic)\n",
        "\n",
        "          self.actor.load_state_dict(torch.load(filename + \"_actor.pth\"))\n",
        "          self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer.pth\"))\n",
        "          self.actor_target = copy.deepcopy(self.actor)\n",
        "\n",
        "    def __dataloader(self) -> DataLoader:\n",
        "        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\n",
        "        dataset = RLDataset(self.memory, self.episode_length)\n",
        "        return DataLoader(dataset=dataset, batch_size=self.batch_size, sampler=None)\n",
        "\n",
        "    def train_dataloader(self) -> DataLoader:\n",
        "        \"\"\"Get train loader.\"\"\"\n",
        "        return self.__dataloader()\n",
        "\n",
        "    def get_device(self, batch) -> str:\n",
        "        \"\"\"Retrieve device currently being used by minibatch.\"\"\"\n",
        "        return batch[0].device.index if self.on_gpu else \"cpu\"\n",
        "\n",
        "\n",
        "from lightning.pytorch import Trainer, cli_lightning_logo, seed_everything\n",
        "\n",
        "def main() -> None:\n",
        "    env = CustomWrapper(gym.make(\"BipedalWalker-v3\"),  min_action = -1.0,  max_action = 1.0)\n",
        "    model = TD3Lightning(env=env,\n",
        "                         state_size=env.observation_space.shape[0],\n",
        "                         action_size=env.action_space.shape[0],\n",
        "                         max_action=env.action_space.high,\n",
        "                         min_action=env.action_space.low,\n",
        "                         sync_rate=10, lr=1e-2,\n",
        "                         episode_length=200,\n",
        "                         batch_size=100,\n",
        "                         warm_start_steps=1000)\n",
        "    trainer = Trainer(accelerator=\"cpu\", devices=1, val_check_interval=100, max_epochs=1000, logger=WandbLogger(log_model=\"all\"))\n",
        "    trainer.fit(model)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "yBsvSByhUh6x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868,
          "referenced_widgets": [
            "c10a1edd674647f1bd066720867c6f65",
            "485cfe24c99a41c681a237e47dd89fd2",
            "79f5a5705d7744b7b91795551227f893",
            "b0d5efa12adc49f3817ab9d97c04c5c4",
            "9bb4b52d2621453b8764421b2e160671",
            "33b0c2863e564b1f933d93b497582a3c",
            "845e47dd594346a785515a61551542ba",
            "04868daf91e843569e279305e4140e9f",
            "b20df300a76a4ce18d63f9916af08409",
            "1e701485d8ea4d378775ba76383e70a4",
            "b86d93e8d1a9455ca05b9e375dedd25f"
          ]
        },
        "outputId": "5a9ec6ef-7bde-43f0-e51e-ae08c0c4e062"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: False, used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>./wandb/run-20240526_042937-2arkb1id</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alangrotti/lightning_logs/runs/2arkb1id' target=\"_blank\">frosty-violet-45</a></strong> to <a href='https://wandb.ai/alangrotti/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alangrotti/lightning_logs' target=\"_blank\">https://wandb.ai/alangrotti/lightning_logs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alangrotti/lightning_logs/runs/2arkb1id' target=\"_blank\">https://wandb.ai/alangrotti/lightning_logs/runs/2arkb1id</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: \n",
            "  | Name          | Type   | Params\n",
            "-----------------------------------------\n",
            "0 | actor         | Actor  | 131 K \n",
            "1 | actor_target  | Actor  | 131 K \n",
            "2 | critic        | Critic | 264 K \n",
            "3 | critic_target | Critic | 264 K \n",
            "-----------------------------------------\n",
            "791 K     Trainable params\n",
            "0         Non-trainable params\n",
            "791 K     Total params\n",
            "3.167     Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type   | Params\n",
            "-----------------------------------------\n",
            "0 | actor         | Actor  | 131 K \n",
            "1 | actor_target  | Actor  | 131 K \n",
            "2 | critic        | Critic | 264 K \n",
            "3 | critic_target | Critic | 264 K \n",
            "-----------------------------------------\n",
            "791 K     Trainable params\n",
            "0         Non-trainable params\n",
            "791 K     Total params\n",
            "3.167     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c10a1edd674647f1bd066720867c6f65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: `Trainer.fit` stopped: `max_epochs=1000` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train**"
      ],
      "metadata": {
        "id": "uV1KanhbJwfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"td3\") # fb372890f5180a16a9cd2df5b9558e55493cd16c"
      ],
      "metadata": {
        "id": "d46eJqqklJvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERQ6QREQC99V"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def td3(n_episodes=2000, max_t=2000):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    times_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    solved = False\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        start_time = time.time()\n",
        "        for t in range(max_t):\n",
        "            action = agent.predict(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            if done or t==(max_t-1):\n",
        "                critic_loss, actor_loss, q, max = agent.learn(t)\n",
        "                break\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        scores_deque.append(score)\n",
        "        times_deque.append(duration)\n",
        "        scores.append(score)\n",
        "        mean_score = np.mean(scores_deque)\n",
        "        mean_times = np.mean(times_deque)\n",
        "\n",
        "        #wandb.log({'Score': mean_score, 'Critic loss': critic_loss, 'Actor loss': actor_loss, 'Average Q': q, 'Max. Q': max, \"Duration \": mean_times}, step=i_episode)\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, mean_score, score), end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "        if i_episode % 500 == 0:\n",
        "            agent.save(\"checkpoint\", str(i_episode))\n",
        "        if mean_score >= 300 and solved == False:\n",
        "            print('\\rSolved at Episode {} !\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "            agent.save(\"checkpoint\")\n",
        "            solved = True\n",
        "\n",
        "    return scores\n",
        "\n",
        "scores = td3()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Result**"
      ],
      "metadata": {
        "id": "_GJ4lM-g8jB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "metadata": {
        "id": "iUJfDXmtMesi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "\n",
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "agent.actor.load_state_dict(torch.load('/content/checkpoint_actor_300.pth'))\n",
        "agent.critic.load_state_dict(torch.load('/content/checkpoint_critic_300.pth'))\n",
        "agent.actor_optimizer.load_state_dict(torch.load('/content/checkpoint_actor_optimizer_300.pth'))\n",
        "agent.critic_optimizer.load_state_dict(torch.load('/content/checkpoint_critic_optimizer_300.pth'))\n",
        "\n",
        "env = gym.make('BipedalWalker-v3')\n",
        "state = env.reset()\n",
        "score = 0\n",
        "img = plt.imshow(env.render('rgb_array'))\n",
        "while True:\n",
        "    img.set_data(env.render('rgb_array'))\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    action = agent.predict(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if np.any(done):\n",
        "        break\n",
        "\n",
        "print(\"Score: {}\".format(score))"
      ],
      "metadata": {
        "id": "IXJHQR6N8uwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test**"
      ],
      "metadata": {
        "id": "pt4M5an2gPmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestModel(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.env = gym.make(\"BipedalWalker-v3\")\n",
        "\n",
        "        # param model and buffer\n",
        "        self.batch_size = 100\n",
        "        self.buffer_size = int(1e5)\n",
        "        self.random_seed = 0\n",
        "        self.error = 0\n",
        "\n",
        "        # size action / state\n",
        "        self.action_size = self.env.action_space.shape[0]\n",
        "        self.state_size = self.env.observation_space.shape[0]\n",
        "\n",
        "        # min / max action\n",
        "        self.min_action = self.env.action_space.low\n",
        "        self.max_action = self.env.action_space.high\n",
        "\n",
        "        # min / max state\n",
        "        self.min_state = 0\n",
        "        self.max_state = 1\n",
        "\n",
        "        # min / max reward\n",
        "        self.min_reward = -300\n",
        "        self.max_reward = 300\n",
        "\n",
        "        self.model = TD3Agent(state_size=self.state_size, action_size=self.action_size, \\\n",
        "                         max_action=self.max_action, min_action=self.min_action, random_seed=self.random_seed)\n",
        "\n",
        "        self.memory = ReplayBufferPer(self.buffer_size)\n",
        "\n",
        "        # param number tests\n",
        "        self.num_attempts = 150\n",
        "\n",
        "    def _randomStates(self):\n",
        "        states = np.array([random.uniform(self.min_state, self.max_state) for _ in range(24)], dtype=np.float32)\n",
        "        return states\n",
        "\n",
        "    def _randomAction(self):\n",
        "        action = np.random.uniform(self.min_action, self.max_action, self.action_size)\n",
        "        return action\n",
        "\n",
        "    def _randomDone(self):\n",
        "        done = random.choice([True, False])\n",
        "        return done\n",
        "\n",
        "    def _randomReward(self):\n",
        "        reward = random.randint(self.min_reward, self.max_reward)\n",
        "        return reward\n",
        "\n",
        "    def test_predict_act(self):\n",
        "        \"\"\" Teste para verificar se o estado de saida da rede contem os valores minimos e maximos de ao que o ambiente exigi\n",
        "\n",
        "            Input: numpy.ndarray [24]\n",
        "            output: numpy.ndarray [4]\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        for _ in range(self.num_attempts):\n",
        "            states = self._randomStates()\n",
        "            action = self.model.predict(states)\n",
        "            is_valid = (isinstance(action, np.ndarray) and np.all(action >= self.min_action) and np.all(action <= self.max_action))\n",
        "\n",
        "            if not is_valid:\n",
        "                self.fail(\"Teste falhou na tentativa {}\".format(_ + 1))\n",
        "\n",
        "    def test_buffer_type(self):\n",
        "        while True:\n",
        "          next_state, reward, done, action, state = self._randomStates(), self._randomReward(), self._randomDone(), self._randomAction(), self._randomStates()\n",
        "\n",
        "          self.memory.add((state, action, reward, next_state, done), reward)\n",
        "\n",
        "          if len(self.memory) > self.batch_size:\n",
        "              break\n",
        "\n",
        "\n",
        "        (states, actions, rewards, next_states, dones), idxs, is_weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        self.assertIsInstance(states, torch.Tensor)\n",
        "        self.assertIsInstance(actions, torch.Tensor)\n",
        "        self.assertIsInstance(rewards, torch.Tensor)\n",
        "        self.assertIsInstance(next_states, torch.Tensor)\n",
        "        self.assertIsInstance(dones, torch.Tensor)\n",
        "\n",
        "    def test_buffer_size(self):\n",
        "        while True:\n",
        "          next_state, reward, done, action, state = self._randomStates(), self._randomReward(), self._randomDone(), self._randomAction(), self._randomStates()\n",
        "          self.memory.add((state, action, reward, next_state, done), reward)\n",
        "\n",
        "          if len(self.memory) > self.batch_size:\n",
        "            break\n",
        "\n",
        "        (states, actions, rewards, next_states, dones), idxs, is_weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        expected_batch_size = self.batch_size\n",
        "        self.assertEqual(states.size(0), expected_batch_size)\n",
        "        self.assertEqual(actions.size(0), expected_batch_size)\n",
        "        self.assertEqual(rewards.size(0), expected_batch_size)\n",
        "        self.assertEqual(next_states.size(0), expected_batch_size)\n",
        "        self.assertEqual(dones.size(0), expected_batch_size)\n",
        "\n",
        "    def test_buffer_range(self):\n",
        "        while True:\n",
        "          next_state, reward, done, action, state = self._randomStates(), self._randomReward(), self._randomDone(), self._randomAction(), self._randomStates()\n",
        "          self.memory.add((state, action, reward, next_state, done), reward)\n",
        "\n",
        "          if len(self.memory) > self.batch_size:\n",
        "            break\n",
        "\n",
        "        (states, actions, rewards, next_states, dones), idxs, weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        self.assertTrue(np.all(states[1].cpu().data.numpy() >= self.min_state) and np.all(states[1].cpu().data.numpy() <= self.max_state))\n",
        "        self.assertTrue(np.all(actions[1].cpu().data.numpy() >= self.min_action) and np.all(actions[1].cpu().data.numpy() <= self.max_action))\n",
        "        self.assertTrue(np.all(rewards[1].cpu().data.numpy() >= self.min_reward) and np.all(rewards[1].cpu().data.numpy() <= self.max_reward))\n",
        "        self.assertTrue(np.all(next_states[1].cpu().data.numpy() >= self.min_state) and np.all(next_states[1].cpu().data.numpy() <= self.max_state))\n",
        "        self.assertTrue(np.all(dones.cpu().data.numpy() >= 0.) and np.all(dones.cpu().data.numpy() <= 1.))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "id": "huHvCkTggSDs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gkNza9le_wuU",
        "ZADS3Hz_NdzM",
        "8wHw8OOk38ps",
        "XXqRDsXVce1Y",
        "_GJ4lM-g8jB9",
        "pt4M5an2gPmn"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c10a1edd674647f1bd066720867c6f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_485cfe24c99a41c681a237e47dd89fd2",
              "IPY_MODEL_79f5a5705d7744b7b91795551227f893",
              "IPY_MODEL_b0d5efa12adc49f3817ab9d97c04c5c4"
            ],
            "layout": "IPY_MODEL_9bb4b52d2621453b8764421b2e160671"
          }
        },
        "485cfe24c99a41c681a237e47dd89fd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33b0c2863e564b1f933d93b497582a3c",
            "placeholder": "",
            "style": "IPY_MODEL_845e47dd594346a785515a61551542ba",
            "value": "Epoch999:"
          }
        },
        "79f5a5705d7744b7b91795551227f893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04868daf91e843569e279305e4140e9f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b20df300a76a4ce18d63f9916af08409",
            "value": 1
          }
        },
        "b0d5efa12adc49f3817ab9d97c04c5c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e701485d8ea4d378775ba76383e70a4",
            "placeholder": "",
            "style": "IPY_MODEL_b86d93e8d1a9455ca05b9e375dedd25f",
            "value": "1/?[00:00&lt;00:00,23.67it/s,v_num=b1id]"
          }
        },
        "9bb4b52d2621453b8764421b2e160671": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "33b0c2863e564b1f933d93b497582a3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "845e47dd594346a785515a61551542ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04868daf91e843569e279305e4140e9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b20df300a76a4ce18d63f9916af08409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e701485d8ea4d378775ba76383e70a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b86d93e8d1a9455ca05b9e375dedd25f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
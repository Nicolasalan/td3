{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicolasalan/td3/blob/main/TD3_official.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUkL1MTSs3Nn"
      },
      "source": [
        "## **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdruBNVvMQ08",
        "outputId": "6aaa8aa3-c036-42e6-989a-2efb1bd42f63"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add script in browser console: `inspect` => `console` => add script.\n",
        "\n",
        "```javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Conectado\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```\n",
        "\n",
        "**source:** https://github.com/sidhantls/ppo_lightning"
      ],
      "metadata": {
        "id": "iSmLN3pgE2Fu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkNza9le_wuU"
      },
      "source": [
        "## **Install**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7Qa_nNJfpVwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f15ddf3-968c-4cc0-dbe1-1feb8af90d65",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 0s (3,942 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349116 sha256=25a921eec213a18e4135110cfd39683bc40a47fa292e95604f69575b50c8a1db\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.2.1\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.3.1-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.3.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.0\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install swig\n",
        "\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]\n",
        "\n",
        "# !pip install torch\n",
        "# !pip install matplotlib\n",
        "# !pip install numpy\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "i1A9WpNXPh06",
        "outputId": "a4624a05-c40c-4661-9a1e-b60569ba3960"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning\n",
            "  Downloading lightning-2.2.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.25.2)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.0)\n",
            "Requirement already satisfied: torch<4.0,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.3.0+cu121)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.11.0)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.2.5-py3-none-any.whl (802 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.3/802.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<4.0,>=1.13.0->lightning)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<4.0,>=1.13.0->lightning)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<4.0,>=1.13.0->lightning)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<4.0,>=1.13.0->lightning)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<4.0,>=1.13.0->lightning)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<4.0,>=1.13.0->lightning)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<4.0,>=1.13.0->lightning)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<4.0,>=1.13.0->lightning)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<4.0,>=1.13.0->lightning)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch<4.0,>=1.13.0->lightning)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<4.0,>=1.13.0->lightning)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=1.13.0->lightning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.2.5 lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pytorch-lightning-2.2.5 torchmetrics-1.4.0.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import**"
      ],
      "metadata": {
        "id": "BwxK-YPh0EKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import torch\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch versions not as required, installing nightly versions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjrS5wB7AOfA",
        "outputId": "34a379d4-9f60-4f7a-8fb4-2cec948fe1c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.3.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we're using a NVIDIA GPU\n",
        "if torch.cuda.is_available():\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find(\"failed\") >= 0:\n",
        "    print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n",
        "\n",
        "  # Get GPU name\n",
        "  gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "  gpu_name = gpu_name[1]\n",
        "  GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving\n",
        "  print(f'GPU name: {GPU_NAME}')\n",
        "\n",
        "  # Get GPU capability score\n",
        "  GPU_SCORE = torch.cuda.get_device_capability()\n",
        "  print(f\"GPU capability score: {GPU_SCORE}\")\n",
        "  if GPU_SCORE >= (8, 0):\n",
        "    print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")\n",
        "  else:\n",
        "    print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")\n",
        "\n",
        "  # Print GPU info\n",
        "  print(f\"GPU information:\\n{gpu_info}\")\n",
        "\n",
        "else:\n",
        "  print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFiWTJo9CzSf",
        "outputId": "1c7bd249-a5e2-4e97-9cfd-fccaf31a9c08"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check available GPU memory and total GPU memory\n",
        "try:\n",
        "  total_free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\n",
        "  print(f\"Total free GPU memory: {round(total_free_gpu_memory * 1e-9, 3)} GB\")\n",
        "  print(f\"Total GPU memory: {round(total_gpu_memory * 1e-9, 3)} GB\")\n",
        "except:\n",
        "  print(\"Please check that you have an NVIDIA GPU and installed a driver from \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13d_P3rnC1HJ",
        "outputId": "0ac72d84-1507-4e06-a534-d832e6dda96b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please check that you have an NVIDIA GPU and installed a driver from \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size depending on amount of GPU memory\n",
        "try:\n",
        "  total_free_gpu_memory_gb = round(total_free_gpu_memory * 1e-9, 3)\n",
        "  if total_free_gpu_memory_gb >= 16:\n",
        "    BATCH_SIZE = 128 # Note: you could experiment with higher values here if you like.\n",
        "    print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE}\")\n",
        "  else:\n",
        "    BATCH_SIZE = 32\n",
        "    print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE}\")\n",
        "except:\n",
        "  BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "Y2qHR2UKDojx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HH0qgyzCC1r7"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 100        # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR_ACTOR = 1e-3         # learning rate of the actor\n",
        "LR_CRITIC = 1e-3        # learning rate of the critic\n",
        "UPDATE_EVERY_STEP = 2   # how often to update the target and actor networks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Replay**"
      ],
      "metadata": {
        "id": "ZADS3Hz_NdzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple, deque\n",
        "from typing import Tuple\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, buffer_size: int, batch_size: int):\n",
        "\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    def add(self, state: np.ndarray, action: np.ndarray, reward: np.float64, next_state: np.float32, done: bool) -> None:\n",
        "        \"\"\"Add experiences to the buffer\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state (np.ndarray): agent states\n",
        "            action (np.ndarray): agent action\n",
        "            reward (np.float64): agent reward\n",
        "            next_state (np.ndarray): agent next_state\n",
        "        \"\"\"\n",
        "\n",
        "        assert isinstance(next_state, np.ndarray), \"Next State is not of data structure (np.ndarray) in REPLAY BUFFER -> next state: {}.\".format(type(next_state))\n",
        "\n",
        "        assert isinstance(state[0], np.float32), \"State is not of type (np.float32) in REPLAY BUFFER -> state type: {}.\".format(type(state))\n",
        "        assert isinstance(action[0], np.float32), \"Action is not of type (np.float32) in REPLAY BUFFER -> action type: {}.\".format(type(action))\n",
        "        assert isinstance(reward, (int, np.float64)), \"Reward is not of type (np.float64 / int) in REPLAY BUFFER -> reward: {}.\".format(type(reward))\n",
        "        assert isinstance(next_state[0], np.float32), \"Next State is not of type (np.float32) in REPLAY BUFFER -> next state type: {}.\".format(type(next_state))\n",
        "\n",
        "        assert state.shape[0] == 24, \"The size of the state is not (24) in REPLAY BUFFER -> state size: {}.\".format(state.shape[0])\n",
        "        assert action.shape[0] == 4, \"The size of the action is not (4) in REPLAY BUFFER -> action size: {}.\".format(state.shape[0])\n",
        "        if isinstance(reward, np.float64):\n",
        "          assert reward.size == 1, \"The size of the reward is not (1) in REPLAY BUFFER -> reward size: {}.\".format(reward.size)\n",
        "        assert next_state.shape[0] == 24, \"The size of the next_state is not (24) in REPLAY BUFFER -> next_state size: {}.\".format(next_state.shape[0])\n",
        "\n",
        "        assert state.ndim == 1, \"The ndim of the state is not (1) in REPLAY BUFFER -> state ndim: {}.\".format(state.ndim)\n",
        "        assert action.ndim == 1, \"The ndim of the action is not (1) in REPLAY BUFFER -> action ndim: {}.\".format(state.ndim)\n",
        "        if isinstance(reward, np.float64):\n",
        "          assert reward.ndim == 0, \"The ndim of the reward is not (0) in REPLAY BUFFER -> reward ndim: {}.\".format(reward.ndim)\n",
        "        assert next_state.ndim == 1, \"The ndim of the next_state is not (1) in REPLAY BUFFER -> next_state ndim: {}.\".format(next_state.ndim)\n",
        "\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None])).int().to(device)\n",
        "\n",
        "        assert isinstance(states, torch.Tensor), \"State is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(actions, torch.Tensor), \"Actions is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(rewards, torch.Tensor), \"Rewards is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(next_states, torch.Tensor), \"Next states is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(dones, torch.Tensor), \"Dones is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "\n",
        "        assert states.dtype == torch.float32, \"The (state) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(states.dtype)\n",
        "        assert actions.dtype == torch.float32,\"The (actions) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(actions.dtype)\n",
        "        assert rewards.dtype == torch.float32, \"The (rewards) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(rewards.dtype)\n",
        "        assert next_states.dtype == torch.float32, \"The (next_states) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(next_states.dtype)\n",
        "        assert dones.dtype == torch.int, \"The (dones) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(dones.dtype)\n",
        "\n",
        "        # TODO\n",
        "        # assert all(tensor.device.type == DEVICE for tensor in [states, actions, rewards, next_states, dones]), \"Each tensor must be on the same device in REPLAY BUFFER\"\n",
        "\n",
        "        return (\n",
        "            states, actions, rewards, next_states, dones\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> None:\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "Eqt5PjlK6aAQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import IterableDataset\n",
        "from typing import Iterator\n",
        "\n",
        "class RLDataset(IterableDataset):\n",
        "    \"\"\"Iterable Dataset containing the ExperienceBuffer which will be updated with new experiences during training.\n",
        "\n",
        "    >>> RLDataset(ReplayBuffer(5))  # doctest: +ELLIPSIS\n",
        "    <...reinforce_learn_Qnet.RLDataset object at ...>\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, buffer: ReplayBuffer, sample_size: int = 200) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            buffer: replay buffer\n",
        "            sample_size: number of experiences to sample at a time\n",
        "        \"\"\"\n",
        "        self.buffer = buffer\n",
        "        self.sample_size = sample_size\n",
        "\n",
        "    def __iter__(self) -> Iterator:\n",
        "        states, actions, rewards, dones, new_states = self.buffer.sample()\n",
        "        for i in range(len(dones)):\n",
        "            yield states[i], actions[i], rewards[i], dones[i], new_states[i]\n",
        "\n",
        "from typing import Iterable, Callable\n",
        "\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "class ExperienceSourceDataset(IterableDataset):\n",
        "\n",
        "    def __init__(self, generate_batch: Callable):\n",
        "        self.generate_batch = generate_batch\n",
        "\n",
        "    def __iter__(self) -> Iterable:\n",
        "        iterator = self.generate_batch()\n",
        "        return iterator\n"
      ],
      "metadata": {
        "id": "fK7XfBvrS8dU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "8wHw8OOk38ps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O7pJLds6C4gq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size: int, action_size: int, max_action: float, l1=400, l2=300):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_size, l1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l1, l2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l2, action_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.reset_parameters()\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.net[0].weight.data.uniform_(*hidden_init(self.net[0]))\n",
        "        self.net[2].weight.data.uniform_(*hidden_init(self.net[2]))\n",
        "        self.net[4].weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state) -> torch.Tensor:\n",
        "        assert isinstance(state, torch.Tensor), \"State is not of type torch.Tensor in ACTOR.\"\n",
        "        assert state.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in ACTOR.\"\n",
        "        assert state.shape[0] <= 24 or state.shape[0] >= BATCH_SIZE, \"The tensor shape is not torch.Size([24]) in ACTOR.\"\n",
        "        assert str(state.device.type) == str(DEVICE), \"The state must be on the same device in ACTOR.\"\n",
        "\n",
        "        # x = self.net(state)\n",
        "        # action = self.max_action * x\n",
        "        return self.net(state)\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, l1=400, l2=300):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # Critic Q1\n",
        "        self.net1 = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, l1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l1, l2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l2, 1)\n",
        "        )\n",
        "\n",
        "        # Critic Q2\n",
        "        self.net2 = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, l1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l1, l2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(l2, 1)\n",
        "        )\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.net1[0].weight.data.uniform_(*hidden_init(self.net1[0]))\n",
        "        self.net1[2].weight.data.uniform_(*hidden_init(self.net1[2]))\n",
        "        self.net1[4].weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "        self.net2[0].weight.data.uniform_(*hidden_init(self.net2[0]))\n",
        "        self.net2[2].weight.data.uniform_(*hidden_init(self.net2[2]))\n",
        "        self.net2[4].weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state, action) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        assert isinstance(state, torch.Tensor), \"State is not of type torch.Tensor in CRITIC.\"\n",
        "        assert state.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert state.shape[0] == BATCH_SIZE, \"The tensor shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(state.device.type) == str(DEVICE), \"The state must be on the same device in CRITIC.\"\n",
        "\n",
        "        assert isinstance(action, torch.Tensor), \"Action is not of type torch.Tensor in CRITIC.\"\n",
        "        assert action.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert action.shape[0] == BATCH_SIZE, \"The action shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(action.device.type) == str(DEVICE), \"The action must be on the same device in CRITIC.\"\n",
        "\n",
        "        sa = torch.cat([state, action], dim=1)\n",
        "\n",
        "        return self.net1(sa), self.net2(sa)\n",
        "\n",
        "    def Q1(self, state, action) -> torch.Tensor:\n",
        "        assert isinstance(state, torch.Tensor), \"State is not of type torch.Tensor in CRITIC.\"\n",
        "        assert state.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert state.shape[0] == BATCH_SIZE, \"The tensor shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(state.device.type) == str(DEVICE), \"The state must be on the same device in CRITIC.\"\n",
        "\n",
        "        assert isinstance(action, torch.Tensor), \"Action is not of type torch.Tensor in CRITIC.\"\n",
        "        assert action.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert action.shape[0] == BATCH_SIZE, \"The action shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(action.device.type) == str(DEVICE), \"The action must be on the same device in CRITIC.\"\n",
        "\n",
        "        sa = torch.cat([state, action], dim=1)\n",
        "\n",
        "        return self.net1(sa)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Wrapper**"
      ],
      "metadata": {
        "id": "z3eHfN0cpzU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import wandb\n",
        "from typing import Union\n",
        "from gym import spaces\n",
        "from gym.spaces import Box\n",
        "\n",
        "class CustomWrapper(gym.Wrapper):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        min_action: Union[float, int, np.ndarray],\n",
        "        max_action: Union[float, int, np.ndarray],\n",
        "    ):\n",
        "        \"\"\"Initializes the :class:`RescaleAction` wrapper.\n",
        "        Args:\n",
        "            env (Env): The environment to apply the wrapper\n",
        "            min_action (float, int or np.ndarray): The min values for each action. This may be a numpy array or a scalar.\n",
        "            max_action (float, int or np.ndarray): The max values for each action. This may be a numpy array or a scalar.\n",
        "        \"\"\"\n",
        "        assert isinstance(\n",
        "            env.action_space, spaces.Box\n",
        "        ), f\"expected Box action space, got {type(env.action_space)}\"\n",
        "        assert np.less_equal(min_action, max_action).all(), (min_action, max_action)\n",
        "\n",
        "        super().__init__(env)\n",
        "        self.min_action = (\n",
        "            np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + min_action\n",
        "        )\n",
        "        self.max_action = (\n",
        "            np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + max_action\n",
        "        )\n",
        "        self.action_space = spaces.Box(\n",
        "            low=min_action,\n",
        "            high=max_action,\n",
        "            shape=env.action_space.shape,\n",
        "            dtype=env.action_space.dtype,\n",
        "        )\n",
        "        low = self.observation_space.low[:24]\n",
        "        high = self.observation_space.high[:24]\n",
        "        self.observation_space = Box(low, high, dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, info = self.env.step(action)\n",
        "        obs = obs[:24]\n",
        "        return obs, reward, terminated, info\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        obs = obs[:24]\n",
        "        return obs\n",
        "\n",
        "    def action(self, action):\n",
        "        \"\"\"Rescales the action affinely from  [:attr:`min_action`, :attr:`max_action`] to the action space of the base environment, :attr:`env`.\n",
        "        Args:\n",
        "            action: The action to rescale\n",
        "        Returns:\n",
        "            The rescaled action\n",
        "        \"\"\"\n",
        "        assert np.all(np.greater_equal(action, self.min_action)), (\n",
        "            action,\n",
        "            self.min_action,\n",
        "        )\n",
        "        assert np.all(np.less_equal(action, self.max_action)), (action, self.max_action)\n",
        "        low = self.env.action_space.low\n",
        "        high = self.env.action_space.high\n",
        "        action = low + (high - low) * (\n",
        "            (action - self.min_action) / (self.max_action - self.min_action)\n",
        "        )\n",
        "        action = np.clip(action, low, high)\n",
        "        return action\n",
        "\n",
        "    def seed(self, seed):\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "# gym.logger.set_level(40)\n",
        "# env = CustomWrapper(gym.make(\"BipedalWalker-v3\"),  min_action = -1.0,  max_action = 1.0)\n",
        "# env.seed(0)\n",
        "\n",
        "# agent = TD3Agent(state_size=env.observation_space.shape[0], \\\n",
        "#                  action_size=env.action_space.shape[0], \\\n",
        "#                  max_action=env.action_space.high, \\\n",
        "#                  min_action=env.action_space.low, continue_training=False)\n"
      ],
      "metadata": {
        "id": "YZNQsiSqciQ5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Agent**"
      ],
      "metadata": {
        "id": "JPdq-Y3U3_0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v3Cd3SiWC5Y5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41f3df4f-14f5-4e35-bc4d-bd2cb27554ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "<ipython-input-13-a9f12f013582>:41: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  state = torch.tensor([self.state])\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "from numpy import inf\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, env: gym.Env, replay_buffer: ReplayBuffer, max_action, min_action) -> None:\n",
        "        \"\"\"Initialize an Agent object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            max_action (ndarray): the maximum valid value for each action vector\n",
        "            min_action (ndarray): the minimum valid value for each action vector\n",
        "            noise (float): the range to generate random noise while learning\n",
        "            noise_std (float): the range to generate random noise while performing action\n",
        "            noise_clip (float): to clip random noise into this range\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.reset()\n",
        "        self.state = self.env.reset()\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.min_action = min_action\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = replay_buffer\n",
        "\n",
        "    def action(self, actor, device) -> np.ndarray:\n",
        "\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        state = torch.tensor([self.state])\n",
        "\n",
        "        if device not in [\"cpu\"]:\n",
        "          state = state.cuda(device).cpu().data.numpy()\n",
        "\n",
        "        action = actor(state).cpu().data.numpy()\n",
        "\n",
        "        action = action.clip(self.min_action[0], self.max_action[0])\n",
        "\n",
        "        return action\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        \"\"\"Resets the environment and updates the state.\"\"\"\n",
        "        self.state = self.env.reset()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, actor: nn.Module, device: str = \"cpu\") -> Tuple[float, bool]:\n",
        "        action = self.action(actor, device)\n",
        "\n",
        "        next_state, reward, done, _ = self.env.step(action[0])\n",
        "\n",
        "        self.memory.add(self.state, action[0], reward, next_state, done)\n",
        "\n",
        "        self.state = next_state\n",
        "\n",
        "        if done:\n",
        "            self.reset()\n",
        "        return reward, done\n",
        "\n",
        "env = CustomWrapper(gym.make(\"BipedalWalker-v3\"),  min_action = -1.0,  max_action = 1.0)\n",
        "memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
        "agent = Agent(env=env, replay_buffer=memory, max_action=env.action_space.high, min_action=env.action_space.low)\n",
        "actor = Actor(env.observation_space.shape[0], env.action_space.shape[0], float(env.action_space.high[0])).to(device)\n",
        "\n",
        "for i in range(10):\n",
        "  agent.step(actor)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, List, Tuple\n",
        "from lightning.pytorch import LightningModule\n",
        "from collections import OrderedDict, deque, namedtuple\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "\n",
        "from numpy import inf\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "NOISE = 0.2\n",
        "NOISE_STD = 0.1\n",
        "NOISE_CLIP = 0.5\n",
        "\n",
        "class TD3Lightning(LightningModule):\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 state_size: int,\n",
        "                 action_size: int,\n",
        "                 max_action: int,\n",
        "                 min_action: int,\n",
        "                 sync_rate: int = 10,\n",
        "                 lr: float = 1e-2,\n",
        "                 batch_size: int = 100,\n",
        "                 episode_length: int = 50,\n",
        "                 warm_start_steps: int = 200):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            max_action (ndarray): the maximum valid value for each action vector\n",
        "            min_action (ndarray): the minimum valid value for each action vector\n",
        "            noise (float): the range to generate random noise while learning\n",
        "            noise_std (float): the range to generate random noise while performing action\n",
        "            noise_clip (float): to clip random noise into this range\n",
        "        \"\"\"\n",
        "        super(TD3Lightning, self).__init__()\n",
        "        self.warm_start_steps = warm_start_steps\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.max_action = max_action\n",
        "        self.min_action = min_action\n",
        "        self.env = env\n",
        "        self.nb_optim_iters = 4,\n",
        "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
        "        self.agent = Agent(self.env, self.memory, self.max_action, self.min_action)\n",
        "\n",
        "        self.total_reward = 0\n",
        "        self.episode_reward = 0\n",
        "        self.lr = lr\n",
        "        self.sync_rate = sync_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.episode_length = episode_length\n",
        "\n",
        "        # Set the device globally\n",
        "        # torch.set_default_device(device)\n",
        "\n",
        "        self.actor = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "        self.actor_target = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "\n",
        "        self.critic = Critic(state_size, action_size).to(device)\n",
        "        self.critic_target = Critic(state_size, action_size).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "        self.populate(self.warm_start_steps)\n",
        "\n",
        "        self.total_reward = 0\n",
        "        self.episode_reward = 0\n",
        "\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "\n",
        "    def populate(self, steps: int = 1000) -> None:\n",
        "        \"\"\"Carries out several random steps through the environment to initially fill up the replay buffer with\n",
        "        experiences.\n",
        "\n",
        "        Args:\n",
        "            steps: number of random steps to populate the buffer with\n",
        "\n",
        "        \"\"\"\n",
        "        print(\"populate\")\n",
        "        for i in range(steps):\n",
        "            self.agent.step(self.actor)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Passes in a state x through the network and returns the policy and a sampled action\n",
        "        Args:\n",
        "            x: environment state\n",
        "        Returns:\n",
        "            Tuple of policy and action\n",
        "        \"\"\"\n",
        "        action = self.actor(state)\n",
        "        action = action * self.max_action\n",
        "        Q1_expected, Q2_expected = self.critic(state, action)\n",
        "\n",
        "        print(\"foward\")\n",
        "\n",
        "        return action, Q1_expected, Q2_expected\n",
        "\n",
        "    def actor_loss(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n",
        "        state, action, reward, next_state, done = batch\n",
        "\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        print(\"loss actor\")\n",
        "\n",
        "        return actor_loss\n",
        "\n",
        "    def critic_loss(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n",
        "        state, action, reward, next_state, done = batch\n",
        "\n",
        "        action_ = action.cpu().numpy()\n",
        "\n",
        "        # ---------------------------- update critic ---------------------------- #\n",
        "        # Get predicted next-state actions and Q values from target models\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Generate a random noise\n",
        "            noise = torch.FloatTensor(action_).data.normal_(0, NOISE).to(device)\n",
        "            noise = noise.clamp(-NOISE_CLIP, NOISE_CLIP)\n",
        "            actions_next = (self.actor_target(next_state) + noise).clamp(self.min_action[0].astype(float), self.max_action[0].astype(float))\n",
        "\n",
        "            Q1_targets_next, Q2_targets_next = self.critic_target(next_state, actions_next)\n",
        "\n",
        "            Q_targets_next = torch.min(Q1_targets_next, Q2_targets_next)\n",
        "\n",
        "            # Compute Q targets for current states (y_i)\n",
        "            Q_targets = reward + (GAMMA * Q_targets_next * (1 - done)).detach()\n",
        "\n",
        "        # Compute critic loss\n",
        "        Q1_expected, Q2_expected = self.critic(state, action)\n",
        "\n",
        "        critic_loss = F.mse_loss(Q1_expected, Q_targets) + F.mse_loss(Q2_expected, Q_targets)\n",
        "\n",
        "        print(\"loss critic\")\n",
        "\n",
        "        return critic_loss\n",
        "\n",
        "    def train_batch(\n",
        "            self,\n",
        "    ) -> Tuple[List[torch.Tensor], List[torch.Tensor], List[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Contains the logic for generating trajectory data to train policy and value network\n",
        "        Yield:\n",
        "           Tuple of Lists containing tensors for states, actions, log probs, qvals and advantage\n",
        "        \"\"\"\n",
        "\n",
        "        for step in range(self.steps_per_epoch):\n",
        "            pi, action, log_prob, value = self.agent(self.state, self.device)\n",
        "            next_state, reward, done, _ = self.env.step(action.cpu().numpy())\n",
        "\n",
        "            self.episode_step += 1\n",
        "\n",
        "            self.batch_states.append(self.state)\n",
        "            self.batch_actions.append(action)\n",
        "            self.batch_logp.append(log_prob)\n",
        "\n",
        "            self.ep_rewards.append(reward)\n",
        "            self.ep_values.append(value.item())\n",
        "\n",
        "            self.state = torch.FloatTensor(next_state)\n",
        "\n",
        "            epoch_end = step == (self.steps_per_epoch - 1)\n",
        "            terminal = len(self.ep_rewards) == self.max_episode_len\n",
        "\n",
        "            if epoch_end or done or terminal:\n",
        "                # if trajectory ends abtruptly, boostrap value of next state\n",
        "                if (terminal or epoch_end) and not done:\n",
        "                    with torch.no_grad():\n",
        "                        _, _, _, value = self.agent(self.state, self.device)\n",
        "                        last_value = value.item()\n",
        "                        steps_before_cutoff = self.episode_step\n",
        "                else:\n",
        "                    last_value = 0\n",
        "                    steps_before_cutoff = 0\n",
        "\n",
        "                # discounted cumulative reward\n",
        "                self.batch_qvals += self.discount_rewards(self.ep_rewards + [last_value], self.gamma)[:-1]\n",
        "                # advantage\n",
        "                self.batch_adv += self.calc_advantage(self.ep_rewards, self.ep_values, last_value)\n",
        "                # logs\n",
        "                self.epoch_rewards.append(sum(self.ep_rewards))\n",
        "                # reset params\n",
        "                self.ep_rewards = []\n",
        "                self.ep_values = []\n",
        "                self.episode_step = 0\n",
        "                self.state = torch.FloatTensor(self.env.reset())\n",
        "\n",
        "            if epoch_end:\n",
        "                train_data = zip(\n",
        "                    self.batch_states, self.batch_actions, self.batch_logp,\n",
        "                    self.batch_qvals, self.batch_adv)\n",
        "\n",
        "                for state, action, logp_old, qval, adv in train_data:\n",
        "                    yield state, action, logp_old, qval, adv\n",
        "\n",
        "                self.batch_states.clear()\n",
        "                self.batch_actions.clear()\n",
        "                self.batch_adv.clear()\n",
        "                self.batch_logp.clear()\n",
        "                self.batch_qvals.clear()\n",
        "\n",
        "                # logging\n",
        "                self.avg_reward = sum(self.epoch_rewards) / self.steps_per_epoch\n",
        "\n",
        "                # if epoch ended abruptly, exlude last cut-short episode to prevent stats skewness\n",
        "                epoch_rewards = self.epoch_rewards\n",
        "                if not done:\n",
        "                    epoch_rewards = epoch_rewards[:-1]\n",
        "\n",
        "                total_epoch_reward = sum(epoch_rewards)\n",
        "                nb_episodes = len(epoch_rewards)\n",
        "\n",
        "                self.avg_ep_reward = total_epoch_reward / nb_episodes\n",
        "                self.avg_ep_len = (self.steps_per_epoch - steps_before_cutoff) / nb_episodes\n",
        "\n",
        "                self.epoch_rewards.clear()\n",
        "\n",
        "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx) -> OrderedDict:\n",
        "        \"\"\" Update policy and value parameters using given batch of experience tuples.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            n_iteraion (int): the number of iterations to train network\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        device = self.get_device(batch)\n",
        "\n",
        "        actor_optimizer, critic_optimizer = self.configure_optimizers()\n",
        "\n",
        "        reward, done = self.agent.step(self.actor, device)\n",
        "        self.episode_reward += reward\n",
        "\n",
        "        critic_loss = self.critic_loss(batch)\n",
        "\n",
        "        if done:\n",
        "            self.total_reward = self.episode_reward\n",
        "            self.episode_reward = 0\n",
        "\n",
        "        ###################\n",
        "        # Optimize Critic #\n",
        "        ###################\n",
        "        print(\"update critic\")\n",
        "        critic_optimizer.zero_grad()\n",
        "        self.manual_backward(critic_loss)\n",
        "        critic_optimizer.step()\n",
        "\n",
        "        if batch_idx % UPDATE_EVERY_STEP == 0:\n",
        "            # ---------------------------- update actor ---------------------------- #\n",
        "            # Compute actor loss\n",
        "            actor_loss = self.actor_loss(batch)\n",
        "\n",
        "            ##################\n",
        "            # Optimize Actor #\n",
        "            ##################\n",
        "            print(\"update actor\")\n",
        "            actor_optimizer.zero_grad()\n",
        "            self.manual_backward(actor_loss)\n",
        "            actor_optimizer.step()\n",
        "\n",
        "            # ----------------------- update target networks ----------------------- #\n",
        "            print(\"update target networks\")\n",
        "            for target_param, local_param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
        "              target_param.data.copy_(TAU*local_param.data + (1.0-TAU)*target_param.data)\n",
        "            for target_param, local_param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
        "              target_param.data.copy_(TAU*local_param.data + (1.0-TAU)*target_param.data)\n",
        "\n",
        "        log = {\n",
        "          \"total_reward\": torch.tensor(self.total_reward).to(device),\n",
        "          \"reward\": torch.tensor(reward).to(device),\n",
        "        }\n",
        "        # print(\"Reward: \", reward)\n",
        "\n",
        "        return OrderedDict({\"log\": log, \"progress_bar\": log})\n",
        "\n",
        "    def configure_optimizers(self) -> List[Optimizer]:\n",
        "        \"\"\" Initialize Adam optimizer\"\"\"\n",
        "        optimizer_actor = optim.Adam(self.actor.parameters(), lr=self.lr)\n",
        "        optimizer_critic = optim.Adam(self.critic.parameters(), lr=self.lr)\n",
        "\n",
        "        return optimizer_actor, optimizer_critic\n",
        "\n",
        "\n",
        "    def optimizer_step(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Run 'nb_optim_iters' number of iterations of gradient descent on actor and critic\n",
        "        for each data sample.\n",
        "        \"\"\"\n",
        "        for i in range(self.nb_optim_iters):\n",
        "            super().optimizer_step(*args, **kwargs)\n",
        "\n",
        "    # def save(self, filename, version) -> None:\n",
        "    #       \"\"\" Save the model \"\"\"\n",
        "    #       torch.save(self.critic.state_dict(), filename + \"_critic_\" + version + \".pth\")\n",
        "    #       torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer_\" + version + \".pth\")\n",
        "\n",
        "    #       torch.save(self.actor.state_dict(), filename + \"_actor_\" + version + \".pth\")\n",
        "    #       torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer_\" + version + \".pth\")\n",
        "\n",
        "    # def load(self, filename) -> None:\n",
        "    #       \"\"\" Load the model \"\"\"\n",
        "    #       self.critic.load_state_dict(torch.load(filename + \"_critic.pth\"))\n",
        "    #       self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer.pth\"))\n",
        "    #       self.critic_target = copy.deepcopy(self.critic)\n",
        "\n",
        "    #       self.actor.load_state_dict(torch.load(filename + \"_actor.pth\"))\n",
        "    #       self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer.pth\"))\n",
        "    #       self.actor_target = copy.deepcopy(self.actor)\n",
        "\n",
        "    def __dataloader(self) -> DataLoader:\n",
        "        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\n",
        "        dataset = RLDataset(self.memory, self.episode_length)\n",
        "        return DataLoader(dataset=dataset, batch_size=self.batch_size, sampler=None)\n",
        "\n",
        "    def train_dataloader(self) -> DataLoader:\n",
        "        \"\"\"Get train loader.\"\"\"\n",
        "        return self.__dataloader()\n",
        "\n",
        "    def get_device(self, batch) -> str:\n",
        "        \"\"\"Retrieve device currently being used by minibatch.\"\"\"\n",
        "        return batch[0].device.index if self.on_gpu else \"cpu\"\n",
        "\n",
        "\n",
        "from lightning.pytorch import Trainer, cli_lightning_logo, seed_everything\n",
        "\n",
        "def main() -> None:\n",
        "    env = CustomWrapper(gym.make(\"BipedalWalker-v3\"),  min_action = -1.0,  max_action = 1.0)\n",
        "    model = TD3Lightning(env=env,\n",
        "                         state_size=env.observation_space.shape[0],\n",
        "                         action_size=env.action_space.shape[0],\n",
        "                         max_action=env.action_space.high,\n",
        "                         min_action=env.action_space.low,\n",
        "                         sync_rate=10, lr=1e-2,\n",
        "                         episode_length=200,\n",
        "                         batch_size=100,\n",
        "                         warm_start_steps=1000)\n",
        "    trainer = Trainer(accelerator=\"cpu\", devices=1, val_check_interval=100, max_epochs=1500, logger=WandbLogger(log_model=\"all\"))\n",
        "    trainer.fit(model)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "yBsvSByhUh6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train**"
      ],
      "metadata": {
        "id": "uV1KanhbJwfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"td3\") # fb372890f5180a16a9cd2df5b9558e55493cd16c"
      ],
      "metadata": {
        "id": "d46eJqqklJvj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286,
          "referenced_widgets": [
            "514451e97b664fddb4fc6b04cce50752",
            "4301188f5a444fce85c0d33ddabef458",
            "046980fd09664534bc624402faf0c023",
            "dcc6b3ed82be428c88a1fa887f387faa",
            "a1296e28d4104f0184f76f7c2961c4cd",
            "320470c1d74f4fe2b80319a0622ccad0",
            "00a9754dc74c43188dee6d828a02acec",
            "3a0397c198584902bc5fc773bcb061e6"
          ]
        },
        "outputId": "8a099652-94ad-4315-a8b9-d3f3feb177d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:15q9grf5) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "514451e97b664fddb4fc6b04cce50752"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">splendid-fire-46</strong> at: <a href='https://wandb.ai/alangrotti/lightning_logs/runs/15q9grf5' target=\"_blank\">https://wandb.ai/alangrotti/lightning_logs/runs/15q9grf5</a><br/> View project at: <a href='https://wandb.ai/alangrotti/lightning_logs' target=\"_blank\">https://wandb.ai/alangrotti/lightning_logs</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240526_133954-15q9grf5/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:15q9grf5). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240526_134012-9dpiuhky</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alangrotti/td3/runs/9dpiuhky' target=\"_blank\">fast-thunder-21</a></strong> to <a href='https://wandb.ai/alangrotti/td3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alangrotti/td3' target=\"_blank\">https://wandb.ai/alangrotti/td3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alangrotti/td3/runs/9dpiuhky' target=\"_blank\">https://wandb.ai/alangrotti/td3/runs/9dpiuhky</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alangrotti/td3/runs/9dpiuhky?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7bb2bae19d50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ERQ6QREQC99V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "51ce12b0-18b0-402b-9f6e-5ba7d9a10d3d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Agent' object has no attribute 'predict'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-5d2ba791e7be>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-5d2ba791e7be>\u001b[0m in \u001b[0;36mtd3\u001b[0;34m(n_episodes, max_t)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'predict'"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def td3(n_episodes=2000, max_t=2000):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    times_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    solved = False\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        start_time = time.time()\n",
        "        for t in range(max_t):\n",
        "            action = agent.predict(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            if done or t==(max_t-1):\n",
        "                critic_loss, actor_loss, q, max = agent.learn(t)\n",
        "                break\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        scores_deque.append(score)\n",
        "        times_deque.append(duration)\n",
        "        scores.append(score)\n",
        "        mean_score = np.mean(scores_deque)\n",
        "        mean_times = np.mean(times_deque)\n",
        "\n",
        "        #wandb.log({'Score': mean_score, 'Critic loss': critic_loss, 'Actor loss': actor_loss, 'Average Q': q, 'Max. Q': max, \"Duration \": mean_times}, step=i_episode)\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, mean_score, score), end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "        if i_episode % 500 == 0:\n",
        "            agent.save(\"checkpoint\", str(i_episode))\n",
        "        if mean_score >= 300 and solved == False:\n",
        "            print('\\rSolved at Episode {} !\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "            agent.save(\"checkpoint\")\n",
        "            solved = True\n",
        "\n",
        "    return scores\n",
        "\n",
        "scores = td3()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Result**"
      ],
      "metadata": {
        "id": "_GJ4lM-g8jB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "metadata": {
        "id": "iUJfDXmtMesi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "\n",
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "agent.actor.load_state_dict(torch.load('/content/checkpoint_actor_300.pth'))\n",
        "agent.critic.load_state_dict(torch.load('/content/checkpoint_critic_300.pth'))\n",
        "agent.actor_optimizer.load_state_dict(torch.load('/content/checkpoint_actor_optimizer_300.pth'))\n",
        "agent.critic_optimizer.load_state_dict(torch.load('/content/checkpoint_critic_optimizer_300.pth'))\n",
        "\n",
        "env = gym.make('BipedalWalker-v3')\n",
        "state = env.reset()\n",
        "score = 0\n",
        "img = plt.imshow(env.render('rgb_array'))\n",
        "while True:\n",
        "    img.set_data(env.render('rgb_array'))\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    action = agent.predict(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if np.any(done):\n",
        "        break\n",
        "\n",
        "print(\"Score: {}\".format(score))"
      ],
      "metadata": {
        "id": "IXJHQR6N8uwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test**"
      ],
      "metadata": {
        "id": "pt4M5an2gPmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestModel(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.env = gym.make(\"BipedalWalker-v3\")\n",
        "\n",
        "        # param model and buffer\n",
        "        self.batch_size = 100\n",
        "        self.buffer_size = int(1e5)\n",
        "        self.random_seed = 0\n",
        "        self.error = 0\n",
        "\n",
        "        # size action / state\n",
        "        self.action_size = self.env.action_space.shape[0]\n",
        "        self.state_size = self.env.observation_space.shape[0]\n",
        "\n",
        "        # min / max action\n",
        "        self.min_action = self.env.action_space.low\n",
        "        self.max_action = self.env.action_space.high\n",
        "\n",
        "        # min / max state\n",
        "        self.min_state = 0\n",
        "        self.max_state = 1\n",
        "\n",
        "        # min / max reward\n",
        "        self.min_reward = -300\n",
        "        self.max_reward = 300\n",
        "\n",
        "        self.model = TD3Agent(state_size=self.state_size, action_size=self.action_size, \\\n",
        "                         max_action=self.max_action, min_action=self.min_action, random_seed=self.random_seed)\n",
        "\n",
        "        self.memory = ReplayBufferPer(self.buffer_size)\n",
        "\n",
        "        # param number tests\n",
        "        self.num_attempts = 150\n",
        "\n",
        "    def _randomStates(self):\n",
        "        states = np.array([random.uniform(self.min_state, self.max_state) for _ in range(24)], dtype=np.float32)\n",
        "        return states\n",
        "\n",
        "    def _randomAction(self):\n",
        "        action = np.random.uniform(self.min_action, self.max_action, self.action_size)\n",
        "        return action\n",
        "\n",
        "    def _randomDone(self):\n",
        "        done = random.choice([True, False])\n",
        "        return done\n",
        "\n",
        "    def _randomReward(self):\n",
        "        reward = random.randint(self.min_reward, self.max_reward)\n",
        "        return reward\n",
        "\n",
        "    def test_predict_act(self):\n",
        "        \"\"\" Teste para verificar se o estado de saida da rede contem os valores minimos e maximos de ação que o ambiente exigi\n",
        "\n",
        "            Input: numpy.ndarray [24]\n",
        "            output: numpy.ndarray [4]\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        for _ in range(self.num_attempts):\n",
        "            states = self._randomStates()\n",
        "            action = self.model.predict(states)\n",
        "            is_valid = (isinstance(action, np.ndarray) and np.all(action >= self.min_action) and np.all(action <= self.max_action))\n",
        "\n",
        "            if not is_valid:\n",
        "                self.fail(\"Teste falhou na tentativa {}\".format(_ + 1))\n",
        "\n",
        "    def test_buffer_type(self):\n",
        "        while True:\n",
        "          next_state, reward, done, action, state = self._randomStates(), self._randomReward(), self._randomDone(), self._randomAction(), self._randomStates()\n",
        "\n",
        "          self.memory.add((state, action, reward, next_state, done), reward)\n",
        "\n",
        "          if len(self.memory) > self.batch_size:\n",
        "              break\n",
        "\n",
        "\n",
        "        (states, actions, rewards, next_states, dones), idxs, is_weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        self.assertIsInstance(states, torch.Tensor)\n",
        "        self.assertIsInstance(actions, torch.Tensor)\n",
        "        self.assertIsInstance(rewards, torch.Tensor)\n",
        "        self.assertIsInstance(next_states, torch.Tensor)\n",
        "        self.assertIsInstance(dones, torch.Tensor)\n",
        "\n",
        "    def test_buffer_size(self):\n",
        "        while True:\n",
        "          next_state, reward, done, action, state = self._randomStates(), self._randomReward(), self._randomDone(), self._randomAction(), self._randomStates()\n",
        "          self.memory.add((state, action, reward, next_state, done), reward)\n",
        "\n",
        "          if len(self.memory) > self.batch_size:\n",
        "            break\n",
        "\n",
        "        (states, actions, rewards, next_states, dones), idxs, is_weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        expected_batch_size = self.batch_size\n",
        "        self.assertEqual(states.size(0), expected_batch_size)\n",
        "        self.assertEqual(actions.size(0), expected_batch_size)\n",
        "        self.assertEqual(rewards.size(0), expected_batch_size)\n",
        "        self.assertEqual(next_states.size(0), expected_batch_size)\n",
        "        self.assertEqual(dones.size(0), expected_batch_size)\n",
        "\n",
        "    def test_buffer_range(self):\n",
        "        while True:\n",
        "          next_state, reward, done, action, state = self._randomStates(), self._randomReward(), self._randomDone(), self._randomAction(), self._randomStates()\n",
        "          self.memory.add((state, action, reward, next_state, done), reward)\n",
        "\n",
        "          if len(self.memory) > self.batch_size:\n",
        "            break\n",
        "\n",
        "        (states, actions, rewards, next_states, dones), idxs, weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        self.assertTrue(np.all(states[1].cpu().data.numpy() >= self.min_state) and np.all(states[1].cpu().data.numpy() <= self.max_state))\n",
        "        self.assertTrue(np.all(actions[1].cpu().data.numpy() >= self.min_action) and np.all(actions[1].cpu().data.numpy() <= self.max_action))\n",
        "        self.assertTrue(np.all(rewards[1].cpu().data.numpy() >= self.min_reward) and np.all(rewards[1].cpu().data.numpy() <= self.max_reward))\n",
        "        self.assertTrue(np.all(next_states[1].cpu().data.numpy() >= self.min_state) and np.all(next_states[1].cpu().data.numpy() <= self.max_state))\n",
        "        self.assertTrue(np.all(dones.cpu().data.numpy() >= 0.) and np.all(dones.cpu().data.numpy() <= 1.))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "id": "huHvCkTggSDs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gkNza9le_wuU",
        "ZADS3Hz_NdzM",
        "8wHw8OOk38ps",
        "XXqRDsXVce1Y",
        "_GJ4lM-g8jB9",
        "pt4M5an2gPmn"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "514451e97b664fddb4fc6b04cce50752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4301188f5a444fce85c0d33ddabef458",
              "IPY_MODEL_046980fd09664534bc624402faf0c023"
            ],
            "layout": "IPY_MODEL_dcc6b3ed82be428c88a1fa887f387faa"
          }
        },
        "4301188f5a444fce85c0d33ddabef458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1296e28d4104f0184f76f7c2961c4cd",
            "placeholder": "​",
            "style": "IPY_MODEL_320470c1d74f4fe2b80319a0622ccad0",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "046980fd09664534bc624402faf0c023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00a9754dc74c43188dee6d828a02acec",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a0397c198584902bc5fc773bcb061e6",
            "value": 1
          }
        },
        "dcc6b3ed82be428c88a1fa887f387faa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1296e28d4104f0184f76f7c2961c4cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "320470c1d74f4fe2b80319a0622ccad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00a9754dc74c43188dee6d828a02acec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a0397c198584902bc5fc773bcb061e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
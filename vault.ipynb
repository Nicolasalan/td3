{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicolasalan/td3/blob/main/vault.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUkL1MTSs3Nn"
      },
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s53BlzVCk8g_",
        "outputId": "5c739e97-3d48-4cb7-ca59-d01903e3f6be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 28 17:12:58 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8    13W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAddaBONmh68"
      },
      "source": [
        "Adicionar script ao console desse navegador: `inspecionar` => `console` => `adicionar script`.\n",
        "```\n",
        "function ConnectButton(){\n",
        "    console.log(\"Conectado\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkNza9le_wuU"
      },
      "source": [
        "# **Install**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qa_nNJfpVwX",
        "outputId": "b96a54aa-fa60-467a-abc2-e9df79952e3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 2s (515 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 120874 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349116 sha256=8a23f0c6d92e32773a25c275c44eda701a97396baf08c0d46a6142ff2690fda0\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.1.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.32.0-py2.py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=4cfbb8237a1feb7894313f8cd14014005dc8dd28961a7b59c1490210d9d03e0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 pathtools-0.1.2 sentry-sdk-1.32.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.15.12\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install swig\n",
        "\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]\n",
        "\n",
        "!pip install torch\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h524c52m_oWK"
      },
      "source": [
        "# **TD3 - Twin Delayed DDPG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mUbRDppiSxX"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "from collections import namedtuple, deque\n",
        "from numpy import inf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgP_RHskquVM"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 100        # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR_ACTOR = 1e-3         # learning rate of the actor\n",
        "LR_CRITIC = 1e-3        # learning rate of the critic\n",
        "UPDATE_EVERY_STEP = 2   # how often to update the target and actor networks\n",
        "SIGMA = 0\n",
        "THETA = 0\n",
        "L1 = 0\n",
        "L2 = 0\n",
        "dropout = 0\n",
        "BatchNorm1d = 0\n",
        "lstm_dim = 0\n",
        "WEIGHT_DECAY = 0\n",
        "LEARN_EVERY = 0         # learning timestep interval\n",
        "LEARN_NUM = 0           # number of learning passes\n",
        "\n",
        "EPSILON = 0             # explore->exploit noise process added to act step #######\n",
        "EPSILON_DECAY = 0       # decay rate for noise process\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPp319_-l8zP"
      },
      "source": [
        "### **🔒Replay Buffer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urnQN79aX7yO"
      },
      "source": [
        "https://github.com/gribeiro2004/Continuous-control-with-DDPG-and-prioritized-experience-replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rhWDGNwqwRJ"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "        Params\n",
        "        ======\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhfVnB18mASp"
      },
      "source": [
        "### **💡Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_NtDH5iq1kP"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, max_action, fc1_units=400, fc2_units=300):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            max_action (float): the maximum valid value for action\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.max_action * torch.tanh(self.fc3(x))\n",
        "\n",
        "    def add_parameter_noise(self, scalar=0.1):\n",
        "        for layer in [self.fc1, self.fc2, self.fc3]:\n",
        "            layer.weight.data += torch.randn_like(layer.weight.data) * scalar\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, l1=400, l2=300):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "    # Q1 architecture\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, l1)\n",
        "        self.l2 = nn.Linear(l1, l2)\n",
        "        self.l3 = nn.Linear(l2, 1)\n",
        "\n",
        "    # Q2 architecture\n",
        "        self.l4 = nn.Linear(state_dim + action_dim, l1)\n",
        "        self.l5 = nn.Linear(l1, l2)\n",
        "        self.l6 = nn.Linear(l2, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        xu = torch.cat([x, u], 1)\n",
        "\n",
        "        x1 = F.relu(self.l1(xu))\n",
        "        x1 = F.relu(self.l2(x1))\n",
        "        x1 = self.l3(x1)\n",
        "\n",
        "        x2 = F.relu(self.l4(xu))\n",
        "        x2 = F.relu(self.l5(x2))\n",
        "        x2 = self.l6(x2)\n",
        "\n",
        "        return x1, x2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRwCO7y9mCLe"
      },
      "source": [
        "### **🚀Agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlmfHeGoq5a1"
      },
      "outputs": [],
      "source": [
        "class TD3Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, max_action, min_action, random_seed, target_noise=0.2, noise_std=0.1, noise_clip=0.5):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            max_action (ndarray): the maximum valid value for each action vector\n",
        "            min_action (ndarray): the minimum valid value for each action vector\n",
        "            random_seed (int): random seed\n",
        "            noise (float): the range to generate random noise while learning\n",
        "            noise_std (float): the range to generate random noise while performing action\n",
        "            noise_clip (float): to clip random noise into this range\n",
        "        \"\"\"\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.max_action = max_action\n",
        "        self.min_action = min_action\n",
        "        self.noise_std = noise_std\n",
        "        self.noise_clip = noise_clip\n",
        "        self.target_noise = target_noise\n",
        "        self.seed = random.seed(random_seed)\n",
        "\n",
        "        print(random.seed(random_seed))\n",
        "\n",
        "        # parameter noise\n",
        "        self.distances = []\n",
        "        self.desired_distance = 0.2\n",
        "        self.scalar = 0.05\n",
        "        self.scalar_decay = 0.99\n",
        "\n",
        "        # Actor Network (w/ Target Network)\n",
        "        self.actor_local = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "        self.actor_target = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
        "\n",
        "        # Critic Network (w/ Target Network)\n",
        "        self.critic_local = Critic(state_size, action_size).to(device)\n",
        "        self.critic_target = Critic(state_size, action_size).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic_local.state_dict())\n",
        "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC)\n",
        "\n",
        "        self.actor_noise = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Save experience in replay memory\"\"\"\n",
        "        # Save experience / reward\n",
        "        if isinstance(state, np.ndarray):\n",
        "            states = state\n",
        "        elif isinstance(state, tuple):\n",
        "            states = np.array(state[0], dtype=np.float32)\n",
        "\n",
        "        self.memory.add(states, action, reward, next_state, done)\n",
        "\n",
        "    def predict(self, states, add_noise=True):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        # Verificando se é uma numpy.ndarray\n",
        "        if isinstance(states, np.ndarray):\n",
        "            convert_state = states\n",
        "        elif isinstance(states, tuple):\n",
        "            convert_state = np.array(states[0], dtype=np.float32)\n",
        "        state = torch.from_numpy(convert_state).float().to(device)\n",
        "        self.actor_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action = self.actor_local(state).cpu().data.numpy()\n",
        "\n",
        "        if add_noise:\n",
        "            # Generate a random noise\n",
        "            self.actor_noise.load_state_dict(self.actor_local.state_dict().copy())\n",
        "            self.actor_noise.add_parameter_noise(self.scalar)\n",
        "            action_noise = self.actor_noise(state).cpu().data.numpy()\n",
        "            distance = np.sqrt(np.mean(np.square(action - action_noise)))\n",
        "            self.distances.append(distance)\n",
        "\n",
        "            if distance > self.desired_distance:\n",
        "                self.scalar *= self.scalar_decay\n",
        "            if distance < self.desired_distance:\n",
        "                self.scalar /= self.scalar_decay\n",
        "\n",
        "            #action = action_noise\n",
        "\n",
        "        self.actor_local.train()\n",
        "\n",
        "        return np.clip(action, self.min_action[0], self.max_action[0]), np.mean(action_noise)\n",
        "\n",
        "    def learn(self, n_iteraion, gamma=GAMMA):\n",
        "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            n_iteraion (int): the number of iterations to train network\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "\n",
        "        if len(self.memory) > BATCH_SIZE:\n",
        "\n",
        "            av_Q = 0\n",
        "            max_Q = -inf\n",
        "            av_loss = 0\n",
        "\n",
        "            for i in range(n_iteraion):\n",
        "                state, action, reward, next_state, done = self.memory.sample()\n",
        "\n",
        "                action_ = action.cpu().numpy()\n",
        "\n",
        "                # ---------------------------- update critic ---------------------------- #\n",
        "                # Get predicted next-state actions and Q values from target models\n",
        "                actions_next = self.actor_target(next_state)\n",
        "\n",
        "                # Generate a random noise\n",
        "                noise = torch.FloatTensor(action_).data.normal_(0, self.target_noise).to(device)\n",
        "                noise = noise.clamp(-self.noise_clip, self.noise_clip)\n",
        "                actions_next = (actions_next + noise).clamp(self.min_action[0].astype(float), self.max_action[0].astype(float))\n",
        "\n",
        "                Q1_targets_next, Q2_targets_next = self.critic_target(next_state, actions_next)\n",
        "\n",
        "                Q_targets_next = torch.min(Q1_targets_next, Q2_targets_next)\n",
        "\n",
        "                av_Q += torch.mean(Q_targets_next)\n",
        "                max_Q = max(max_Q, torch.max(Q_targets_next))\n",
        "\n",
        "                # Compute Q targets for current states (y_i)\n",
        "                Q_targets = reward + (gamma * Q_targets_next * (1 - done)).detach()\n",
        "                # Compute critic loss\n",
        "                Q1_expected, Q2_expected = self.critic_local(state, action)\n",
        "                critic_loss = F.mse_loss(Q1_expected, Q_targets) + F.mse_loss(Q2_expected, Q_targets)\n",
        "\n",
        "                # Minimize the loss\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                if i % UPDATE_EVERY_STEP == 0:\n",
        "                    # ---------------------------- update actor ---------------------------- #\n",
        "                    # Compute actor loss\n",
        "                    actor_loss, _ = self.critic_local(state, self.actor_local(state))\n",
        "                    actor_loss = -actor_loss.mean()\n",
        "                    # Minimize the loss\n",
        "                    self.actor_optimizer.zero_grad()\n",
        "                    actor_loss.backward()\n",
        "                    self.actor_optimizer.step()\n",
        "\n",
        "                    # ----------------------- update target networks ----------------------- #\n",
        "                    self.soft_update(self.critic_local, self.critic_target, TAU)\n",
        "                    self.soft_update(self.actor_local, self.actor_target, TAU)\n",
        "\n",
        "                av_loss += critic_loss\n",
        "\n",
        "            # Write new values\n",
        "            loss = av_loss / n_iteraion\n",
        "            av = av_Q / n_iteraion\n",
        "            max_policy = max_Q\n",
        "\n",
        "            return [loss, av, max_policy]\n",
        "\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        Params\n",
        "        ======\n",
        "            local_model: PyTorch model (weights will be copied from)\n",
        "            target_model: PyTorch model (weights will be copied to)\n",
        "            tau (float): interpolation parameter\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbR-GAM9ur-w"
      },
      "source": [
        "## **📌Env**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2VUEGY4qpK1"
      },
      "source": [
        "### **BipedalWalker**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvW_s-fjybXA"
      },
      "source": [
        "#### **Gym**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R4XFQdxlPZh",
        "outputId": "1130cbb1-b7d4-4f2e-e072-904d1e329b99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamanho de cada ação: 4\n",
            "Ligação superior de cada ação: 1.0\n",
            "Ligação inferior de cada ação: -1.0\n",
            "Cada um observa um estado com comprimento: 24\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"BipedalWalker-v3\", hardcore=True)\n",
        "\n",
        "# tamanho de cada ação\n",
        "action_size = env.action_space\n",
        "print('Tamanho de cada ação:', action_size.shape[0])\n",
        "\n",
        "# vínculo superior de cada ação\n",
        "upper_bond = env.action_space.high\n",
        "print('Ligação superior de cada ação:', upper_bond[0])\n",
        "\n",
        "# vínculo inferior de cada ação\n",
        "lower_bond = env.action_space.low\n",
        "print('Ligação inferior de cada ação:', lower_bond[0])\n",
        "\n",
        "#examina o espaço de estados\n",
        "states = env.observation_space\n",
        "state_size = states.shape[0]\n",
        "print('Cada um observa um estado com comprimento: {}'.format(state_size))\n",
        "\n",
        "agent = TD3Agent(state_size=env.observation_space.shape[0], \\\n",
        "                 action_size=env.action_space.shape[0], \\\n",
        "                 max_action=env.action_space.high, \\\n",
        "                 min_action=env.action_space.low, random_seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_Oib4Wr-RUN"
      },
      "source": [
        "#### **Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuInGOo0rIsd"
      },
      "outputs": [],
      "source": [
        "def td3(n_episodes=1000, max_t=2000):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action, noise = agent.predict(state)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            if done or t==(max_t-1):\n",
        "                loss, av_q, max_q = agent.learn(t)\n",
        "                break\n",
        "\n",
        "        scores_deque.append(score)\n",
        "        scores.append(score)\n",
        "        mean_score = np.mean(scores_deque)\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f} | \\tScore: {:.2f} | \\tLoss: {:.2f} | \\tAv. Q: {:.2f} | \\tMax. Q: {:.2f} | \\tAverage Noise: {:.2f} | \\tAverage Action: {:.2f}'.format(i_episode, mean_score, score, loss, av_q, max_q, noise, np.mean(action)), end=\"\")\n",
        "        if i_episode % 10 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f} | \\tLoss: {:.2f} | \\tAv. Q: {:.2f} | \\tMax. Q: {:.2f}'.format(i_episode, mean_score,loss, av_q, max_q))\n",
        "        if mean_score >= 300 and solved == False:\n",
        "            print('\\rSolved at Episode {} !\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "            torch.save(agent.critic_local.state_dict(), 'critic_checkpoint.pth')\n",
        "            torch.save(agent.actor_local.state_dict(), 'actor_checkpoint.pth')\n",
        "            solved = True\n",
        "\n",
        "    return scores\n",
        "\n",
        "scores = td3()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpZo5QBk-V3n"
      },
      "source": [
        "#### **Plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "Bsp8Iib-12R_",
        "outputId": "debff742-1e24-44bb-ed0d-24a150906709"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Episode #')"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(1, len(scores)+1), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toYwUl7G-dwD"
      },
      "source": [
        "#### **Result**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scaWsZ_C6MSb"
      },
      "outputs": [],
      "source": [
        "# Instalações para ver o agente\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-HShjLN3Gi4"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "\n",
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "agent.actor_local.load_state_dict(torch.load('actor_checkpoint.pth'))\n",
        "agent.critic_local.load_state_dict(torch.load('critic_checkpoint.pth'))\n",
        "\n",
        "env = gym.make('BipedalWalker-v3')\n",
        "state = env.reset()\n",
        "score = 0\n",
        "img = plt.imshow(env.render('rgb_array'))\n",
        "while True:\n",
        "    img.set_data(env.render('rgb_array'))\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    action = agent.predict(state)\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if np.any(done):\n",
        "        break\n",
        "\n",
        "print(\"Score: {}\".format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBTA4Y6rxDiU"
      },
      "source": [
        "### **Custom Env**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HclUPO3VYHBb"
      },
      "outputs": [],
      "source": [
        "import pygame\n",
        "import math\n",
        "\n",
        "screen_width = 1500\n",
        "screen_height = 800\n",
        "check_point = ((1200, 660), (1250, 120), (190, 200), (1030, 270), (250, 475), (650, 690))\n",
        "\n",
        "class Car:\n",
        "    def __init__(self, car_file, map_file, pos):\n",
        "        self.surface = pygame.image.load(car_file)\n",
        "        self.map = pygame.image.load(map_file)\n",
        "        self.surface = pygame.transform.scale(self.surface, (100, 100))\n",
        "        self.rotate_surface = self.surface\n",
        "        self.pos = pos\n",
        "        self.angle = 0\n",
        "        self.speed = 0\n",
        "        self.center = [self.pos[0] + 50, self.pos[1] + 50]\n",
        "        self.radars = []\n",
        "        self.radars_for_draw = []\n",
        "        self.is_alive = True\n",
        "        self.current_check = 0\n",
        "        self.prev_distance = 0\n",
        "        self.cur_distance = 0\n",
        "        self.goal = False\n",
        "        self.check_flag = False\n",
        "        self.distance = 0\n",
        "        self.time_spent = 0\n",
        "        for d in range(-90, 120, 45):\n",
        "            self.check_radar(d)\n",
        "\n",
        "        for d in range(-90, 120, 45):\n",
        "            self.check_radar_for_draw(d)\n",
        "\n",
        "    def draw(self, screen):\n",
        "        screen.blit(self.rotate_surface, self.pos)\n",
        "\n",
        "    def draw_collision(self, screen):\n",
        "        for i in range(4):\n",
        "            x = int(self.four_points[i][0])\n",
        "            y = int(self.four_points[i][1])\n",
        "            pygame.draw.circle(screen, (255, 255, 255), (x, y), 5)\n",
        "\n",
        "    def draw_radar(self, screen):\n",
        "        for r in self.radars_for_draw:\n",
        "            pos, dist = r\n",
        "            pygame.draw.line(screen, (0, 255, 0), self.center, pos, 1)\n",
        "            pygame.draw.circle(screen, (0, 255, 0), pos, 5)\n",
        "\n",
        "    def check_collision(self):\n",
        "        self.is_alive = True\n",
        "        for p in self.four_points:\n",
        "            if self.map.get_at((int(p[0]), int(p[1]))) == (255, 255, 255, 255):\n",
        "                self.is_alive = False\n",
        "                break\n",
        "\n",
        "    def check_radar(self, degree):\n",
        "        len = 0\n",
        "        x = int(self.center[0] + math.cos(math.radians(360 - (self.angle + degree))) * len)\n",
        "        y = int(self.center[1] + math.sin(math.radians(360 - (self.angle + degree))) * len)\n",
        "\n",
        "        while not self.map.get_at((x, y)) == (255, 255, 255, 255) and len < 300:\n",
        "            len = len + 1\n",
        "            x = int(self.center[0] + math.cos(math.radians(360 - (self.angle + degree))) * len)\n",
        "            y = int(self.center[1] + math.sin(math.radians(360 - (self.angle + degree))) * len)\n",
        "\n",
        "        dist = int(math.sqrt(math.pow(x - self.center[0], 2) + math.pow(y - self.center[1], 2)))\n",
        "        self.radars.append([(x, y), dist])\n",
        "\n",
        "\n",
        "    def check_radar_for_draw(self, degree):\n",
        "        len = 0\n",
        "        x = int(self.center[0] + math.cos(math.radians(360 - (self.angle + degree))) * len)\n",
        "        y = int(self.center[1] + math.sin(math.radians(360 - (self.angle + degree))) * len)\n",
        "\n",
        "        while not self.map.get_at((x, y)) == (255, 255, 255, 255) and len < 300:\n",
        "            len = len + 1\n",
        "            x = int(self.center[0] + math.cos(math.radians(360 - (self.angle + degree))) * len)\n",
        "            y = int(self.center[1] + math.sin(math.radians(360 - (self.angle + degree))) * len)\n",
        "\n",
        "        dist = int(math.sqrt(math.pow(x - self.center[0], 2) + math.pow(y - self.center[1], 2)))\n",
        "        self.radars_for_draw.append([(x, y), dist])\n",
        "\n",
        "    def check_checkpoint(self):\n",
        "        p = check_point[self.current_check]\n",
        "        self.prev_distance = self.cur_distance\n",
        "        dist = get_distance(p, self.center)\n",
        "        if dist < 70:\n",
        "            self.current_check += 1\n",
        "            self.prev_distance = 9999\n",
        "            self.check_flag = True\n",
        "            if self.current_check >= len(check_point):\n",
        "                self.current_check = 0\n",
        "                self.goal = True\n",
        "            else:\n",
        "                self.goal = False\n",
        "\n",
        "        self.cur_distance = dist\n",
        "\n",
        "    def update(self):\n",
        "        #check speed\n",
        "        self.speed -= 0.5\n",
        "        if self.speed > 10:\n",
        "            self.speed = 10\n",
        "        if self.speed < 1:\n",
        "            self.speed = 1\n",
        "\n",
        "        #check position\n",
        "        self.rotate_surface = rot_center(self.surface, self.angle)\n",
        "        self.pos[0] += math.cos(math.radians(360 - self.angle)) * self.speed\n",
        "        if self.pos[0] < 20:\n",
        "            self.pos[0] = 20\n",
        "        elif self.pos[0] > screen_width - 120:\n",
        "            self.pos[0] = screen_width - 120\n",
        "\n",
        "        self.distance += self.speed\n",
        "        self.time_spent += 1\n",
        "        self.pos[1] += math.sin(math.radians(360 - self.angle)) * self.speed\n",
        "        if self.pos[1] < 20:\n",
        "            self.pos[1] = 20\n",
        "        elif self.pos[1] > screen_height - 120:\n",
        "            self.pos[1] = screen_height - 120\n",
        "\n",
        "        # caculate 4 collision points\n",
        "        self.center = [int(self.pos[0]) + 50, int(self.pos[1]) + 50]\n",
        "        len = 40\n",
        "        left_top = [self.center[0] + math.cos(math.radians(360 - (self.angle + 30))) * len, self.center[1] + math.sin(math.radians(360 - (self.angle + 30))) * len]\n",
        "        right_top = [self.center[0] + math.cos(math.radians(360 - (self.angle + 150))) * len, self.center[1] + math.sin(math.radians(360 - (self.angle + 150))) * len]\n",
        "        left_bottom = [self.center[0] + math.cos(math.radians(360 - (self.angle + 210))) * len, self.center[1] + math.sin(math.radians(360 - (self.angle + 210))) * len]\n",
        "        right_bottom = [self.center[0] + math.cos(math.radians(360 - (self.angle + 330))) * len, self.center[1] + math.sin(math.radians(360 - (self.angle + 330))) * len]\n",
        "        self.four_points = [left_top, right_top, left_bottom, right_bottom]\n",
        "\n",
        "class PyGame2D:\n",
        "    def __init__(self):\n",
        "        pygame.init()\n",
        "        self.screen = pygame.display.set_mode((screen_width, screen_height))\n",
        "        self.clock = pygame.time.Clock()\n",
        "        self.font = pygame.font.SysFont(\"Arial\", 30)\n",
        "        self.car = Car('car.png', 'map.png', [700, 650])\n",
        "        self.game_speed = 60\n",
        "        self.mode = 0\n",
        "\n",
        "    def action(self, action):\n",
        "        if action == 0:\n",
        "            self.car.speed += 2\n",
        "        if action == 1:\n",
        "            self.car.angle += 5\n",
        "        elif action == 2:\n",
        "            self.car.angle -= 5\n",
        "\n",
        "        self.car.update()\n",
        "        self.car.check_collision()\n",
        "        self.car.check_checkpoint()\n",
        "\n",
        "        self.car.radars.clear()\n",
        "        for d in range(-90, 120, 45):\n",
        "            self.car.check_radar(d)\n",
        "\n",
        "    def evaluate(self):\n",
        "        reward = 0\n",
        "        \"\"\"\n",
        "        if self.car.check_flag:\n",
        "            self.car.check_flag = False\n",
        "            reward = 2000 - self.car.time_spent\n",
        "            self.car.time_spent = 0\n",
        "        \"\"\"\n",
        "        if not self.car.is_alive:\n",
        "            reward = -10000 + self.car.distance\n",
        "\n",
        "        elif self.car.goal:\n",
        "            reward = 10000\n",
        "        return reward\n",
        "\n",
        "    def is_done(self):\n",
        "        if not self.car.is_alive or self.car.goal:\n",
        "            self.car.current_check = 0\n",
        "            self.car.distance = 0\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def observe(self):\n",
        "        # return state\n",
        "        radars = self.car.radars\n",
        "        ret = [0, 0, 0, 0, 0]\n",
        "        for i, r in enumerate(radars):\n",
        "            ret[i] = int(r[1] / 30)\n",
        "\n",
        "        return tuple(ret)\n",
        "\n",
        "    def view(self):\n",
        "        # draw game\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT:\n",
        "                done = True\n",
        "            elif event.type == pygame.KEYDOWN:\n",
        "                if event.key == pygame.K_m:\n",
        "                    self.mode += 1\n",
        "                    self.mode = self.mode % 3\n",
        "\n",
        "        self.screen.blit(self.car.map, (0, 0))\n",
        "\n",
        "\n",
        "        if self.mode == 1:\n",
        "            self.screen.fill((0, 0, 0))\n",
        "\n",
        "        self.car.radars_for_draw.clear()\n",
        "        for d in range(-90, 120, 45):\n",
        "            self.car.check_radar_for_draw(d)\n",
        "\n",
        "        pygame.draw.circle(self.screen, (255, 255, 0), check_point[self.car.current_check], 70, 1)\n",
        "        self.car.draw_collision(self.screen)\n",
        "        self.car.draw_radar(self.screen)\n",
        "        self.car.draw(self.screen)\n",
        "\n",
        "\n",
        "        text = self.font.render(\"Press 'm' to change view mode\", True, (255, 255, 0))\n",
        "        text_rect = text.get_rect()\n",
        "        text_rect.center = (screen_width/2, 100)\n",
        "        self.screen.blit(text, text_rect)\n",
        "\n",
        "\n",
        "\n",
        "        pygame.display.flip()\n",
        "        self.clock.tick(self.game_speed)\n",
        "\n",
        "\n",
        "def get_distance(p1, p2):\n",
        "\treturn math.sqrt(math.pow((p1[0] - p2[0]), 2) + math.pow((p1[1] - p2[1]), 2))\n",
        "\n",
        "def rot_center(image, angle):\n",
        "    orig_rect = image.get_rect()\n",
        "    rot_image = pygame.transform.rotate(image, angle)\n",
        "    rot_rect = orig_rect.copy()\n",
        "    rot_rect.center = rot_image.get_rect().center\n",
        "    rot_image = rot_image.subsurface(rot_rect).copy()\n",
        "    return rot_image\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "\n",
        "class CustomEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.pygame = PyGame2D()\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        self.observation_space = spaces.Box(np.array([0, 0, 0, 0, 0]), np.array([10, 10, 10, 10, 10]), dtype=np.int)\n",
        "\n",
        "    def reset(self):\n",
        "        del self.pygame\n",
        "        self.pygame = PyGame2D()\n",
        "        obs = self.pygame.observe()\n",
        "        return obs\n",
        "\n",
        "    def step(self, action):\n",
        "        self.pygame.action(action)\n",
        "        obs = self.pygame.observe()\n",
        "        reward = self.pygame.evaluate()\n",
        "        done = self.pygame.is_done()\n",
        "        return obs, reward, done, {}\n",
        "\n",
        "    def render(self, mode=\"human\", close=False):\n",
        "        self.pygame.view()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUtqBlXkI9kF"
      },
      "source": [
        "# **Others RL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSWeqvPStrLE"
      },
      "source": [
        "## **TD3 .v1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieZIc55qxY1M",
        "outputId": "6541305c-3e14-4920-ffc4-ac27e7013d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "\u001b[1;31mE: \u001b[0mUnable to locate package python-opengl\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 7,812 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.1 [28.0 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.1 [863 kB]\n",
            "Fetched 7,812 kB in 0s (42.5 MB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 120874 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.1_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.1_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.1) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.1) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !apt install python-opengl\n",
        "    !apt install ffmpeg\n",
        "    !apt install xvfb\n",
        "    !pip install pyvirtualdisplay\n",
        "    from pyvirtualdisplay import Display\n",
        "\n",
        "    # Start virtual display\n",
        "    dis = Display(visible=0, size=(600, 400))\n",
        "    dis.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM8y9_r9xeiB"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import os\n",
        "import random\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDa1CgEOxfnS"
      },
      "outputs": [],
      "source": [
        "if torch.backends.cudnn.enabled:\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed = 777\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5pje4qKxhRi"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(self, obs_dim: int, size: int, batch_size: int = 32):\n",
        "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        act: np.ndarray,\n",
        "        rew: float,\n",
        "        next_obs: np.ndarray,\n",
        "        done: bool,\n",
        "    ):\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
        "        return dict(\n",
        "            obs=self.obs_buf[idxs],\n",
        "            next_obs=self.next_obs_buf[idxs],\n",
        "            acts=self.acts_buf[idxs],\n",
        "            rews=self.rews_buf[idxs],\n",
        "            done=self.done_buf[idxs],\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGGO78ZuxiIf"
      },
      "outputs": [],
      "source": [
        "class GaussianNoise:\n",
        "    \"\"\"Gaussian Noise.\n",
        "    Taken from https://github.com/vitchyr/rlkit\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        action_dim: int,\n",
        "        min_sigma: float = 1.0,\n",
        "        max_sigma: float = 1.0,\n",
        "        decay_period: int = 1000000,\n",
        "    ):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        self.action_dim = action_dim\n",
        "        self.max_sigma = max_sigma\n",
        "        self.min_sigma = min_sigma\n",
        "        self.decay_period = decay_period\n",
        "\n",
        "    def sample(self, t: int = 0) -> float:\n",
        "        \"\"\"Get an action with gaussian noise.\"\"\"\n",
        "        sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(\n",
        "            1.0, t / self.decay_period\n",
        "        )\n",
        "        return np.random.normal(0, sigma, size=self.action_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zQYSWyqxknm"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, in_dim: int, out_dim: int, init_w: float = 3e-3):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.hidden1 = nn.Linear(in_dim, 128)\n",
        "        self.hidden2 = nn.Linear(128, 128)\n",
        "        self.out = nn.Linear(128, out_dim)\n",
        "\n",
        "        self.out.weight.data.uniform_(-init_w, init_w)\n",
        "        self.out.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        x = F.relu(self.hidden1(state))\n",
        "        x = F.relu(self.hidden2(x))\n",
        "        action = self.out(x).tanh()\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, in_dim: int, init_w: float = 3e-3):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.hidden1 = nn.Linear(in_dim, 128)\n",
        "        self.hidden2 = nn.Linear(128, 128)\n",
        "        self.out = nn.Linear(128, 1)\n",
        "\n",
        "        self.out.weight.data.uniform_(-init_w, init_w)\n",
        "        self.out.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        x = torch.cat((state, action), dim=-1)\n",
        "        x = F.relu(self.hidden1(x))\n",
        "        x = F.relu(self.hidden2(x))\n",
        "        value = self.out(x)\n",
        "\n",
        "        return value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "me-jt6Xbxl_p"
      },
      "outputs": [],
      "source": [
        "class TD3Agent:\n",
        "    \"\"\"TD3Agent interacting with environment.\n",
        "\n",
        "    Attribute:\n",
        "        env (gym.Env): openAI Gym environment\n",
        "        actor1 (nn.Module): target actor model to select actions\n",
        "        actor2 (nn.Module): target actor model to select actions\n",
        "        actor_target1 (nn.Module): actor model to predict next actions\n",
        "        actor_target2 (nn.Module): actor model to predict next actions\n",
        "        actor_optimizer (Optimizer): optimizer for training actor\n",
        "        critic1 (nn.Module): critic model to predict state values\n",
        "        critic2 (nn.Module): critic model to predict state values\n",
        "        critic_target1 (nn.Module): target critic model to predict state values\n",
        "        critic_target2 (nn.Module): target critic model to predict state values\n",
        "        critic_optimizer (Optimizer): optimizer for training critic\n",
        "        memory (ReplayBuffer): replay memory to store transitions\n",
        "        batch_size (int): batch size for sampling\n",
        "        gamma (float): discount factor\n",
        "        tau (float): parameter for soft target update\n",
        "        initial_random_steps (int): initial random action steps\n",
        "        exploration_noise (GaussianNoise): gaussian noise for policy\n",
        "        target_policy_noise (GaussianNoise): gaussian noise for target policy\n",
        "        target_policy_noise_clip (float): clip target gaussian noise\n",
        "        device (torch.device): cpu / gpu\n",
        "        transition (list): temporory storage for the recent transition\n",
        "        policy_update_freq (int): update actor every time critic updates this times\n",
        "        total_step (int): total step numbers\n",
        "        is_test (bool): flag to show the current mode (train / test)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        memory_size: int,\n",
        "        batch_size: int,\n",
        "        gamma: float = 0.99,\n",
        "        tau: float = 5e-3,\n",
        "        exploration_noise: float = 0.1,\n",
        "        target_policy_noise: float = 0.2,\n",
        "        target_policy_noise_clip: float = 0.5,\n",
        "        initial_random_steps: int = int(1e4),\n",
        "        policy_update_freq: int = 2,\n",
        "    ):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        obs_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.shape[0]\n",
        "\n",
        "        self.env = env\n",
        "        self.memory = ReplayBuffer(obs_dim, memory_size, batch_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.initial_random_steps = initial_random_steps\n",
        "        self.policy_update_freq = policy_update_freq\n",
        "\n",
        "        # device: cpu / gpu\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(self.device)\n",
        "\n",
        "        # noise\n",
        "        self.exploration_noise = GaussianNoise(\n",
        "            action_dim, exploration_noise, exploration_noise\n",
        "        )\n",
        "        self.target_policy_noise = GaussianNoise(\n",
        "            action_dim, target_policy_noise, target_policy_noise\n",
        "        )\n",
        "        self.target_policy_noise_clip = target_policy_noise_clip\n",
        "\n",
        "        # networks\n",
        "        self.actor = Actor(obs_dim, action_dim).to(self.device)\n",
        "        self.actor_target = Actor(obs_dim, action_dim).to(self.device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "\n",
        "        self.critic1 = Critic(obs_dim + action_dim).to(self.device)\n",
        "        self.critic_target1 = Critic(obs_dim + action_dim).to(self.device)\n",
        "        self.critic_target1.load_state_dict(self.critic1.state_dict())\n",
        "\n",
        "        self.critic2 = Critic(obs_dim + action_dim).to(self.device)\n",
        "        self.critic_target2 = Critic(obs_dim + action_dim).to(self.device)\n",
        "        self.critic_target2.load_state_dict(self.critic2.state_dict())\n",
        "\n",
        "        # concat critic parameters to use one optim\n",
        "        critic_parameters = list(self.critic1.parameters()) + list(\n",
        "            self.critic2.parameters()\n",
        "        )\n",
        "\n",
        "        # optimizer\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "        self.critic_optimizer = optim.Adam(critic_parameters, lr=1e-3)\n",
        "\n",
        "        # transition to store in memory\n",
        "        self.transition = list()\n",
        "\n",
        "        # total steps count\n",
        "        self.total_step = 0\n",
        "\n",
        "        # update step for actor\n",
        "        self.update_step = 0\n",
        "\n",
        "        # mode: train / test\n",
        "        self.is_test = False\n",
        "\n",
        "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Select an action from the input state.\"\"\"\n",
        "        # if initial random action should be conducted\n",
        "        if self.total_step < self.initial_random_steps and not self.is_test:\n",
        "            selected_action = self.env.action_space.sample()\n",
        "        else:\n",
        "            selected_action = (\n",
        "                self.actor(torch.FloatTensor(state).to(self.device))[0]\n",
        "                .detach()\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "\n",
        "        # add noise for exploration during training\n",
        "        if not self.is_test:\n",
        "            noise = self.exploration_noise.sample()\n",
        "            selected_action = np.clip(\n",
        "                selected_action + noise, -1.0, 1.0\n",
        "            )\n",
        "\n",
        "            self.transition = [state, selected_action]\n",
        "\n",
        "        return selected_action\n",
        "\n",
        "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
        "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "        if not self.is_test:\n",
        "            self.transition += [reward, next_state, done]\n",
        "            self.memory.store(*self.transition)\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def update_model(self) -> torch.Tensor:\n",
        "        \"\"\"Update the model by gradient descent.\"\"\"\n",
        "        device = self.device  # for shortening the following lines\n",
        "\n",
        "        samples = self.memory.sample_batch()\n",
        "        states = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
        "        next_states = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
        "        actions = torch.FloatTensor(samples[\"acts\"].reshape(-1, 1)).to(device)\n",
        "        rewards = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
        "        dones = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "        masks = 1 - dones\n",
        "\n",
        "        # get actions with noise\n",
        "        noise = torch.FloatTensor(self.target_policy_noise.sample()).to(device)\n",
        "        clipped_noise = torch.clamp(\n",
        "            noise, -self.target_policy_noise_clip, self.target_policy_noise_clip\n",
        "        )\n",
        "\n",
        "        next_actions = (self.actor_target(next_states) + clipped_noise).clamp(\n",
        "            -1.0, 1.0\n",
        "        )\n",
        "\n",
        "        # min (Q_1', Q_2')\n",
        "        next_values1 = self.critic_target1(next_states, next_actions)\n",
        "        next_values2 = self.critic_target2(next_states, next_actions)\n",
        "        next_values = torch.min(next_values1, next_values2)\n",
        "\n",
        "        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n",
        "        #       = r                       otherwise\n",
        "        curr_returns = rewards + self.gamma * next_values * masks\n",
        "        curr_returns = curr_returns.detach()\n",
        "\n",
        "        # critic loss\n",
        "        values1 = self.critic1(states, actions)\n",
        "        values2 = self.critic2(states, actions)\n",
        "        critic1_loss = F.mse_loss(values1, curr_returns)\n",
        "        critic2_loss = F.mse_loss(values2, curr_returns)\n",
        "\n",
        "        # train critic\n",
        "        critic_loss = critic1_loss + critic2_loss\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        if self.total_step % self.policy_update_freq == 0:\n",
        "            # train actor\n",
        "            actor_loss = -self.critic1(states, self.actor(states)).mean()\n",
        "\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "            # target update\n",
        "            self._target_soft_update()\n",
        "        else:\n",
        "            actor_loss = torch.zeros(1)\n",
        "\n",
        "        return actor_loss.data, critic_loss.data\n",
        "\n",
        "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
        "        \"\"\"Train the agent.\"\"\"\n",
        "        self.is_test = False\n",
        "\n",
        "        state = self.env.reset()\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "        scores = []\n",
        "        score = 0\n",
        "\n",
        "        for self.total_step in range(1, num_frames + 1):\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done = self.step(action)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            # if episode ends\n",
        "            if done:\n",
        "                state = env.reset()\n",
        "                scores.append(score)\n",
        "                score = 0\n",
        "\n",
        "            # if training is ready\n",
        "            if (\n",
        "                len(self.memory) >= self.batch_size\n",
        "                and self.total_step > self.initial_random_steps\n",
        "            ):\n",
        "                actor_loss, critic_loss = self.update_model()\n",
        "                actor_losses.append(actor_loss)\n",
        "                critic_losses.append(critic_loss)\n",
        "\n",
        "            # plotting\n",
        "            if self.total_step % plotting_interval == 0:\n",
        "                self._plot(self.total_step, scores, actor_losses, critic_losses)\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "    def test(self):\n",
        "        \"\"\"Test the agent.\"\"\"\n",
        "        self.is_test = True\n",
        "\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "\n",
        "        frames = []\n",
        "        while not done:\n",
        "            frames.append(self.env.render(mode=\"rgb_array\"))\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done = self.step(action)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "        print(\"score: \", score)\n",
        "        self.env.close()\n",
        "\n",
        "        return frames\n",
        "\n",
        "    def _target_soft_update(self):\n",
        "        \"\"\"Soft-update: target = tau*local + (1-tau)*target.\"\"\"\n",
        "        tau = self.tau\n",
        "        for t_param, l_param in zip(\n",
        "            self.actor_target.parameters(), self.actor.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
        "\n",
        "        for t_param, l_param in zip(\n",
        "            self.critic_target1.parameters(), self.critic1.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
        "\n",
        "        for t_param, l_param in zip(\n",
        "            self.critic_target2.parameters(), self.critic2.parameters()\n",
        "        ):\n",
        "            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n",
        "\n",
        "    def _plot(\n",
        "        self,\n",
        "        frame_idx: int,\n",
        "        scores: List[float],\n",
        "        actor_losses: List[float],\n",
        "        critic_losses: List[float],\n",
        "    ):\n",
        "        \"\"\"Plot the training progresses.\"\"\"\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(30, 5))\n",
        "        plt.subplot(131)\n",
        "        plt.title(\"frame %s. score: %s\" % (frame_idx, np.mean(scores[-10:])))\n",
        "        plt.plot(scores)\n",
        "        plt.subplot(132)\n",
        "        plt.title(\"actor_loss\")\n",
        "        plt.plot(actor_losses)\n",
        "        plt.subplot(133)\n",
        "        plt.title(\"critic_loss\")\n",
        "        plt.plot(critic_losses)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPk3wwcwxnVk"
      },
      "outputs": [],
      "source": [
        "class ActionNormalizer(gym.ActionWrapper):\n",
        "    \"\"\"Rescale and relocate the actions.\"\"\"\n",
        "\n",
        "    def action(self, action: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Change the range (-1, 1) to (low, high).\"\"\"\n",
        "        low = self.action_space.low\n",
        "        high = self.action_space.high\n",
        "\n",
        "        scale_factor = (high - low) / 2\n",
        "        reloc_factor = high - scale_factor\n",
        "\n",
        "        action = action * scale_factor + reloc_factor\n",
        "        action = np.clip(action, low, high)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def reverse_action(self, action: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Change the range (low, high) to (-1, 1).\"\"\"\n",
        "        low = self.action_space.low\n",
        "        high = self.action_space.high\n",
        "\n",
        "        scale_factor = (high - low) / 2\n",
        "        reloc_factor = high - scale_factor\n",
        "\n",
        "        action = (action - reloc_factor) / scale_factor\n",
        "        action = np.clip(action, -1.0, 1.0)\n",
        "\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyIERG2TxqKU",
        "outputId": "595978b2-a8cb-46e5-afb4-8160a3c6600a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[777]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# environment\n",
        "env_id = \"Pendulum-v1\"\n",
        "env = gym.make(env_id)\n",
        "env = ActionNormalizer(env)\n",
        "env.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZjsBTZTxtNW",
        "outputId": "7872b894-93a2-461b-9bbc-24cf30b12292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# parameters\n",
        "num_frames = 100000\n",
        "memory_size = 100000\n",
        "batch_size = 128\n",
        "initial_random_steps = 10000\n",
        "\n",
        "agent = TD3Agent(\n",
        "    env, memory_size, batch_size, initial_random_steps=initial_random_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "0tsF3aPbxu-5",
        "outputId": "8e90e76b-6f8c-439d-8a7c-0a09b7b5b30e"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-4326fee8443c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-717fb12e613f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_frames, plotting_interval)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;31m# plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mplotting_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-717fb12e613f>\u001b[0m in \u001b[0;36m_plot\u001b[0;34m(self, frame_idx, scores, actor_losses, critic_losses)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"critic_loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \"\"\"\n\u001b[1;32m    445\u001b[0m     \u001b[0m_warn_if_gui_out_of_main_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_get_backend_mod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib_inline/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfigure_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             display(\n\u001b[0m\u001b[1;32m     91\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-2>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2364\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2366\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2367\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2368\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2230\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2231\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2232\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2233\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2234\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \"\"\"\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    455\u001b[0m         *pil_kwargs* and *metadata* are forwarded).\n\u001b[1;32m    456\u001b[0m         \"\"\"\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         mpl.image.imsave(\n\u001b[1;32m    459\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    398\u001b[0m              (self.toolbar._wait_cursor_for_draw_cm() if self.toolbar\n\u001b[1;32m    399\u001b[0m               else nullcontext()):\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterizing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3139\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   3141\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[1;32m   3142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0m_draw_rasterized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists_rasterized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3064\u001b[0;31m         mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   3065\u001b[0m             renderer, self, artists, self.figure.suppressComposite)\n\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dashes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dash_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrozen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw_path\u001b[0;34m(self, gc, path, transform, rgbFace)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgbFace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOverflowError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mcant_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "agent.train(num_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7nr7A_tyZr6"
      },
      "source": [
        "## **TD3 .v2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-HuKx9XyfUW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch as T\n",
        "\n",
        "class Buffer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        observationDim: int,\n",
        "        actionDim: int,\n",
        "        device: T.device,\n",
        "        size: int = 1_000_000,\n",
        "    ):\n",
        "        self.device = device\n",
        "        # use a fixed-size buffer to prevent constant list instantiations\n",
        "        self.states = T.zeros((size, observationDim), device=self.device)\n",
        "        self.actions = T.zeros((size, actionDim), device=self.device)\n",
        "        self.rewards = T.zeros(size, device=self.device)\n",
        "        self.nextStates = T.zeros((size, observationDim), device=self.device)\n",
        "        self.doneFlags = T.zeros(size, device=self.device)\n",
        "        # use a pointer to keep track of where in the buffer we are\n",
        "        self.pointer = 0\n",
        "        # use current size to ensure we don't train on any non-existent data points\n",
        "        self.currentSize = 0\n",
        "        self.size = size\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        state: np.ndarray,\n",
        "        action: np.ndarray,\n",
        "        reward: float,\n",
        "        nextState: np.ndarray,\n",
        "        doneFlag: bool,\n",
        "    ):\n",
        "        # store all the data for this transition\n",
        "        ptr = self.pointer\n",
        "        tensorState = T.tensor(state, device=self.device)\n",
        "        tensorAction = T.tensor(action, device=self.device)\n",
        "        tensorNextState = T.tensor(nextState, device=self.device)\n",
        "        self.states[ptr, :] = tensorState\n",
        "        self.actions[ptr, :] = tensorAction\n",
        "        self.rewards[ptr] = reward\n",
        "        self.nextStates[ptr, :] = tensorNextState\n",
        "        self.doneFlags[ptr] = float(doneFlag)\n",
        "        # update the pointer and current size\n",
        "        self.pointer = (self.pointer + 1) % self.size\n",
        "        self.currentSize = min(self.currentSize + 1, self.size)\n",
        "\n",
        "    def getMiniBatch(self, size: int) -> dict[str, T.Tensor]:\n",
        "        # ensure size is not bigger than the current size of the buffer\n",
        "        size = min(size, self.currentSize)\n",
        "        # generate random indices\n",
        "        indices = T.randint(0, self.currentSize, (size,), device=self.device)\n",
        "        # return the mini-batch of transitions\n",
        "        return {\n",
        "            \"states\": self.states[indices, :],\n",
        "            \"actions\": self.actions[indices, :],\n",
        "            \"rewards\": self.rewards[indices],\n",
        "            \"nextStates\": self.nextStates[indices, :],\n",
        "            \"doneFlags\": self.doneFlags[indices],\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcSJsQMfylSb"
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        shape: list,\n",
        "        outputActivation: Callable,\n",
        "        learningRate: float,\n",
        "        device: T.device,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # initialize the network\n",
        "        layers = []\n",
        "        for i in range(1, len(shape)):\n",
        "            dim1 = shape[i - 1]\n",
        "            dim2 = shape[i]\n",
        "            layers.append(nn.Linear(dim1, dim2))\n",
        "            if i < len(shape) - 1:\n",
        "                layers.append(nn.ReLU())\n",
        "        layers.append(outputActivation())\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learningRate)\n",
        "        self.device = device\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state: T.Tensor) -> T.Tensor:\n",
        "        return self.network(state)\n",
        "\n",
        "    def gradientDescentStep(self, loss: T.Tensor, retainGraph: bool = False) -> None:\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=retainGraph)\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K-8B1DEynV2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "from gymnasium.core import Env\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: Env,\n",
        "        learningRate: float,\n",
        "        gamma: float,\n",
        "        tau: float,\n",
        "        shouldLoad: bool = True,\n",
        "        saveFolder: str = \"saved\",\n",
        "    ):\n",
        "        self.observationDim = env.observation_space.shape[0]\n",
        "        self.actionDim = env.action_space.shape[0]\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        # check if the saveFolder path exists\n",
        "        if not os.path.isdir(saveFolder):\n",
        "            os.mkdir(saveFolder)\n",
        "        self.envName = os.path.join(saveFolder, env.name + \".\")\n",
        "        name = self.envName\n",
        "        self.device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
        "        self.buffer = (\n",
        "            pickle.load(open(name + \"Replay\", \"rb\"))\n",
        "            if shouldLoad and os.path.exists(name + \"Replay\")\n",
        "            else Buffer(self.observationDim, self.actionDim, self.device)\n",
        "        )\n",
        "        # initialize the actor and critics\n",
        "        self.actor = (\n",
        "            pickle.load(open(name + \"Actor\", \"rb\"))\n",
        "            if shouldLoad and os.path.exists(name + \"Actor\")\n",
        "            else Network(\n",
        "                [self.observationDim, 256, 256, self.actionDim],\n",
        "                nn.Tanh,\n",
        "                learningRate,\n",
        "                self.device,\n",
        "            )\n",
        "        )\n",
        "        self.critic1 = (\n",
        "            pickle.load(open(name + \"Critic1\", \"rb\"))\n",
        "            if shouldLoad and os.path.exists(name + \"Critic1\")\n",
        "            else Network(\n",
        "                [self.observationDim + self.actionDim, 256, 256, 1],\n",
        "                nn.Identity,\n",
        "                learningRate,\n",
        "                self.device,\n",
        "            )\n",
        "        )\n",
        "        self.critic2 = (\n",
        "            pickle.load(open(name + \"Critic2\", \"rb\"))\n",
        "            if shouldLoad and os.path.exists(name + \"Critic2\")\n",
        "            else Network(\n",
        "                [self.observationDim + self.actionDim, 256, 256, 1],\n",
        "                nn.Identity,\n",
        "                learningRate,\n",
        "                self.device,\n",
        "            )\n",
        "        )\n",
        "        # create target networks\n",
        "        self.targetActor = (\n",
        "            pickle.load(open(name + \"TargetActor\", \"rb\"))\n",
        "            if shouldLoad and os.path.exists(name + \"TargetActor\")\n",
        "            else deepcopy(self.actor)\n",
        "        )\n",
        "        self.targetCritic1 = (\n",
        "            pickle.load(open(name + \"TargetCritic1\", \"rb\"))\n",
        "            if shouldLoad and os.path.exists(name + \"TargetCritic1\")\n",
        "            else deepcopy(self.critic1)\n",
        "        )\n",
        "        self.targetCritic2 = (\n",
        "            pickle.load(open(name + \"TargetCritic2\", \"rb\"))\n",
        "            if shouldLoad and os.path.exists(name + \"TargetCritic2\")\n",
        "            else deepcopy(self.critic2)\n",
        "        )\n",
        "\n",
        "    def getNoisyAction(self, state: np.ndarray, sigma: float) -> np.ndarray:\n",
        "        deterministicAction = self.getDeterministicAction(state)\n",
        "        noise = np.random.normal(0, sigma, deterministicAction.shape)\n",
        "        return np.clip(deterministicAction + noise, -1, +1)\n",
        "\n",
        "    def getDeterministicAction(self, state: np.ndarray) -> np.ndarray:\n",
        "        actions: T.Tensor = self.actor.forward(T.tensor(state, device=self.device))\n",
        "        return actions.cpu().detach().numpy()\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        miniBatchSize: int,\n",
        "        trainingSigma: float,\n",
        "        trainingClip: float,\n",
        "        updatePolicy: bool,\n",
        "    ):\n",
        "        # randomly sample a mini-batch from the replay buffer\n",
        "        miniBatch = self.buffer.getMiniBatch(miniBatchSize)\n",
        "        # create tensors to start generating computational graph\n",
        "        states = miniBatch[\"states\"]\n",
        "        actions = miniBatch[\"actions\"]\n",
        "        rewards = miniBatch[\"rewards\"]\n",
        "        nextStates = miniBatch[\"nextStates\"]\n",
        "        dones = miniBatch[\"doneFlags\"]\n",
        "        # compute the targets\n",
        "        targets = self.computeTargets(\n",
        "            rewards, nextStates, dones, trainingSigma, trainingClip\n",
        "        )\n",
        "        # do a single step on each critic network\n",
        "        Q1Loss = self.computeQLoss(self.critic1, states, actions, targets)\n",
        "        self.critic1.gradientDescentStep(Q1Loss, True)\n",
        "        Q2Loss = self.computeQLoss(self.critic2, states, actions, targets)\n",
        "        self.critic2.gradientDescentStep(Q2Loss)\n",
        "        if updatePolicy:\n",
        "            # do a single step on the actor network\n",
        "            policyLoss = self.computePolicyLoss(states)\n",
        "            self.actor.gradientDescentStep(policyLoss)\n",
        "            # update target networks\n",
        "            self.updateTargetNetwork(self.targetActor, self.actor)\n",
        "            self.updateTargetNetwork(self.targetCritic1, self.critic1)\n",
        "            self.updateTargetNetwork(self.targetCritic2, self.critic2)\n",
        "\n",
        "    def computeTargets(\n",
        "        self,\n",
        "        rewards: T.Tensor,\n",
        "        nextStates: T.Tensor,\n",
        "        dones: T.Tensor,\n",
        "        trainingSigma: float,\n",
        "        trainingClip: float,\n",
        "    ) -> T.Tensor:\n",
        "        targetActions = self.targetActor.forward(nextStates.float())\n",
        "        # create additive noise for target actions\n",
        "        noise = T.normal(0, trainingSigma, targetActions.shape, device=self.device)\n",
        "        clippedNoise = T.clip(noise, -trainingClip, +trainingClip)\n",
        "        targetActions = T.clip(targetActions + clippedNoise, -1, +1)\n",
        "        # compute targets\n",
        "        targetQ1Values = T.squeeze(\n",
        "            self.targetCritic1.forward(T.hstack([nextStates, targetActions]).float())\n",
        "        )\n",
        "        targetQ2Values = T.squeeze(\n",
        "            self.targetCritic2.forward(T.hstack([nextStates, targetActions]).float())\n",
        "        )\n",
        "        targetQValues = T.minimum(targetQ1Values, targetQ2Values)\n",
        "        return rewards + self.gamma * (1 - dones) * targetQValues\n",
        "\n",
        "    def computeQLoss(\n",
        "        self, network: Network, states: T.Tensor, actions: T.Tensor, targets: T.Tensor\n",
        "    ) -> T.Tensor:\n",
        "        # compute the MSE of the Q function with respect to the targets\n",
        "        QValues = T.squeeze(network.forward(T.hstack([states, actions]).float()))\n",
        "        return T.square(QValues - targets).mean()\n",
        "\n",
        "    def computePolicyLoss(self, states: T.Tensor):\n",
        "        actions = self.actor.forward(states.float())\n",
        "        QValues = T.squeeze(self.critic1.forward(T.hstack([states, actions]).float()))\n",
        "        return -QValues.mean()\n",
        "\n",
        "    def updateTargetNetwork(self, targetNetwork: Network, network: Network):\n",
        "        with T.no_grad():\n",
        "            for targetParameter, parameter in zip(\n",
        "                targetNetwork.parameters(), network.parameters()\n",
        "            ):\n",
        "                targetParameter.mul_(1 - self.tau)\n",
        "                targetParameter.add_(self.tau * parameter)\n",
        "\n",
        "    def save(self):\n",
        "        name = self.envName\n",
        "        pickle.dump(self.buffer, open(name + \"Replay\", \"wb\"))\n",
        "        pickle.dump(self.actor, open(name + \"Actor\", \"wb\"))\n",
        "        pickle.dump(self.critic1, open(name + \"Critic1\", \"wb\"))\n",
        "        pickle.dump(self.critic2, open(name + \"Critic2\", \"wb\"))\n",
        "        pickle.dump(self.targetActor, open(name + \"TargetActor\", \"wb\"))\n",
        "        pickle.dump(self.targetCritic1, open(name + \"TargetCritic1\", \"wb\"))\n",
        "        pickle.dump(self.targetCritic2, open(name + \"TargetCritic2\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EfSN3i3eyo-n",
        "outputId": "adb7a372-5cb6-4718-e303-fd6a9071736a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training episode: 1.389768123626709 s\n",
            "testing episode: 3.224743604660034 s\n",
            "episode      1 --- total reward: -108.27 --- running average: -108.27\n",
            "training episode: 1.4740779399871826 s\n",
            "testing episode: 1.80613112449646 s\n",
            "episode      2 --- total reward: -120.63 --- running average: -108.39\n",
            "training episode: 2.845924139022827 s\n",
            "testing episode: 1.812751293182373 s\n",
            "episode      3 --- total reward: -125.50 --- running average: -108.56\n",
            "training episode: 1.3504078388214111 s\n",
            "testing episode: 1.8716399669647217 s\n",
            "episode      4 --- total reward: -121.97 --- running average: -108.69\n",
            "training episode: 1.4610397815704346 s\n",
            "testing episode: 1.853344440460205 s\n",
            "episode      5 --- total reward: -127.49 --- running average: -108.88\n",
            "training episode: 1.4002277851104736 s\n",
            "testing episode: 1.8435988426208496 s\n",
            "episode      6 --- total reward: -127.52 --- running average: -109.07\n",
            "training episode: 1.284496545791626 s\n",
            "testing episode: 1.7311351299285889 s\n",
            "episode      7 --- total reward: -123.36 --- running average: -109.21\n",
            "training episode: 2.342637062072754 s\n",
            "testing episode: 1.6823451519012451 s\n",
            "episode      8 --- total reward: -123.21 --- running average: -109.35\n",
            "training episode: 2.1853392124176025 s\n",
            "testing episode: 3.2974863052368164 s\n",
            "episode      9 --- total reward: -119.64 --- running average: -109.46\n",
            "training episode: 1.9914665222167969 s\n",
            "testing episode: 2.8901960849761963 s\n",
            "episode     10 --- total reward: -118.22 --- running average: -109.54\n",
            "training episode: 2.670506477355957 s\n",
            "testing episode: 64.80962085723877 s\n",
            "episode     11 --- total reward: -167.14 --- running average: -110.12\n",
            "training episode: 8.521506547927856 s\n",
            "testing episode: 64.44171214103699 s\n",
            "episode     12 --- total reward: -113.01 --- running average: -110.15\n",
            "training episode: 37.88418412208557 s\n",
            "testing episode: 2.2078933715820312 s\n",
            "episode     13 --- total reward: -108.63 --- running average: -110.13\n",
            "training episode: 3.6912569999694824 s\n",
            "testing episode: 3.5407779216766357 s\n",
            "episode     14 --- total reward: -106.27 --- running average: -110.09\n",
            "training episode: 1.6068603992462158 s\n",
            "testing episode: 2.086930990219116 s\n",
            "episode     15 --- total reward: -110.74 --- running average: -110.10\n",
            "training episode: 3.4875106811523438 s\n",
            "testing episode: 5.553773880004883 s\n",
            "episode     16 --- total reward: -110.18 --- running average: -110.10\n",
            "training episode: 2.6686131954193115 s\n",
            "testing episode: 6.922647714614868 s\n",
            "episode     17 --- total reward: -129.25 --- running average: -110.29\n",
            "training episode: 2.9563095569610596 s\n",
            "testing episode: 3.824329137802124 s\n",
            "episode     18 --- total reward: -111.24 --- running average: -110.30\n",
            "training episode: 3.924497365951538 s\n",
            "testing episode: 3.4610331058502197 s\n",
            "episode     19 --- total reward: -120.32 --- running average: -110.40\n",
            "training episode: 2.6458253860473633 s\n",
            "testing episode: 2.7777037620544434 s\n",
            "episode     20 --- total reward: -117.42 --- running average: -110.47\n",
            "training episode: 1.6188278198242188 s\n",
            "testing episode: 3.140286445617676 s\n",
            "episode     21 --- total reward: -116.75 --- running average: -110.54\n",
            "training episode: 1.8332328796386719 s\n",
            "testing episode: 5.442877531051636 s\n",
            "episode     22 --- total reward: -127.64 --- running average: -110.71\n",
            "training episode: 1.9801995754241943 s\n",
            "testing episode: 4.1897358894348145 s\n",
            "episode     23 --- total reward: -119.17 --- running average: -110.79\n",
            "training episode: 1.7717247009277344 s\n",
            "testing episode: 64.5212790966034 s\n",
            "episode     24 --- total reward:  -97.81 --- running average: -110.66\n",
            "training episode: 35.316397190093994 s\n",
            "testing episode: 1.7727694511413574 s\n",
            "episode     25 --- total reward: -120.21 --- running average: -110.76\n",
            "training episode: 4.708056211471558 s\n",
            "testing episode: 4.41909122467041 s\n",
            "episode     26 --- total reward: -118.20 --- running average: -110.83\n",
            "training episode: 1.252504587173462 s\n",
            "testing episode: 2.9423797130584717 s\n",
            "episode     27 --- total reward: -115.33 --- running average: -110.88\n",
            "training episode: 2.8728623390197754 s\n",
            "testing episode: 2.699850559234619 s\n",
            "episode     28 --- total reward: -115.67 --- running average: -110.92\n",
            "training episode: 1.924926996231079 s\n",
            "testing episode: 2.2191832065582275 s\n",
            "episode     29 --- total reward: -113.92 --- running average: -110.95\n",
            "training episode: 1.428894281387329 s\n",
            "testing episode: 2.456211805343628 s\n",
            "episode     30 --- total reward: -115.05 --- running average: -110.99\n",
            "training episode: 1.554630994796753 s\n",
            "testing episode: 3.4551570415496826 s\n",
            "episode     31 --- total reward: -118.58 --- running average: -111.07\n",
            "training episode: 2.259333610534668 s\n",
            "testing episode: 2.9755699634552 s\n",
            "episode     32 --- total reward: -117.21 --- running average: -111.13\n",
            "training episode: 2.3273468017578125 s\n",
            "testing episode: 2.979372978210449 s\n",
            "episode     33 --- total reward: -116.99 --- running average: -111.19\n",
            "training episode: 1.7723338603973389 s\n",
            "testing episode: 2.9420855045318604 s\n",
            "episode     34 --- total reward: -116.97 --- running average: -111.25\n",
            "training episode: 2.1741042137145996 s\n",
            "testing episode: 4.142715215682983 s\n",
            "episode     35 --- total reward: -110.17 --- running average: -111.24\n",
            "training episode: 2.4976370334625244 s\n",
            "testing episode: 3.5126712322235107 s\n",
            "episode     36 --- total reward: -109.22 --- running average: -111.22\n",
            "training episode: 3.109605550765991 s\n",
            "testing episode: 3.095137357711792 s\n",
            "episode     37 --- total reward: -116.33 --- running average: -111.27\n",
            "training episode: 1.3548839092254639 s\n",
            "testing episode: 2.6539628505706787 s\n",
            "episode     38 --- total reward: -115.56 --- running average: -111.31\n",
            "training episode: 2.6014668941497803 s\n",
            "testing episode: 2.3766469955444336 s\n",
            "episode     39 --- total reward: -113.70 --- running average: -111.34\n",
            "training episode: 5.083886623382568 s\n",
            "testing episode: 2.9003005027770996 s\n",
            "episode     40 --- total reward: -115.34 --- running average: -111.38\n",
            "training episode: 1.9233002662658691 s\n",
            "testing episode: 2.58272647857666 s\n",
            "episode     41 --- total reward: -114.52 --- running average: -111.41\n",
            "training episode: 16.74818229675293 s\n",
            "testing episode: 64.50361919403076 s\n",
            "episode     42 --- total reward:  -71.81 --- running average: -111.01\n",
            "training episode: 8.756149768829346 s\n",
            "testing episode: 64.4282796382904 s\n",
            "episode     43 --- total reward:  -98.45 --- running average: -110.89\n",
            "training episode: 38.81948137283325 s\n",
            "testing episode: 8.619797229766846 s\n",
            "episode     44 --- total reward: -106.19 --- running average: -110.84\n",
            "training episode: 6.299286127090454 s\n",
            "testing episode: 2.818819761276245 s\n",
            "episode     45 --- total reward: -120.19 --- running average: -110.93\n",
            "training episode: 3.070237159729004 s\n",
            "testing episode: 8.504225730895996 s\n",
            "episode     46 --- total reward: -126.89 --- running average: -111.09\n",
            "training episode: 5.514919996261597 s\n",
            "testing episode: 64.48568892478943 s\n",
            "episode     47 --- total reward:  -84.58 --- running average: -110.83\n",
            "training episode: 37.987602949142456 s\n",
            "testing episode: 64.47126126289368 s\n",
            "episode     48 --- total reward:  -69.95 --- running average: -110.42\n",
            "training episode: 37.89960026741028 s\n",
            "testing episode: 4.269606113433838 s\n",
            "episode     49 --- total reward: -119.27 --- running average: -110.51\n",
            "training episode: 2.229785203933716 s\n",
            "testing episode: 2.9031097888946533 s\n",
            "episode     50 --- total reward: -125.18 --- running average: -110.65\n",
            "training episode: 1.3462896347045898 s\n",
            "testing episode: 4.306188344955444 s\n",
            "episode     51 --- total reward: -108.04 --- running average: -110.63\n",
            "training episode: 34.32686686515808 s\n",
            "testing episode: 4.764154434204102 s\n",
            "episode     52 --- total reward: -109.94 --- running average: -110.62\n",
            "training episode: 5.520526647567749 s\n",
            "testing episode: 2.3812203407287598 s\n",
            "episode     53 --- total reward: -109.19 --- running average: -110.61\n",
            "training episode: 2.498932123184204 s\n",
            "testing episode: 3.5433576107025146 s\n",
            "episode     54 --- total reward: -126.08 --- running average: -110.76\n",
            "training episode: 4.336552143096924 s\n",
            "testing episode: 2.788846731185913 s\n",
            "episode     55 --- total reward: -100.10 --- running average: -110.65\n",
            "training episode: 2.8448245525360107 s\n",
            "testing episode: 2.140599012374878 s\n",
            "episode     56 --- total reward: -110.73 --- running average: -110.65\n",
            "training episode: 2.13543701171875 s\n",
            "testing episode: 2.05334734916687 s\n",
            "episode     57 --- total reward: -107.31 --- running average: -110.62\n",
            "training episode: 2.5174038410186768 s\n",
            "testing episode: 6.409247159957886 s\n",
            "episode     58 --- total reward: -137.70 --- running average: -110.89\n",
            "training episode: 2.440389633178711 s\n",
            "testing episode: 2.695511817932129 s\n",
            "episode     59 --- total reward: -111.39 --- running average: -110.90\n",
            "training episode: 6.230615854263306 s\n",
            "testing episode: 4.592305898666382 s\n",
            "episode     60 --- total reward: -132.60 --- running average: -111.11\n",
            "training episode: 5.254519462585449 s\n",
            "testing episode: 3.1743550300598145 s\n",
            "episode     61 --- total reward: -111.07 --- running average: -111.11\n",
            "training episode: 1.5913567543029785 s\n",
            "testing episode: 3.3897337913513184 s\n",
            "episode     62 --- total reward: -109.72 --- running average: -111.10\n",
            "training episode: 1.5432326793670654 s\n",
            "testing episode: 64.52476954460144 s\n",
            "episode     63 --- total reward: -158.69 --- running average: -111.58\n",
            "training episode: 6.453911781311035 s\n",
            "testing episode: 2.854907274246216 s\n",
            "episode     64 --- total reward: -112.15 --- running average: -111.58\n",
            "training episode: 1.7886455059051514 s\n",
            "testing episode: 64.56238389015198 s\n",
            "episode     65 --- total reward:  -84.04 --- running average: -111.31\n",
            "training episode: 5.2917492389678955 s\n",
            "testing episode: 64.48968935012817 s\n",
            "episode     66 --- total reward: -111.81 --- running average: -111.31\n",
            "training episode: 7.338925123214722 s\n",
            "testing episode: 1.6915028095245361 s\n",
            "episode     67 --- total reward: -103.81 --- running average: -111.24\n",
            "training episode: 1.646270751953125 s\n",
            "testing episode: 1.6066715717315674 s\n",
            "episode     68 --- total reward: -106.01 --- running average: -111.18\n",
            "training episode: 2.6948158740997314 s\n",
            "testing episode: 2.9372851848602295 s\n",
            "episode     69 --- total reward: -117.52 --- running average: -111.25\n",
            "training episode: 1.3264315128326416 s\n",
            "testing episode: 2.662809133529663 s\n",
            "episode     70 --- total reward: -117.91 --- running average: -111.31\n",
            "training episode: 1.2005846500396729 s\n",
            "testing episode: 1.614004135131836 s\n",
            "episode     71 --- total reward: -106.36 --- running average: -111.26\n",
            "training episode: 4.1557981967926025 s\n",
            "testing episode: 5.159032821655273 s\n",
            "episode     72 --- total reward: -130.58 --- running average: -111.46\n",
            "training episode: 1.8782548904418945 s\n",
            "testing episode: 1.6931257247924805 s\n",
            "episode     73 --- total reward: -108.67 --- running average: -111.43\n",
            "training episode: 34.864856481552124 s\n",
            "testing episode: 6.16918683052063 s\n",
            "episode     74 --- total reward: -117.09 --- running average: -111.49\n",
            "training episode: 6.290771722793579 s\n",
            "testing episode: 6.033170223236084 s\n",
            "episode     75 --- total reward: -133.77 --- running average: -111.71\n",
            "training episode: 2.3211758136749268 s\n",
            "testing episode: 3.2184340953826904 s\n",
            "episode     76 --- total reward: -114.46 --- running average: -111.74\n",
            "training episode: 1.757331132888794 s\n",
            "testing episode: 4.969517946243286 s\n",
            "episode     77 --- total reward: -110.93 --- running average: -111.73\n",
            "training episode: 2.220790147781372 s\n",
            "testing episode: 4.310918092727661 s\n",
            "episode     78 --- total reward: -119.62 --- running average: -111.81\n",
            "training episode: 3.1456127166748047 s\n",
            "testing episode: 3.142563819885254 s\n",
            "episode     79 --- total reward: -116.03 --- running average: -111.85\n",
            "training episode: 18.406694173812866 s\n",
            "testing episode: 3.4220757484436035 s\n",
            "episode     80 --- total reward: -115.87 --- running average: -111.89\n",
            "training episode: 1.4465217590332031 s\n",
            "testing episode: 7.386408805847168 s\n",
            "episode     81 --- total reward: -141.76 --- running average: -112.19\n",
            "training episode: 2.1877896785736084 s\n",
            "testing episode: 3.1821529865264893 s\n",
            "episode     82 --- total reward: -112.96 --- running average: -112.20\n",
            "training episode: 4.140344619750977 s\n",
            "testing episode: 3.664024591445923 s\n",
            "episode     83 --- total reward: -121.39 --- running average: -112.29\n",
            "training episode: 2.7605550289154053 s\n",
            "testing episode: 4.434030055999756 s\n",
            "episode     84 --- total reward: -123.41 --- running average: -112.40\n",
            "training episode: 11.98661184310913 s\n",
            "testing episode: 2.6204442977905273 s\n",
            "episode     85 --- total reward: -120.43 --- running average: -112.48\n",
            "training episode: 2.130539655685425 s\n",
            "testing episode: 3.5136168003082275 s\n",
            "episode     86 --- total reward: -127.33 --- running average: -112.63\n",
            "training episode: 2.986271619796753 s\n",
            "testing episode: 3.4341001510620117 s\n",
            "episode     87 --- total reward: -137.70 --- running average: -112.88\n",
            "training episode: 1.5442352294921875 s\n",
            "testing episode: 6.7653419971466064 s\n",
            "episode     88 --- total reward: -136.00 --- running average: -113.11\n",
            "training episode: 2.805955648422241 s\n",
            "testing episode: 4.070498704910278 s\n",
            "episode     89 --- total reward: -125.56 --- running average: -113.23\n",
            "training episode: 2.4839608669281006 s\n",
            "testing episode: 3.6220903396606445 s\n",
            "episode     90 --- total reward: -136.88 --- running average: -113.47\n",
            "training episode: 33.68233895301819 s\n",
            "testing episode: 9.773132562637329 s\n",
            "episode     91 --- total reward: -138.11 --- running average: -113.72\n",
            "training episode: 6.873151063919067 s\n",
            "testing episode: 3.2564268112182617 s\n",
            "episode     92 --- total reward: -122.19 --- running average: -113.80\n",
            "training episode: 4.3841142654418945 s\n",
            "testing episode: 9.136858463287354 s\n",
            "episode     93 --- total reward: -134.27 --- running average: -114.01\n",
            "training episode: 3.0049033164978027 s\n",
            "testing episode: 10.159378051757812 s\n",
            "episode     94 --- total reward: -137.62 --- running average: -114.24\n",
            "training episode: 3.9587626457214355 s\n",
            "testing episode: 8.861772537231445 s\n",
            "episode     95 --- total reward: -136.05 --- running average: -114.46\n",
            "training episode: 2.3745675086975098 s\n",
            "testing episode: 3.421555757522583 s\n",
            "episode     96 --- total reward:  -98.57 --- running average: -114.30\n",
            "training episode: 2.707967519760132 s\n",
            "testing episode: 9.422176837921143 s\n",
            "episode     97 --- total reward: -120.56 --- running average: -114.36\n",
            "training episode: 2.675917863845825 s\n",
            "testing episode: 4.225343942642212 s\n",
            "episode     98 --- total reward: -116.74 --- running average: -114.39\n",
            "training episode: 5.116823673248291 s\n",
            "testing episode: 7.821014404296875 s\n",
            "episode     99 --- total reward: -133.04 --- running average: -114.57\n",
            "training episode: 1.498403787612915 s\n",
            "testing episode: 6.08176326751709 s\n",
            "episode    100 --- total reward: -130.83 --- running average: -114.74\n",
            "training episode: 1.6874418258666992 s\n",
            "testing episode: 6.446357011795044 s\n",
            "episode    101 --- total reward: -130.10 --- running average: -114.89\n",
            "training episode: 3.077983856201172 s\n",
            "testing episode: 1.7777466773986816 s\n",
            "episode    102 --- total reward: -110.42 --- running average: -114.85\n",
            "training episode: 12.569020986557007 s\n",
            "testing episode: 64.47979807853699 s\n",
            "episode    103 --- total reward: -134.48 --- running average: -115.04\n",
            "training episode: 37.99028730392456 s\n",
            "testing episode: 3.0159473419189453 s\n",
            "episode    104 --- total reward: -101.95 --- running average: -114.91\n",
            "training episode: 1.634662389755249 s\n",
            "testing episode: 64.67230677604675 s\n",
            "episode    105 --- total reward:  -81.49 --- running average: -114.58\n",
            "training episode: 34.19403791427612 s\n",
            "testing episode: 64.49390983581543 s\n",
            "episode    106 --- total reward:  -86.89 --- running average: -114.30\n",
            "training episode: 36.98148512840271 s\n",
            "testing episode: 64.49876642227173 s\n",
            "episode    107 --- total reward: -108.15 --- running average: -114.24\n",
            "training episode: 37.203608751297 s\n",
            "testing episode: 64.54346823692322 s\n",
            "episode    108 --- total reward: -103.10 --- running average: -114.13\n",
            "training episode: 7.4951372146606445 s\n",
            "testing episode: 64.52654457092285 s\n",
            "episode    109 --- total reward: -101.52 --- running average: -114.00\n",
            "training episode: 8.072471618652344 s\n",
            "testing episode: 11.176340818405151 s\n",
            "episode    110 --- total reward: -137.26 --- running average: -114.23\n",
            "training episode: 33.977110147476196 s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-caec65ab4600>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDeterministicAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mnextState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcoord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0mscaled_poly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mSCALE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mSCALE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaled_poly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m             \u001b[0mgfxdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maapolygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import gymnasium as gym\n",
        "from os import path\n",
        "from time import time\n",
        "\n",
        "# HYPERPARAMETERS BELOW\n",
        "gamma = 0.99  # discount factor for rewards\n",
        "learningRate = 3e-4  # learning rate for actor and critic networks\n",
        "tau = 0.005  # tracking parameter used to update target networks slowly\n",
        "actionSigma = 0.1  # contributes noise to deterministic policy output\n",
        "trainingSigma = 0.2  # contributes noise to target actions\n",
        "trainingClip = 0.5  # clips target actions to keep them close to true actions\n",
        "miniBatchSize = 100  # how large a mini-batch should be when updating\n",
        "policyDelay = 2  # how many steps to wait before updating the policy\n",
        "resume = True  # resume from previous checkpoint if possible?\n",
        "render = True  # render out the environment on-screen?\n",
        "\n",
        "envName = \"BipedalWalker-v3\"\n",
        "\n",
        "for trial in range(64):\n",
        "    renderMode = \"human\" if render else None\n",
        "    env = gym.make(envName, render_mode=renderMode)\n",
        "    env.name = envName + \"_\" + str(trial)\n",
        "    csvName = env.name + \"-data.csv\"\n",
        "    agent = Agent(env, learningRate, gamma, tau, resume)\n",
        "    state, info = env.reset()\n",
        "    step = 0\n",
        "    runningReward = None\n",
        "\n",
        "    # determine the last episode if we have saved training in progress\n",
        "    numEpisode = 0\n",
        "    if resume and path.exists(csvName):\n",
        "        fileData = list(csv.reader(open(csvName)))\n",
        "        lastLine = fileData[-1]\n",
        "        numEpisode = int(lastLine[0])\n",
        "\n",
        "    start_time = time()\n",
        "\n",
        "    while numEpisode <= 2000:\n",
        "        # choose an action from the agent's policy\n",
        "        action = agent.getNoisyAction(state, actionSigma)\n",
        "        # take a step in the environment and collect information\n",
        "        nextState, reward, terminated, truncated, info = env.step(action)\n",
        "        # store data in buffer\n",
        "        agent.buffer.store(state, action, reward, nextState, terminated)\n",
        "\n",
        "        if terminated or truncated:\n",
        "            elapsed_time = time() - start_time\n",
        "            start_time = time()\n",
        "            print(f\"training episode: {elapsed_time} s\")\n",
        "            numEpisode += 1\n",
        "            # evaluate the deterministic agent on a test episode\n",
        "            sumRewards = 0.0\n",
        "            state, info = env.reset()\n",
        "            terminated = truncated = False\n",
        "            while not terminated and not truncated:\n",
        "                action = agent.getDeterministicAction(state)\n",
        "                nextState, reward, terminated, truncated, info = env.step(action)\n",
        "                if render:\n",
        "                    env.render()\n",
        "                state = nextState\n",
        "                sumRewards += reward\n",
        "            elapsed_time = time() - start_time\n",
        "            start_time = time()\n",
        "            print(f\"testing episode: {elapsed_time} s\")\n",
        "            state, info = env.reset()\n",
        "            # keep a running average to see how well we're doing\n",
        "            runningReward = (\n",
        "                sumRewards\n",
        "                if runningReward is None\n",
        "                else runningReward * 0.99 + sumRewards * 0.01\n",
        "            )\n",
        "            # log progress in csv file\n",
        "            fields = [numEpisode, sumRewards, runningReward]\n",
        "            with open(env.name + \"-data.csv\", \"a\", newline=\"\") as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow(fields)\n",
        "            agent.save()\n",
        "            # print episode tracking\n",
        "            print(\n",
        "                f\"episode {numEpisode:6d} --- \"\n",
        "                + f\"total reward: {sumRewards:7.2f} --- \"\n",
        "                + f\"running average: {runningReward:7.2f}\",\n",
        "                flush=True,\n",
        "            )\n",
        "        else:\n",
        "            state = nextState\n",
        "        step += 1\n",
        "\n",
        "        shouldUpdatePolicy = step % policyDelay == 0\n",
        "        agent.update(miniBatchSize, trainingSigma, trainingClip, shouldUpdatePolicy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22VHxD16zHOP"
      },
      "source": [
        "## **TD3 .v3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIf_cuDFzKIa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, action_dim)\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, state):\n",
        "        a = F.relu(self.l1(state))\n",
        "        a = F.relu(self.l2(a))\n",
        "        a = torch.tanh(self.l3(a)) * self.max_action\n",
        "        return a\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        state_action = torch.cat([state, action], 1)\n",
        "\n",
        "        q = F.relu(self.l1(state_action))\n",
        "        q = F.relu(self.l2(q))\n",
        "        q = self.l3(q)\n",
        "        return q\n",
        "\n",
        "class TD3:\n",
        "    def __init__(self, lr, state_dim, action_dim, max_action):\n",
        "\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "\n",
        "        self.critic_1 = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_1_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
        "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=lr)\n",
        "\n",
        "        self.critic_2 = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_2_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
        "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=lr)\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = np.array(state)\n",
        "        state.astype(np.float32)\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "    def update(self, replay_buffer, n_iter, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay):\n",
        "\n",
        "        for i in range(n_iter):\n",
        "            # Sample a batch of transitions from replay buffer:\n",
        "            state, action_, reward, next_state, done = replay_buffer.sample(batch_size)\n",
        "            state = torch.FloatTensor(state).to(device)\n",
        "            action = torch.FloatTensor(action_).to(device)\n",
        "            reward = torch.FloatTensor(reward).reshape((batch_size,1)).to(device)\n",
        "            next_state = torch.FloatTensor(next_state).to(device)\n",
        "            done = torch.FloatTensor(done).reshape((batch_size,1)).to(device)\n",
        "\n",
        "            # Select next action according to target policy:\n",
        "            noise = torch.FloatTensor(action_).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (self.actor_target(next_state) + noise)\n",
        "            next_action = next_action.clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            # Compute target Q-value:\n",
        "            target_Q1 = self.critic_1_target(next_state, next_action)\n",
        "            target_Q2 = self.critic_2_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + ((1-done) * gamma * target_Q).detach()\n",
        "\n",
        "            # Optimize Critic 1:\n",
        "            current_Q1 = self.critic_1(state, action)\n",
        "            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n",
        "            self.critic_1_optimizer.zero_grad()\n",
        "            loss_Q1.backward()\n",
        "            self.critic_1_optimizer.step()\n",
        "\n",
        "            # Optimize Critic 2:\n",
        "            current_Q2 = self.critic_2(state, action)\n",
        "            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n",
        "            self.critic_2_optimizer.zero_grad()\n",
        "            loss_Q2.backward()\n",
        "            self.critic_2_optimizer.step()\n",
        "\n",
        "            # Delayed policy updates:\n",
        "            if i % policy_delay == 0:\n",
        "                # Compute actor loss:\n",
        "                actor_loss = -self.critic_1(state, self.actor(state)).mean()\n",
        "\n",
        "                # Optimize the actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                # Polyak averaging update:\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "\n",
        "                for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "\n",
        "                for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "\n",
        "\n",
        "    def save(self, directory, name):\n",
        "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, name))\n",
        "        torch.save(self.actor_target.state_dict(), '%s/%s_actor_target.pth' % (directory, name))\n",
        "\n",
        "        torch.save(self.critic_1.state_dict(), '%s/%s_crtic_1.pth' % (directory, name))\n",
        "        torch.save(self.critic_1_target.state_dict(), '%s/%s_critic_1_target.pth' % (directory, name))\n",
        "\n",
        "        torch.save(self.critic_2.state_dict(), '%s/%s_crtic_2.pth' % (directory, name))\n",
        "        torch.save(self.critic_2_target.state_dict(), '%s/%s_critic_2_target.pth' % (directory, name))\n",
        "\n",
        "    def load(self, directory, name):\n",
        "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load('%s/%s_actor_target.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "\n",
        "        self.critic_1.load_state_dict(torch.load('%s/%s_crtic_1.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        self.critic_1_target.load_state_dict(torch.load('%s/%s_critic_1_target.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "\n",
        "        self.critic_2.load_state_dict(torch.load('%s/%s_crtic_2.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        self.critic_2_target.load_state_dict(torch.load('%s/%s_critic_2_target.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "\n",
        "\n",
        "    def load_actor(self, directory, name):\n",
        "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load('%s/%s_actor_target.pth' % (directory, name), map_location=lambda storage, loc: storage))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHgKyflVzNZS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "def train():\n",
        "    ######### Hyperparameters #########\n",
        "    env_name = \"BipedalWalker-v3\"\n",
        "    log_interval = 10           # print avg reward after interval\n",
        "    random_seed = 0\n",
        "    gamma = 0.99                # discount for future rewards\n",
        "    batch_size = 100            # num of transitions sampled from replay buffer\n",
        "    lr = 0.001\n",
        "    exploration_noise = 0.1\n",
        "    polyak = 0.995              # target policy update parameter (1-tau)\n",
        "    policy_noise = 0.2          # target policy smoothing noise\n",
        "    noise_clip = 0.5\n",
        "    policy_delay = 2            # delayed policy updates parameter\n",
        "    max_episodes = 1000         # max num of episodes\n",
        "    max_timesteps = 2000        # max timesteps in one episode\n",
        "    directory = \"./preTrained/{}\".format(env_name) # save trained models\n",
        "    filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
        "    ###################################\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "\n",
        "    policy = TD3(lr, state_dim, action_dim, max_action)\n",
        "    replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
        "\n",
        "    if random_seed:\n",
        "        print(\"Random Seed: {}\".format(random_seed))\n",
        "        env.seed(random_seed)\n",
        "        torch.manual_seed(random_seed)\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "    # logging variables:\n",
        "    avg_reward = 0\n",
        "    ep_reward = 0\n",
        "    log_f = open(\"log.txt\",\"w+\")\n",
        "\n",
        "    # training procedure:\n",
        "    for episode in range(1, max_episodes+1):\n",
        "        state = env.reset()\n",
        "        for t in range(max_timesteps):\n",
        "            # select action and add exploration noise:\n",
        "            action = policy.select_action(state)\n",
        "            action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
        "            action = action.clip(env.action_space.low, env.action_space.high)\n",
        "\n",
        "            # take action in env:\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            replay_buffer.add((state, action, reward, next_state, float(done)))\n",
        "            state = next_state\n",
        "\n",
        "            avg_reward += reward\n",
        "            ep_reward += reward\n",
        "\n",
        "            # if episode is done then update policy:\n",
        "            if done or t==(max_timesteps-1):\n",
        "                policy.update(replay_buffer, t, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay)\n",
        "                break\n",
        "\n",
        "        # logging updates:\n",
        "        log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
        "        log_f.flush()\n",
        "        ep_reward = 0\n",
        "\n",
        "        # if avg reward > 300 then save and stop traning:\n",
        "        if (avg_reward/log_interval) >= 300:\n",
        "            print(\"########## Solved! ###########\")\n",
        "            name = filename + '_solved'\n",
        "            policy.save(directory, name)\n",
        "            log_f.close()\n",
        "            break\n",
        "\n",
        "        if episode > 500:\n",
        "            policy.save(directory, filename)\n",
        "\n",
        "        # print avg reward every log interval:\n",
        "        if episode % log_interval == 0:\n",
        "            avg_reward = int(avg_reward / log_interval)\n",
        "            print(\"Episode: {}\\tAverage Reward: {}\".format(episode, avg_reward))\n",
        "            avg_reward = 0\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjuk9iexzicJ"
      },
      "source": [
        "## **TD3 .v4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOyxgRfF113v"
      },
      "outputs": [],
      "source": [
        "!apt-get update && apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\"\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIxAtu0R1-ip",
        "outputId": "59de3585-3246-4c22-f29f-457f31fd90c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stable_baselines3.__version__='2.1.0'\n"
          ]
        }
      ],
      "source": [
        "import stable_baselines3\n",
        "\n",
        "print(f\"{stable_baselines3.__version__=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqpLEgC7znXQ"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "from stable_baselines3 import TD3\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "import wandb\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpHkIroo28LL"
      },
      "source": [
        "wandb: fb372890f5180a16a9cd2df5b9558e55493cd16c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJY2h1quzqku"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"policy_type\": \"MlpPolicy\",\n",
        "    \"env_name\": \"BipedalWalker-v3\",\n",
        "}\n",
        "run = wandb.init(\n",
        "    project=\"BiPedalWalker-v3\",\n",
        "    config=config,\n",
        "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
        "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
        "    save_code=True,  # optional\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhB2nXjSztOu"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"BipedalWalker-v3\")\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "for _ in range(200):\n",
        "  # Take a random action\n",
        "  action = env.action_space.sample()\n",
        "  print(\"Action taken:\", action)\n",
        "  env.render()\n",
        "\n",
        "  # Do this action in the environment and get\n",
        "  # next_state, reward, done and info\n",
        "  observation, reward, done, info, _ = env.step(action)\n",
        "\n",
        "  # If the game is done (in our case we land, crashed or timeout)\n",
        "  if done:\n",
        "      # Reset the environment\n",
        "      print(\"Environment is reset\")\n",
        "      observation = env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gn_3NtUzxwF"
      },
      "outputs": [],
      "source": [
        "env = make_vec_env(\"BipedalWalker-v3\", n_envs=32)\n",
        "eval_env = make_vec_env(\"BipedalWalker-v3\", n_envs=1)\n",
        "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=300, verbose=1)\n",
        "eval_callback = EvalCallback(eval_env, callback_on_new_best=callback_on_best, verbose=1)\n",
        "model = TD3(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        learning_rate=0.0001,\n",
        "        batch_size=128,\n",
        "        gamma=0.999,\n",
        "        train_freq=32,\n",
        "        gradient_steps=32,\n",
        "        tensorboard_log='model_log/',\n",
        "        verbose=0\n",
        ")\n",
        "env_id = 'BipedalWalker-v3'\n",
        "model.learn(total_timesteps=50000000, callback=[WandbCallback() , eval_callback])\n",
        "model.save('300-Trained.zip')\n",
        "model = TD3.load('30M_Trained.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWk5OxP03j5v"
      },
      "outputs": [],
      "source": [
        "eval_env = gym.make(\"BipedalWalker-v3\")\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=1, deterministic=True, render=True)\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n",
        "eval_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAatMkwQCxk5"
      },
      "source": [
        "## **TD3 .v5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PhKTN31AC0QQ"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import copy\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "from collections import namedtuple, deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH0qgyzCC1r7",
        "outputId": "8cec7110-7be8-4b08-8f5b-fd56b7459bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 100        # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR_ACTOR = 1e-3         # learning rate of the actor\n",
        "LR_CRITIC = 1e-3        # learning rate of the critic\n",
        "UPDATE_EVERY_STEP = 2   # how often to update the target and actor networks\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-PIsAAovC3pL"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "O7pJLds6C4gq"
      },
      "outputs": [],
      "source": [
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size, max_action, seed, l1=400, l2=300):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        #self.seed = torch.manual_seed(seed)\n",
        "        self.l1 = nn.Linear(state_size, l1)\n",
        "        self.l2 = nn.Linear(l1, l2)\n",
        "        self.l3 = nn.Linear(l2, action_size)\n",
        "        self.reset_parameters()\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.l1.weight.data.uniform_(*hidden_init(self.l1))\n",
        "        self.l2.weight.data.uniform_(*hidden_init(self.l2))\n",
        "        self.l3.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.l1(state))\n",
        "        x = F.relu(self.l2(x))\n",
        "        action = self.max_action * torch.tanh(self.l3(x))\n",
        "        return action\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, seed, l1=400, l2=300):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        #self.seed = torch.manual_seed(seed)\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, l1)\n",
        "        self.l2 = nn.Linear(l1, l2)\n",
        "        self.l3 = nn.Linear(action_dim, l2)\n",
        "        self.l4 = nn.Linear(l2, 1)\n",
        "        self.reset_parameters_q1()\n",
        "\n",
        "        self.l5 = nn.Linear(state_dim + action_dim, l1)\n",
        "        self.l6 = nn.Linear(l1, l2)\n",
        "        self.l7 = nn.Linear(action_dim, l2)\n",
        "        self.l8 = nn.Linear(l2, 1)\n",
        "        self.reset_parameters_q2()\n",
        "\n",
        "    def reset_parameters_q1(self):\n",
        "        self.l1.weight.data.uniform_(*hidden_init(self.l1))\n",
        "        self.l2.weight.data.uniform_(*hidden_init(self.l2))\n",
        "        self.l3.weight.data.uniform_(*hidden_init(self.l3))\n",
        "        self.l4.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def reset_parameters_q2(self):\n",
        "        self.l5.weight.data.uniform_(*hidden_init(self.l5))\n",
        "        self.l6.weight.data.uniform_(*hidden_init(self.l6))\n",
        "        self.l7.weight.data.uniform_(*hidden_init(self.l7))\n",
        "        self.l8.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "\n",
        "        s1 = F.relu(self.l1(torch.cat([state, action], dim=1)))\n",
        "        s1 = F.relu(self.l2(s1))\n",
        "        a1 = F.relu(self.l3(action))\n",
        "        s1 = s1 + a1\n",
        "        q1 = self.l4(s1)\n",
        "\n",
        "        s2 = F.relu(self.l1(torch.cat([state, action], dim=1)))\n",
        "        s2 = F.relu(self.l2(s2))\n",
        "        a2 = F.relu(self.l3(action))\n",
        "        s2 = s2 + a2\n",
        "        q2 = self.l4(s2)\n",
        "        return q1, q2\n",
        "\n",
        "    def Q1(self, state, action):\n",
        "        s1 = F.relu(self.l1(torch.cat([state, action], dim=1)))\n",
        "        s1 = F.relu(self.l2(s1))\n",
        "        a1 = F.relu(self.l3(action))\n",
        "        s1 = s1 + a1\n",
        "        q1 = self.l4(s1)\n",
        "        return q1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "v3Cd3SiWC5Y5"
      },
      "outputs": [],
      "source": [
        "from numpy import inf\n",
        "\n",
        "class TD3Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, max_action, min_action, random_seed, noise=0.2, noise_std=0.1, noise_clip=0.5):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            max_action (ndarray): the maximum valid value for each action vector\n",
        "            min_action (ndarray): the minimum valid value for each action vector\n",
        "            random_seed (int): random seed\n",
        "            noise (float): the range to generate random noise while learning\n",
        "            noise_std (float): the range to generate random noise while performing action\n",
        "            noise_clip (float): to clip random noise into this range\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.max_action = max_action\n",
        "        self.min_action = min_action\n",
        "        self.noise = noise\n",
        "        self.noise_std = noise_std\n",
        "        self.noise_clip = noise_clip\n",
        "\n",
        "        # Actor Network (w/ Target Network)\n",
        "        self.actor = Actor(state_size, action_size, float(max_action[0]), random_seed).to(device)\n",
        "        self.actor_target = Actor(state_size, action_size, float(max_action[0]), random_seed).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
        "\n",
        "        # Critic Network (w/ Target Network)\n",
        "        self.critic = Critic(state_size, action_size, random_seed).to(device)\n",
        "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Save experience in replay memory\"\"\"\n",
        "\n",
        "        if isinstance(state, np.ndarray):\n",
        "            states = state\n",
        "        elif isinstance(state, tuple):\n",
        "            states = np.array(state[0], dtype=np.float32)\n",
        "\n",
        "        self.memory.add(states, action, reward, next_state, done)\n",
        "\n",
        "    def predict(self, states, add_noise=True):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        if isinstance(states, np.ndarray):\n",
        "            convert_state = states\n",
        "        elif isinstance(states, tuple):\n",
        "            convert_state = np.array(states[0], dtype=np.float32)\n",
        "\n",
        "        state = torch.from_numpy(convert_state).float().to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action = self.actor(state).cpu().data.numpy()\n",
        "\n",
        "        return action.clip(self.min_action[0], self.max_action[0])\n",
        "\n",
        "    def learn(self, n_iteraion, gamma=GAMMA):\n",
        "        \"\"\" Update policy and value parameters using given batch of experience tuples.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            n_iteraion (int): the number of iterations to train network\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "\n",
        "        if len(self.memory) > BATCH_SIZE:\n",
        "            average_Q = 0\n",
        "            max_Q = -inf\n",
        "            average_loss = 0\n",
        "\n",
        "            for i in range(n_iteraion):\n",
        "                state, action, reward, next_state, done = self.memory.sample()\n",
        "\n",
        "                action_ = action.cpu().numpy()\n",
        "\n",
        "                # ---------------------------- update critic ---------------------------- #\n",
        "                # Get predicted next-state actions and Q values from target models\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    # Generate a random noise\n",
        "                    noise = torch.FloatTensor(action_).data.normal_(0, self.noise).to(device)\n",
        "                    noise = noise.clamp(-self.noise_clip, self.noise_clip)\n",
        "                    actions_next = (self.actor_target(next_state) + noise).clamp(self.min_action[0].astype(float), self.max_action[0].astype(float))\n",
        "\n",
        "                    Q1_targets_next, Q2_targets_next = self.critic_target(next_state, actions_next)\n",
        "\n",
        "                    Q_targets_next = torch.min(Q1_targets_next, Q2_targets_next)\n",
        "\n",
        "                    average_Q += torch.mean(Q_targets_next)\n",
        "                    max_Q = max(max_Q, torch.max(Q_targets_next))\n",
        "\n",
        "                    # Compute Q targets for current states (y_i)\n",
        "                    Q_targets = reward + (gamma * Q_targets_next * (1 - done)).detach()\n",
        "\n",
        "                # Compute critic loss\n",
        "                Q1_expected, Q2_expected = self.critic(state, action)\n",
        "                critic_loss = F.mse_loss(Q1_expected, Q_targets) + F.mse_loss(Q2_expected, Q_targets)\n",
        "\n",
        "                # Minimize the loss\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                if i % UPDATE_EVERY_STEP == 0:\n",
        "                    # ---------------------------- update actor ---------------------------- #\n",
        "                    # Compute actor loss\n",
        "                    actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\n",
        "                    # Minimize the loss\n",
        "                    self.actor_optimizer.zero_grad()\n",
        "                    actor_loss.backward()\n",
        "                    self.actor_optimizer.step()\n",
        "\n",
        "                    # ----------------------- update target networks ----------------------- #\n",
        "                    self.soft_update(self.critic, self.critic_target, TAU)\n",
        "                    self.soft_update(self.actor, self.actor_target, TAU)\n",
        "\n",
        "                average_loss += critic_loss\n",
        "\n",
        "            loss = average_loss / n_iteraion\n",
        "            average_policy = average_Q / n_iteraion\n",
        "            max_policy = max_Q\n",
        "\n",
        "            return loss, average_policy, max_policy\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        Params\n",
        "        ======\n",
        "            local_model: PyTorch model (weights will be copied from)\n",
        "            target_model: PyTorch model (weights will be copied to)\n",
        "            tau (float): interpolation parameter\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "    def save(self, filename):\n",
        "          \"\"\" Save the model \"\"\"\n",
        "          torch.save(self.critic.state_dict(), filename + \"_critic\")\n",
        "          torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
        "\n",
        "          torch.save(self.actor.state_dict(), filename + \"_actor\")\n",
        "          torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
        "\n",
        "    def load(self, filename):\n",
        "          \"\"\" Load the model \"\"\"\n",
        "          self.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
        "          self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
        "          self.critic_target = copy.deepcopy(self.critic)\n",
        "\n",
        "          self.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
        "          self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
        "          self.actor_target = copy.deepcopy(self.actor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9tSI3g3C9K0",
        "outputId": "a59ab832-556a-4706-a381-048557243c7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho de cada ação: 4\n",
            "Ligação superior de cada ação: 1.0\n",
            "Ligação inferior de cada ação: -1.0\n",
            "Cada um observa um estado com comprimento: 24\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"BipedalWalker-v3\")\n",
        "\n",
        "# tamanho de cada ação\n",
        "action_size = env.action_space\n",
        "print('Tamanho de cada ação:', action_size.shape[0])\n",
        "\n",
        "# vínculo superior de cada ação\n",
        "upper_bond = env.action_space.high\n",
        "print('Ligação superior de cada ação:', upper_bond[0])\n",
        "\n",
        "# vínculo inferior de cada ação\n",
        "lower_bond = env.action_space.low\n",
        "print('Ligação inferior de cada ação:', lower_bond[0])\n",
        "\n",
        "#examina o espaço de estados\n",
        "states = env.observation_space\n",
        "state_size = states.shape[0]\n",
        "print('Cada um observa um estado com comprimento: {}'.format(state_size))\n",
        "\n",
        "agent = TD3Agent(state_size=env.observation_space.shape[0], \\\n",
        "                 action_size=env.action_space.shape[0], \\\n",
        "                 max_action=env.action_space.high, \\\n",
        "                 min_action=env.action_space.low, random_seed=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493,
          "referenced_widgets": [
            "b69214b87ead4a4286a6c9de01edc08b",
            "924d5c11570842838eb195466ecabf9f",
            "bc1a7ea7261c488a87ed5b7b4c898c3f",
            "4dc5ed34b1a94652a66a96e185bc19ec",
            "2bcf356025e540a1b5f372fcffdd040d",
            "efd8a533c7374341ba1186800c74cb63",
            "a7bb1a09f351488d9634a6fda46897b5",
            "41c81c9fdcf64e8fa1c07a80e38a57c8"
          ]
        },
        "id": "RVNLbR1PNH44",
        "outputId": "6eabff15-9154-47d1-d5d3-3f727395f989"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:y1lhczey) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.400783…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b69214b87ead4a4286a6c9de01edc08b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Average Q</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇█</td></tr><tr><td>Max. Q</td><td>▁▂▂▂▂▂▂▂▂▂▃▃▃▃▆▅▇▇▆▆▇▇▇▇▇▇▇▇▇███▇█▇▇█▇██</td></tr><tr><td>loss</td><td>▁▁█▂▃▃▁▃▂▁▂▅▂▂▅█▅▃▃▂▃▅▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂</td></tr><tr><td>score</td><td>▁▄▅▅▅▆▆▆▆▆▆▆▅▄▃▃▃▃▃▄▄▅▆▆▇▇█▇▅▅▄▄▅▄▄▅▅▆▇▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Average Q</td><td>29.9844</td></tr><tr><td>Max. Q</td><td>46.70114</td></tr><tr><td>loss</td><td>1.91763</td></tr><tr><td>score</td><td>-108.87742</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">effortless-bird-25</strong> at: <a href='https://wandb.ai/grottimeireles/td3/runs/y1lhczey' target=\"_blank\">https://wandb.ai/grottimeireles/td3/runs/y1lhczey</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20231028_183830-y1lhczey/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:y1lhczey). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231028_194004-5ubs1zm7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/grottimeireles/td3/runs/5ubs1zm7' target=\"_blank\">ancient-terrain-26</a></strong> to <a href='https://wandb.ai/grottimeireles/td3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/grottimeireles/td3' target=\"_blank\">https://wandb.ai/grottimeireles/td3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/grottimeireles/td3/runs/5ubs1zm7' target=\"_blank\">https://wandb.ai/grottimeireles/td3/runs/5ubs1zm7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/grottimeireles/td3/runs/5ubs1zm7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f1beb016f80>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"td3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERQ6QREQC99V",
        "outputId": "9764a255-46d2-4adb-8b0d-d7d28baa4e46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: -124.36\n",
            "Episode 200\tAverage Score: -117.73\n",
            "Episode 300\tAverage Score: -122.95\n",
            "Episode 350\tAverage Score: -130.82\tScore: -120.73"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "def td3(n_episodes=1000, max_t=2000):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    solved = False\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.predict(state)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            if done or t==(max_t-1):\n",
        "                loss, q, max = agent.learn(t)\n",
        "                break\n",
        "\n",
        "        scores_deque.append(score)\n",
        "        scores.append(score)\n",
        "        mean_score = np.mean(scores_deque)\n",
        "\n",
        "        wandb.log({'score': mean_score, 'loss': loss, 'Average Q': q, 'Max. Q': max}, step=i_episode)\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, mean_score, score), end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "        if mean_score >= 300 and solved == False:\n",
        "            solved = True\n",
        "            print('\\rSolved at Episode {} !\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "\n",
        "            agent.save(\"checkpoint\")\n",
        "\n",
        "    return scores\n",
        "\n",
        "scores = td3()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "gkNza9le_wuU",
        "h524c52m_oWK",
        "WpZo5QBk-V3n",
        "toYwUl7G-dwD",
        "WBTA4Y6rxDiU",
        "wSWeqvPStrLE",
        "I7nr7A_tyZr6",
        "22VHxD16zHOP",
        "wjuk9iexzicJ"
      ],
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOg1nOpMB/Z7zu1Nf1OjEfi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b69214b87ead4a4286a6c9de01edc08b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_924d5c11570842838eb195466ecabf9f",
              "IPY_MODEL_bc1a7ea7261c488a87ed5b7b4c898c3f"
            ],
            "layout": "IPY_MODEL_4dc5ed34b1a94652a66a96e185bc19ec"
          }
        },
        "924d5c11570842838eb195466ecabf9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bcf356025e540a1b5f372fcffdd040d",
            "placeholder": "​",
            "style": "IPY_MODEL_efd8a533c7374341ba1186800c74cb63",
            "value": "0.001 MB of 0.011 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "bc1a7ea7261c488a87ed5b7b4c898c3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7bb1a09f351488d9634a6fda46897b5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41c81c9fdcf64e8fa1c07a80e38a57c8",
            "value": 0.10370745714044421
          }
        },
        "4dc5ed34b1a94652a66a96e185bc19ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bcf356025e540a1b5f372fcffdd040d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efd8a533c7374341ba1186800c74cb63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7bb1a09f351488d9634a6fda46897b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41c81c9fdcf64e8fa1c07a80e38a57c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
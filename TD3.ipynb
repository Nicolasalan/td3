{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicolasalan/td3/blob/main/TD3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUkL1MTSs3Nn"
      },
      "source": [
        "## **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdruBNVvMQ08",
        "outputId": "19c3309d-749f-4913-e9ae-4a6442daf17e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add script in browser console: `inspect` => `console` => add script.\n",
        "\n",
        "```javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Conectado\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```"
      ],
      "metadata": {
        "id": "iSmLN3pgE2Fu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkNza9le_wuU"
      },
      "source": [
        "## **Install**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Qa_nNJfpVwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbcfd3e3-ff9c-4921-ffe6-e9c0ca14b7c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,296 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 121752 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349151 sha256=5aed02cbe7cf56d7f133e014816da4b09b0d56a6dc31db609169b16adc7001a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.2.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.45.0-py2.py3-none-any.whl (267 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.43 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.45.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.6\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install swig\n",
        "\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]\n",
        "\n",
        "!pip install torch\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import**"
      ],
      "metadata": {
        "id": "BwxK-YPh0EKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import torch\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch versions not as required, installing nightly versions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjrS5wB7AOfA",
        "outputId": "9b3ab0be-4d2d-4095-8f2a-ec53d5948aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.2.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we're using a NVIDIA GPU\n",
        "if torch.cuda.is_available():\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find(\"failed\") >= 0:\n",
        "    print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n",
        "\n",
        "  # Get GPU name\n",
        "  gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "  gpu_name = gpu_name[1]\n",
        "  GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving\n",
        "  print(f'GPU name: {GPU_NAME}')\n",
        "\n",
        "  # Get GPU capability score\n",
        "  GPU_SCORE = torch.cuda.get_device_capability()\n",
        "  print(f\"GPU capability score: {GPU_SCORE}\")\n",
        "  if GPU_SCORE >= (8, 0):\n",
        "    print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")\n",
        "  else:\n",
        "    print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")\n",
        "\n",
        "  # Print GPU info\n",
        "  print(f\"GPU information:\\n{gpu_info}\")\n",
        "\n",
        "else:\n",
        "  print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFiWTJo9CzSf",
        "outputId": "fbb9707c-3a52-4afb-b651-5bd8361a7eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check available GPU memory and total GPU memory\n",
        "try:\n",
        "  total_free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\n",
        "  print(f\"Total free GPU memory: {round(total_free_gpu_memory * 1e-9, 3)} GB\")\n",
        "  print(f\"Total GPU memory: {round(total_gpu_memory * 1e-9, 3)} GB\")\n",
        "except:\n",
        "  print(\"Please check that you have an NVIDIA GPU and installed a driver from \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13d_P3rnC1HJ",
        "outputId": "4aa4873b-253d-404c-8959-065b43a59d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please check that you have an NVIDIA GPU and installed a driver from \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size depending on amount of GPU memory\n",
        "try:\n",
        "  total_free_gpu_memory_gb = round(total_free_gpu_memory * 1e-9, 3)\n",
        "  if total_free_gpu_memory_gb >= 16:\n",
        "    BATCH_SIZE = 128 # Note: you could experiment with higher values here if you like.\n",
        "    print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE}\")\n",
        "  else:\n",
        "    BATCH_SIZE = 32\n",
        "    print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE}\")\n",
        "except:\n",
        "  BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "Y2qHR2UKDojx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH0qgyzCC1r7"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 100        # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR_ACTOR = 1e-3         # learning rate of the actor\n",
        "LR_CRITIC = 1e-3        # learning rate of the critic\n",
        "UPDATE_EVERY_STEP = 2   # how often to update the target and actor networks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Replay**"
      ],
      "metadata": {
        "id": "ZADS3Hz_NdzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple, deque\n",
        "from typing import Tuple\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, buffer_size: int, batch_size: int):\n",
        "\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    def add(self, state: np.ndarray, action: np.ndarray, reward: np.float64, next_state: np.float32, done: bool) -> None:\n",
        "        \"\"\"Add experiences to the buffer\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state (np.ndarray): agent states\n",
        "            action (np.ndarray): agent action\n",
        "            reward (np.float64): agent reward\n",
        "            next_state (np.ndarray): agent next_state\n",
        "        \"\"\"\n",
        "\n",
        "        assert isinstance(state, np.ndarray), \"State is not of data structure (np.ndarray) in REPLAY BUFFER -> state: {}.\".format(type(state))\n",
        "        assert isinstance(action, np.ndarray), \"Action is not of data structure (np.ndarray) in REPLAY BUFFER -> action: {}.\".format(type(action))\n",
        "        assert isinstance(next_state, np.ndarray), \"Next State is not of data structure (np.ndarray) in REPLAY BUFFER -> next state: {}.\".format(type(next_state))\n",
        "\n",
        "        assert isinstance(state[0], np.float32), \"State is not of type (np.float32) in REPLAY BUFFER -> state type: {}.\".format(type(state))\n",
        "        assert isinstance(action[0], np.float32), \"Action is not of type (np.float32) in REPLAY BUFFER -> action type: {}.\".format(type(action))\n",
        "        assert isinstance(reward, (int, np.float64)), \"Reward is not of type (np.float64 / int) in REPLAY BUFFER -> reward: {}.\".format(type(reward))\n",
        "        assert isinstance(next_state[0], np.float32), \"Next State is not of type (np.float32) in REPLAY BUFFER -> next state type: {}.\".format(type(next_state))\n",
        "        assert isinstance(done, bool), \"Done is not of type (bool) in REPLAY BUFFER -> done type: {}.\".format(type(done))\n",
        "\n",
        "        assert state.shape[0] == 24, \"The size of the state is not (24) in REPLAY BUFFER -> state size: {}.\".format(state.shape[0])\n",
        "        assert action.shape[0] == 4, \"The size of the action is not (4) in REPLAY BUFFER -> action size: {}.\".format(state.shape[0])\n",
        "        if isinstance(reward, np.float64):\n",
        "          assert reward.size == 1, \"The size of the reward is not (1) in REPLAY BUFFER -> reward size: {}.\".format(reward.size)\n",
        "        assert next_state.shape[0] == 24, \"The size of the next_state is not (24) in REPLAY BUFFER -> next_state size: {}.\".format(next_state.shape[0])\n",
        "\n",
        "        assert state.ndim == 1, \"The ndim of the state is not (1) in REPLAY BUFFER -> state ndim: {}.\".format(state.ndim)\n",
        "        assert action.ndim == 1, \"The ndim of the action is not (1) in REPLAY BUFFER -> action ndim: {}.\".format(state.ndim)\n",
        "        if isinstance(reward, np.float64):\n",
        "          assert reward.ndim == 0, \"The ndim of the reward is not (0) in REPLAY BUFFER -> reward ndim: {}.\".format(reward.ndim)\n",
        "        assert next_state.ndim == 1, \"The ndim of the next_state is not (1) in REPLAY BUFFER -> next_state ndim: {}.\".format(next_state.ndim)\n",
        "\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None])).int().to(device)\n",
        "\n",
        "        assert isinstance(states, torch.Tensor), \"State is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(actions, torch.Tensor), \"Actions is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(rewards, torch.Tensor), \"Rewards is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(next_states, torch.Tensor), \"Next states is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "        assert isinstance(dones, torch.Tensor), \"Dones is not of type torch.Tensor in REPLAY BUFFER.\"\n",
        "\n",
        "        assert states.dtype == torch.float32, \"The (state) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(states.dtype)\n",
        "        assert actions.dtype == torch.float32,\"The (actions) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(actions.dtype)\n",
        "        assert rewards.dtype == torch.float32, \"The (rewards) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(rewards.dtype)\n",
        "        assert next_states.dtype == torch.float32, \"The (next_states) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(next_states.dtype)\n",
        "        assert dones.dtype == torch.int, \"The (dones) tensor elements are not of type torch.float32 in the REPLAY BUFFER -> {}.\".format(dones.dtype)\n",
        "\n",
        "        # TODO\n",
        "        # assert all(tensor.device.type == DEVICE for tensor in [states, actions, rewards, next_states, dones]), \"Each tensor must be on the same device in REPLAY BUFFER\"\n",
        "\n",
        "        return (\n",
        "            states, actions, rewards, next_states, dones\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> None:\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "Eqt5PjlK6aAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6376e05e-1df5-44e8-d1fb-a708923513fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "8wHw8OOk38ps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7pJLds6C4gq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size: int, action_size: int, max_action: float, l1=400, l2=300):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_size, l1)\n",
        "        self.l2 = nn.Linear(l1, l2)\n",
        "        self.l3 = nn.Linear(l2, action_size)\n",
        "        self.reset_parameters()\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.l1.weight.data.uniform_(*hidden_init(self.l1))\n",
        "        self.l2.weight.data.uniform_(*hidden_init(self.l2))\n",
        "        self.l3.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state) -> torch.Tensor:\n",
        "\n",
        "\n",
        "        assert isinstance(state, torch.Tensor), \"State is not of type torch.Tensor in ACTOR.\"\n",
        "        assert state.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in ACTOR.\"\n",
        "        assert state.shape[0] <= 24 or state.shape[0] >= BATCH_SIZE, \"The tensor shape is not torch.Size([24]) in ACTOR.\"\n",
        "        assert str(state.device.type) == str(DEVICE), \"The state must be on the same device in ACTOR.\"\n",
        "\n",
        "        x = F.relu(self.l1(state))\n",
        "        x = F.relu(self.l2(x))\n",
        "        action = self.max_action * torch.tanh(self.l3(x))\n",
        "        return action\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, l1=400, l2=300):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, l1)\n",
        "        self.l2 = nn.Linear(l1, l2)\n",
        "        self.l3 = nn.Linear(action_dim, l2)\n",
        "        self.l4 = nn.Linear(l2, 1)\n",
        "        self.reset_parameters_q1()\n",
        "\n",
        "        self.l5 = nn.Linear(state_dim + action_dim, l1)\n",
        "        self.l6 = nn.Linear(l1, l2)\n",
        "        self.l7 = nn.Linear(action_dim, l2)\n",
        "        self.l8 = nn.Linear(l2, 1)\n",
        "        self.reset_parameters_q2()\n",
        "\n",
        "    def reset_parameters_q1(self):\n",
        "        self.l1.weight.data.uniform_(*hidden_init(self.l1))\n",
        "        self.l2.weight.data.uniform_(*hidden_init(self.l2))\n",
        "        self.l3.weight.data.uniform_(*hidden_init(self.l3))\n",
        "        self.l4.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def reset_parameters_q2(self):\n",
        "        self.l5.weight.data.uniform_(*hidden_init(self.l5))\n",
        "        self.l6.weight.data.uniform_(*hidden_init(self.l6))\n",
        "        self.l7.weight.data.uniform_(*hidden_init(self.l7))\n",
        "        self.l8.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state, action) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        assert isinstance(state, torch.Tensor), \"State is not of type torch.Tensor in CRITIC.\"\n",
        "        assert state.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert state.shape[0] == BATCH_SIZE, \"The tensor shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(state.device.type) == str(DEVICE), \"The state must be on the same device  in CRITIC.\"\n",
        "\n",
        "        assert isinstance(action, torch.Tensor), \"Action is not of type torch.Tensor in CRITIC.\"\n",
        "        assert action.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert action.shape[0] == BATCH_SIZE, \"The action shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(action.device.type) == str(DEVICE), \"The action must be on the same device  in CRITIC.\"\n",
        "\n",
        "        s = torch.cat([state, action], dim=1)\n",
        "\n",
        "        s1 = F.relu(self.l1(s))\n",
        "        s1 = F.relu(self.l2(s1))\n",
        "        a1 = F.relu(self.l3(action))\n",
        "        s1 = s1 + a1\n",
        "        q1 = self.l4(s1)\n",
        "\n",
        "        s2 = F.relu(self.l5(s))\n",
        "        s2 = F.relu(self.l6(s2))\n",
        "        a2 = F.relu(self.l7(action))\n",
        "        s2 = s2 + a2\n",
        "        q2 = self.l8(s2)\n",
        "        return (q1, q2)\n",
        "\n",
        "    def Q1(self, state, action) -> torch.Tensor:\n",
        "        assert isinstance(state, torch.Tensor), \"State is not of type torch.Tensor in CRITIC.\"\n",
        "        assert state.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert state.shape[0] == BATCH_SIZE, \"The tensor shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(state.device.type) == str(DEVICE), \"The state must be on the same device in CRITIC.\"\n",
        "\n",
        "        assert isinstance(action, torch.Tensor), \"Action is not of type torch.Tensor in CRITIC.\"\n",
        "        assert action.dtype == torch.float32, \"Tensor elements are not of type torch.float32 in CRITIC.\"\n",
        "        assert action.shape[0] == BATCH_SIZE, \"The action shape is not torch.Size([100]) in CRITIC.\"\n",
        "        assert str(action.device.type) == str(DEVICE), \"The action must be on the same device in CRITIC.\"\n",
        "\n",
        "        s = torch.cat([state, action], dim=1)\n",
        "\n",
        "        s1 = F.relu(self.l1(s))\n",
        "        s1 = F.relu(self.l2(s1))\n",
        "        a1 = F.relu(self.l3(action))\n",
        "        s1 = s1 + a1\n",
        "        q1 = self.l4(s1)\n",
        "        return q1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Agent**"
      ],
      "metadata": {
        "id": "JPdq-Y3U3_0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3Cd3SiWC5Y5"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "from numpy import inf\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class TD3Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size: int, action_size: int, max_action: int, min_action: int, noise=0.2, noise_std=0.1, noise_clip=0.5, continue_training=False):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            max_action (ndarray): the maximum valid value for each action vector\n",
        "            min_action (ndarray): the minimum valid value for each action vector\n",
        "            noise (float): the range to generate random noise while learning\n",
        "            noise_std (float): the range to generate random noise while performing action\n",
        "            noise_clip (float): to clip random noise into this range\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.max_action = max_action\n",
        "        self.min_action = min_action\n",
        "        self.noise = noise\n",
        "        self.noise_std = noise_std\n",
        "        self.noise_clip = noise_clip\n",
        "\n",
        "        # Set the device globally\n",
        "        torch.set_default_device(device)\n",
        "\n",
        "        if continue_training:\n",
        "\n",
        "          # Transfer Learning\n",
        "          self.actor = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "          self.actor.load_state_dict(torch.load('/content/checkpoint_actor_300.pth'))\n",
        "          self.actor_target = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "          self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
        "          self.actor_optimizer.load_state_dict(torch.load('/content/checkpoint_actor_optimizer_300.pth'))\n",
        "\n",
        "          self.critic = Critic(state_size, action_size).to(device)\n",
        "          self.critic.load_state_dict(torch.load('/content/checkpoint_critic_300.pth'))\n",
        "          self.critic_target = Critic(state_size, action_size).to(device)\n",
        "          self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "          self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
        "          self.critic_optimizer.load_state_dict(torch.load('/content/checkpoint_critic_optimizer_300.pth'))\n",
        "\n",
        "        else:\n",
        "          # Actor Network (w/ Target Network)\n",
        "          self.actor = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "          self.actor_target = Actor(state_size, action_size, float(max_action[0])).to(device)\n",
        "          self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
        "\n",
        "          # Critic Network (w/ Target Network)\n",
        "          self.critic = Critic(state_size, action_size).to(device)\n",
        "          self.critic_target = Critic(state_size, action_size).to(device)\n",
        "          self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "          self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done) -> None:\n",
        "        \"\"\"Save experience in replay memory\"\"\"\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "    def predict(self, states) -> np.ndarray:\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "\n",
        "        assert isinstance(states, np.ndarray), \"States is not of data structure (np.ndarray) in PREDICT -> states: {}.\".format(type(states))\n",
        "        assert isinstance(states[0], np.float32), \"States is not of type (np.float32) in PREDICT -> states type: {}.\".format(type(states))\n",
        "        assert states.shape[0] == 24, \"The size of the states is not (24) in PREDICT -> states size: {}.\".format(states.shape[0])\n",
        "        assert states.ndim == 1, \"The ndim of the states is not (1) in PREDICT -> states ndim: {}.\".format(states.ndim)\n",
        "\n",
        "        state = torch.from_numpy(states).float().to(device)\n",
        "\n",
        "        self.actor.eval()\n",
        "        with torch.no_grad():\n",
        "          action = self.actor(state).cpu().data.numpy()\n",
        "\n",
        "        self.actor.train()\n",
        "\n",
        "        action = action.clip(self.min_action[0], self.max_action[0])\n",
        "\n",
        "        assert len(action) == self.action_size, \"The action size is different from the defined size in PREDICT.\"\n",
        "        assert isinstance(action[0], np.float32), \"Action is not of type (np.float32) in PREDICT -> action type: {}.\".format(type(action))\n",
        "\n",
        "        return action\n",
        "\n",
        "    def learn(self, n_iteraion: int, gamma=GAMMA) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\" Update policy and value parameters using given batch of experience tuples.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            n_iteraion (int): the number of iterations to train network\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "\n",
        "        if len(self.memory) > BATCH_SIZE:\n",
        "            average_Q = 0\n",
        "            max_Q = -inf\n",
        "            average_critic_loss = 0\n",
        "            average_actor_loss = 0\n",
        "\n",
        "            for i in range(n_iteraion):\n",
        "                state, action, reward, next_state, done = self.memory.sample()\n",
        "\n",
        "                action_ = action.cpu().numpy()\n",
        "\n",
        "                # ---------------------------- update critic ---------------------------- #\n",
        "                # Get predicted next-state actions and Q values from target models\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    # Generate a random noise\n",
        "                    noise = torch.FloatTensor(action_).data.normal_(0, self.noise).to(device)\n",
        "                    noise = noise.clamp(-self.noise_clip, self.noise_clip)\n",
        "                    actions_next = (self.actor_target(next_state) + noise).clamp(self.min_action[0].astype(float), self.max_action[0].astype(float))\n",
        "\n",
        "                    Q1_targets_next, Q2_targets_next = self.critic_target(next_state, actions_next)\n",
        "\n",
        "                    Q_targets_next = torch.min(Q1_targets_next, Q2_targets_next)\n",
        "\n",
        "                    average_Q += torch.mean(Q_targets_next)\n",
        "                    max_Q = max(max_Q, torch.max(Q_targets_next))\n",
        "\n",
        "                    # Compute Q targets for current states (y_i)\n",
        "                    Q_targets = reward + (gamma * Q_targets_next * (1 - done)).detach()\n",
        "\n",
        "                # Compute critic loss\n",
        "                Q1_expected, Q2_expected = self.critic(state, action)\n",
        "                critic_loss = F.mse_loss(Q1_expected, Q_targets) + F.mse_loss(Q2_expected, Q_targets)\n",
        "\n",
        "                # Minimize the loss\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                if i % UPDATE_EVERY_STEP == 0:\n",
        "                    # ---------------------------- update actor ---------------------------- #\n",
        "                    # Compute actor loss\n",
        "                    actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\n",
        "                    # Minimize the loss\n",
        "                    self.actor_optimizer.zero_grad()\n",
        "                    actor_loss.backward()\n",
        "                    self.actor_optimizer.step()\n",
        "\n",
        "                    # ----------------------- update target networks ----------------------- #\n",
        "                    self.soft_update(self.critic, self.critic_target, TAU)\n",
        "                    self.soft_update(self.actor, self.actor_target, TAU)\n",
        "\n",
        "                average_critic_loss += critic_loss\n",
        "                average_actor_loss += actor_loss\n",
        "\n",
        "            loss_critic = average_critic_loss / n_iteraion\n",
        "            loss_actor = average_actor_loss / n_iteraion\n",
        "            average_policy = average_Q / n_iteraion\n",
        "            max_policy = max_Q\n",
        "\n",
        "            return (loss_critic, loss_actor, average_policy, max_policy)\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau) -> None:\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        Params\n",
        "        ======\n",
        "            local_model: PyTorch model (weights will be copied from)\n",
        "            target_model: PyTorch model (weights will be copied to)\n",
        "            tau (float): interpolation parameter\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "    def save(self, filename, version) -> None:\n",
        "          \"\"\" Save the model \"\"\"\n",
        "          torch.save(self.critic.state_dict(), filename + \"_critic_\" + version + \".pth\")\n",
        "          torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer_\" + version + \".pth\")\n",
        "\n",
        "          torch.save(self.actor.state_dict(), filename + \"_actor_\" + version + \".pth\")\n",
        "          torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer_\" + version + \".pth\")\n",
        "\n",
        "    def load(self, filename) -> None:\n",
        "          \"\"\" Load the model \"\"\"\n",
        "          self.critic.load_state_dict(torch.load(filename))\n",
        "          self.critic_optimizer.load_state_dict(torch.load(filename))\n",
        "          self.critic_target = copy.deepcopy(self.critic)\n",
        "\n",
        "          self.actor.load_state_dict(torch.load(filename))\n",
        "          self.actor_optimizer.load_state_dict(torch.load(filename))\n",
        "          self.actor_target = copy.deepcopy(self.actor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Wrapper**"
      ],
      "metadata": {
        "id": "XXqRDsXVce1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import wandb\n",
        "from typing import Union\n",
        "from gym import spaces\n",
        "from gym.spaces import Box\n",
        "\n",
        "class CustomWrapper(gym.Wrapper):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        min_action: Union[float, int, np.ndarray],\n",
        "        max_action: Union[float, int, np.ndarray],\n",
        "    ):\n",
        "        \"\"\"Initializes the :class:`RescaleAction` wrapper.\n",
        "        Args:\n",
        "            env (Env): The environment to apply the wrapper\n",
        "            min_action (float, int or np.ndarray): The min values for each action. This may be a numpy array or a scalar.\n",
        "            max_action (float, int or np.ndarray): The max values for each action. This may be a numpy array or a scalar.\n",
        "        \"\"\"\n",
        "        assert isinstance(\n",
        "            env.action_space, spaces.Box\n",
        "        ), f\"expected Box action space, got {type(env.action_space)}\"\n",
        "        assert np.less_equal(min_action, max_action).all(), (min_action, max_action)\n",
        "\n",
        "        super().__init__(env)\n",
        "        self.min_action = (\n",
        "            np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + min_action\n",
        "        )\n",
        "        self.max_action = (\n",
        "            np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + max_action\n",
        "        )\n",
        "        self.action_space = spaces.Box(\n",
        "            low=min_action,\n",
        "            high=max_action,\n",
        "            shape=env.action_space.shape,\n",
        "            dtype=env.action_space.dtype,\n",
        "        )\n",
        "        low = self.observation_space.low[:24]\n",
        "        high = self.observation_space.high[:24]\n",
        "        self.observation_space = Box(low, high, dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, info = self.env.step(action)\n",
        "        obs = obs[:24]\n",
        "        return obs, reward, terminated, info\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        obs = obs[:24]\n",
        "        return obs\n",
        "\n",
        "    def action(self, action):\n",
        "        \"\"\"Rescales the action affinely from  [:attr:`min_action`, :attr:`max_action`] to the action space of the base environment, :attr:`env`.\n",
        "        Args:\n",
        "            action: The action to rescale\n",
        "        Returns:\n",
        "            The rescaled action\n",
        "        \"\"\"\n",
        "        assert np.all(np.greater_equal(action, self.min_action)), (\n",
        "            action,\n",
        "            self.min_action,\n",
        "        )\n",
        "        assert np.all(np.less_equal(action, self.max_action)), (action, self.max_action)\n",
        "        low = self.env.action_space.low\n",
        "        high = self.env.action_space.high\n",
        "        action = low + (high - low) * (\n",
        "            (action - self.min_action) / (self.max_action - self.min_action)\n",
        "        )\n",
        "        action = np.clip(action, low, high)\n",
        "        return action\n",
        "\n",
        "    def seed(self, seed):\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "gym.logger.set_level(40)\n",
        "env = CustomWrapper(gym.make(\"BipedalWalker-v3\"),  min_action = -1.0,  max_action = 1.0)\n",
        "env.seed(0)\n",
        "\n",
        "agent = TD3Agent(state_size=env.observation_space.shape[0], \\\n",
        "                 action_size=env.action_space.shape[0], \\\n",
        "                 max_action=env.action_space.high, \\\n",
        "                 min_action=env.action_space.low, continue_training=False)\n"
      ],
      "metadata": {
        "id": "YZNQsiSqciQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train**"
      ],
      "metadata": {
        "id": "uV1KanhbJwfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"td3\") # fb372890f5180a16a9cd2df5b9558e55493cd16c"
      ],
      "metadata": {
        "id": "d46eJqqklJvj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "7247cf36-e934-4d7e-e0d2-8d0029fe818b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240408_231356-46h1uzjf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alangrotti/td3/runs/46h1uzjf' target=\"_blank\">laced-breeze-20</a></strong> to <a href='https://wandb.ai/alangrotti/td3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alangrotti/td3' target=\"_blank\">https://wandb.ai/alangrotti/td3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alangrotti/td3/runs/46h1uzjf' target=\"_blank\">https://wandb.ai/alangrotti/td3/runs/46h1uzjf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alangrotti/td3/runs/46h1uzjf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7afcc97f0c10>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERQ6QREQC99V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "fc5b5010-7b8d-4089-de3e-17d8bc90d7cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 8\tAverage Score: -101.28\tScore: -102.52"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-f1eae60cae44>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-f1eae60cae44>\u001b[0m in \u001b[0;36mtd3\u001b[0;34m(n_episodes, max_t)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-fbc8fffb5d2b>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, n_iteraion, gamma)\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0mactions_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                     \u001b[0mQ1_targets_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ2_targets_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mQ_targets_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ1_targets_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ2_targets_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-4e4c4ad4f368>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mq1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def td3(n_episodes=2000, max_t=2000):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    times_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    solved = False\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        start_time = time.time()\n",
        "        for t in range(max_t):\n",
        "            action = agent.predict(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            if done or t==(max_t-1):\n",
        "                critic_loss, actor_loss, q, max = agent.learn(t)\n",
        "                break\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        scores_deque.append(score)\n",
        "        times_deque.append(duration)\n",
        "        scores.append(score)\n",
        "        mean_score = np.mean(scores_deque)\n",
        "        mean_times = np.mean(times_deque)\n",
        "\n",
        "        #wandb.log({'Score': mean_score, 'Critic loss': critic_loss, 'Actor loss': actor_loss, 'Average Q': q, 'Max. Q': max, \"Duration \": mean_times}, step=i_episode)\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, mean_score, score), end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "        if i_episode % 500 == 0:\n",
        "            agent.save(\"checkpoint\", str(i_episode))\n",
        "        if mean_score >= 300 and solved == False:\n",
        "            print('\\rSolved at Episode {} !\\tAverage Score: {:.2f}'.format(i_episode, mean_score))\n",
        "            agent.save(\"checkpoint\")\n",
        "            solved = True\n",
        "\n",
        "    return scores\n",
        "\n",
        "scores = td3()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Result**"
      ],
      "metadata": {
        "id": "_GJ4lM-g8jB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "metadata": {
        "id": "iUJfDXmtMesi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "\n",
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "agent.actor.load_state_dict(torch.load('/content/checkpoint_actor_300.pth'))\n",
        "agent.critic.load_state_dict(torch.load('/content/checkpoint_critic_300.pth'))\n",
        "agent.actor_optimizer.load_state_dict(torch.load('/content/checkpoint_actor_optimizer_300.pth'))\n",
        "agent.critic_optimizer.load_state_dict(torch.load('/content/checkpoint_critic_optimizer_300.pth'))\n",
        "\n",
        "env = gym.make('BipedalWalker-v3')\n",
        "state = env.reset()\n",
        "score = 0\n",
        "img = plt.imshow(env.render('rgb_array'))\n",
        "while True:\n",
        "    img.set_data(env.render('rgb_array'))\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    action = agent.predict(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if np.any(done):\n",
        "        break\n",
        "\n",
        "print(\"Score: {}\".format(score))"
      ],
      "metadata": {
        "id": "IXJHQR6N8uwN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "176daad0-4a4a-4003-a152-0efc59547e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 263.1796404668868\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test**"
      ],
      "metadata": {
        "id": "pt4M5an2gPmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestModel(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.env = gym.make(\"BipedalWalker-v3\")\n",
        "\n",
        "        # param model and buffer\n",
        "        self.batch_size = 100\n",
        "        self.buffer_size = int(1e5)\n",
        "        self.random_seed = 0\n",
        "        self.error = 0\n",
        "\n",
        "        # size action / state\n",
        "        self.action_size = self.env.action_space.shape[0]\n",
        "        self.state_size = self.env.observation_space.shape[0]\n",
        "\n",
        "        # min / max action\n",
        "        self.min_action = self.env.action_space.low\n",
        "        self.max_action = self.env.action_space.high\n",
        "\n",
        "        # min / max state\n",
        "        self.min_state = 0\n",
        "        self.max_state = 1\n",
        "\n",
        "        # min / max reward\n",
        "        self.min_reward = -300\n",
        "        self.max_reward = 300\n",
        "\n",
        "        self.model = TD3Agent(state_size=self.state_size, action_size=self.action_size, \\\n",
        "                         max_action=self.max_action, min_action=self.min_action, random_seed=self.random_seed)\n",
        "\n",
        "        self.memory = ReplayBufferPer(self.buffer_size)\n",
        "\n",
        "        # param number tests\n",
        "        self.num_attempts = 150\n",
        "\n",
        "    def _randomStates(self):\n",
        "        states = np.array([random.uniform(self.min_state, self.max_state) for _ in range(24)], dtype=np.float32)\n",
        "        return states\n",
        "\n",
        "    def _randomAction(self):\n",
        "        action = np.random.uniform(self.min_action, self.max_action, self.action_size)\n",
        "        return action\n",
        "\n",
        "    def _randomDone(self):\n",
        "        done = random.choice([True, False])\n",
        "        return done\n",
        "\n",
        "    def _randomReward(self):\n",
        "        reward = random.randint(self.min_reward, self.max_reward)\n",
        "        return reward\n",
        "\n",
        "    def test_predict_act(self):\n",
        "        \"\"\" Teste para verificar se o estado de saida da rede contem os valores minimos e maximos de ação que o ambiente exigi\n",
        "\n",
        "            Input: numpy.ndarray [24]\n",
        "            output: numpy.ndarray [4]\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        for _ in range(self.num_attempts):\n",
        "            states = self._randomStates()\n",
        "            action = self.model.predict(states)\n",
        "            is_valid = (isinstance(action, np.ndarray) and np.all(action >= self.min_action) and np.all(action <= self.max_action))\n",
        "\n",
        "            if not is_valid:\n",
        "                self.fail(\"Teste falhou na tentativa {}\".format(_ + 1))\n",
        "\n",
        "    def test_buffer_type(self):\n",
        "        while True:\n",
        "          next_state, reward, done, action, state = self._randomStates(), self._randomReward(), self._randomDone(), self._randomAction(), self._randomStates()\n",
        "\n",
        "          self.memory.add((state, action, reward, next_state, done), reward)\n",
        "\n",
        "          if len(self.memory) > self.batch_size:\n",
        "              break\n",
        "\n",
        "\n",
        "        (states, actions, rewards, next_states, dones), idxs, is_weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        self.assertIsInstance(states, torch.Tensor)\n",
        "        self.assertIsInstance(actions, torch.Tensor)\n",
        "        self.assertIsInstance(rewards, torch.Tensor)\n",
        "        self.assertIsInstance(next_states, torch.Tensor)\n",
        "        self.assertIsInstance(dones, torch.Tensor)\n",
        "\n",
        "    def test_buffer_size(self):\n",
        "        while True:\n",
        "          next_state, reward, done, action, state = self._randomStates(), self._randomReward(), self._randomDone(), self._randomAction(), self._randomStates()\n",
        "          self.memory.add((state, action, reward, next_state, done), reward)\n",
        "\n",
        "          if len(self.memory) > self.batch_size:\n",
        "            break\n",
        "\n",
        "        (states, actions, rewards, next_states, dones), idxs, is_weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        expected_batch_size = self.batch_size\n",
        "        self.assertEqual(states.size(0), expected_batch_size)\n",
        "        self.assertEqual(actions.size(0), expected_batch_size)\n",
        "        self.assertEqual(rewards.size(0), expected_batch_size)\n",
        "        self.assertEqual(next_states.size(0), expected_batch_size)\n",
        "        self.assertEqual(dones.size(0), expected_batch_size)\n",
        "\n",
        "    def test_buffer_range(self):\n",
        "        while True:\n",
        "          next_state, reward, done, action, state = self._randomStates(), self._randomReward(), self._randomDone(), self._randomAction(), self._randomStates()\n",
        "          self.memory.add((state, action, reward, next_state, done), reward)\n",
        "\n",
        "          if len(self.memory) > self.batch_size:\n",
        "            break\n",
        "\n",
        "        (states, actions, rewards, next_states, dones), idxs, weights = self.memory.sample(self.batch_size)\n",
        "\n",
        "        self.assertTrue(np.all(states[1].cpu().data.numpy() >= self.min_state) and np.all(states[1].cpu().data.numpy() <= self.max_state))\n",
        "        self.assertTrue(np.all(actions[1].cpu().data.numpy() >= self.min_action) and np.all(actions[1].cpu().data.numpy() <= self.max_action))\n",
        "        self.assertTrue(np.all(rewards[1].cpu().data.numpy() >= self.min_reward) and np.all(rewards[1].cpu().data.numpy() <= self.max_reward))\n",
        "        self.assertTrue(np.all(next_states[1].cpu().data.numpy() >= self.min_state) and np.all(next_states[1].cpu().data.numpy() <= self.max_state))\n",
        "        self.assertTrue(np.all(dones.cpu().data.numpy() >= 0.) and np.all(dones.cpu().data.numpy() <= 1.))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "id": "huHvCkTggSDs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gkNza9le_wuU",
        "ZADS3Hz_NdzM",
        "8wHw8OOk38ps",
        "XXqRDsXVce1Y",
        "_GJ4lM-g8jB9",
        "pt4M5an2gPmn"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}